无人车感知是自动驾驶系统的基础，相当于车辆的 “眼睛” 和 “耳朵”，核心是通过各类传感器采集环境数据，再借助算法处理信息，最终实现环境理解与自身定位，为后续决策和控制提供依据，以下是其关键组成部分的详细介绍：
核心感知硬件
不同传感器特性互补，共同构建起多维度的感知体系，具体信息如下：
传感器类型	工作原理	核心优势	典型应用场景
激光雷达	发射激光脉冲并接收反射信号，生成 3D 点云图	厘米级测距精度，可构建三维环境模型，360° 全方位扫描	高精地图绘制、复杂障碍物识别、精准定位
车载摄像头	采集二维图像，通过图像算法做模式识别	还原接近人眼的环境细节，可识别交通标志、车道线等	交通信号灯识别、车道线检测、行人及车辆分类
毫米波雷达	发射 10 - 300GHz 的电磁波，基于多普勒效应测距测速	抗雾、雨、尘能力强，全天候工作，测距远	自适应巡航、碰撞预警、盲区探测、自动紧急制动
超声波雷达	发射 40kHz 超声波，通过时间差计算距离	近距离测距精度高（误差 1 - 3 厘米），成本低	倒车辅助、自动泊车等低速近距离障碍物探测
GNSS/IMU 系统	GNSS 提供卫星定位，IMU 测量加速度和角速度	GNSS 实现基础定位，IMU 可在 GNSS 信号丢失时补位	车辆自身高精度定位，辅助路径规划
核心感知任务
感知系统通过硬件采集数据后，需完成一系列关键任务以提炼有效信息，支撑后续驾驶决策：
环境要素识别：这是核心任务之一，不仅要检测出行人、车辆、骑行者等移动目标，还要识别交通标志、信号灯、护栏等静态物体。比如通过摄像头结合 YOLO 算法快速检测前方车辆，借助激光雷达区分行人与路边障碍物，同时还能通过算法预测行人横穿马路、车辆违规变道等目标的行为趋势。
道路场景理解：重点完成车道线检测和可行驶区域判断。通过提取图像中的车道线边缘、颜色特征，用多项式拟合弯曲车道线并计算曲率，确定车辆相对车道线的偏移量；还可通过神经网络分割图像，精准划分出可行驶区域，规避施工路段、路沿石等不可通行区域。
自身精准定位：无人车需明确自身在环境中的位置，通常结合 GNSS 的卫星定位数据、IMU 的惯性导航数据，再融合激光雷达的点云与高精地图匹配结果，实现厘米级定位，确保车辆行驶在规划路径上。
关键支撑技术
单一传感器存在局限性，需依靠技术手段整合优化数据，保障感知的可靠性：
多传感器融合：通过卡尔曼滤波等算法整合不同传感器数据。例如激光雷达弥补摄像头在恶劣天气下的识别短板，摄像头则解决激光雷达对目标分类精度不足的问题，形成冗余设计，减少单一传感器故障带来的风险。
深度学习赋能：借助 CNN（卷积神经网络）处理激光雷达 3D 点云数据和摄像头图像数据，实现目标分类；通过 LSTM 网络分析目标运动轨迹，提升动态目标行为预测的准确性，像 YOLO、Faster - RCNN 等算法已成为目标实时检测的主流方案。