# train_agent.py
"""
å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è®­ç»ƒè„šæœ¬
- ç¯å¢ƒ: CarlaEnvMultiObs
- ç®—æ³•: PPO (Proximal Policy Optimization)
- ç‰¹æ€§: è‡ªåŠ¨æ—¥å¿—ã€å®šæœŸè¯„ä¼°ã€æœ€ä½³æ¨¡å‹ä¿å­˜ã€TensorBoard æ”¯æŒ
"""

import os
import argparse
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from carla_env.carla_env_multi_obs import CarlaEnvMultiObs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--timesteps", type=int, default=300000, help="æ€»è®­ç»ƒæ­¥æ•° (é»˜è®¤: 300000)")
    parser.add_argument("--log_dir", type=str, default="./logs", help="æ—¥å¿—ç›®å½•")
    parser.add_argument("--model_save_path", type=str, default="./checkpoints/best_model.zip", help="æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„")
    args = parser.parse_args()

    # åˆ›å»ºå¿…è¦ç›®å½•
    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(os.path.dirname(args.model_save_path), exist_ok=True)

    print("ğŸš€ å¼€å§‹è®­ç»ƒ PPO æ™ºèƒ½ä½“...")
    print(f"  - æ€»æ­¥æ•°: {args.timesteps:,}")
    print(f"  - æ—¥å¿—ç›®å½•: {args.log_dir}")
    print(f"  - æ¨¡å‹ä¿å­˜è·¯å¾„: {args.model_save_path}")

    # åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒï¼ˆå•è¿›ç¨‹ï¼Œä¾¿äºè°ƒè¯•ï¼‰
    env = CarlaEnvMultiObs(keep_alive_after_exit=False)  # è®­ç»ƒæ—¶ä¸ä¿ç•™è½¦è¾†
    env = Monitor(env, filename=os.path.join(args.log_dir, "train_monitor.csv"))

    # åˆå§‹åŒ–è¯„ä¼°ç¯å¢ƒï¼ˆç‹¬ç«‹å®ä¾‹ï¼Œé¿å…å¹²æ‰°è®­ç»ƒï¼‰
    eval_env = CarlaEnvMultiObs(keep_alive_after_exit=False)
    eval_env = Monitor(eval_env, filename=os.path.join(args.log_dir, "eval_monitor.csv"))

    # åˆ›å»º PPO æ¨¡å‹ï¼ˆä½¿ç”¨ Stable Baselines3 é»˜è®¤è¶…å‚ï¼Œé€‚åˆè¿ç»­æ§åˆ¶ï¼‰
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log=args.log_dir,
        device="auto"  # è‡ªåŠ¨é€‰æ‹© GPU/CPU
    )

    # è®¾ç½®è¯„ä¼°å›è°ƒï¼šæ¯ 5000 æ­¥è¯„ä¼°ä¸€æ¬¡ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.dirname(args.model_save_path),
        log_path=args.log_dir,
        eval_freq=5000,          # æ¯ 5000 è®­ç»ƒæ­¥è¯„ä¼°ä¸€æ¬¡
        deterministic=True,      # è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        render=False,            # ä¸æ¸²æŸ“ï¼ˆåŠ é€Ÿè¯„ä¼°ï¼‰
        n_eval_episodes=3,       # æ¯æ¬¡è¯„ä¼°è·‘ 3 è½®å–å¹³å‡
        verbose=1
    )

    # ï¼ˆå¯é€‰ï¼‰æ·»åŠ æ£€æŸ¥ç‚¹å›è°ƒï¼šæ¯ 5 ä¸‡æ­¥ä¿å­˜ä¸€ä¸ª checkpoint
    # checkpoint_callback = CheckpointCallback(save_freq=50000, save_path="./checkpoints/", name_prefix="ppo_carla")

    try:
        # å¼€å§‹è®­ç»ƒ
        model.learn(
            total_timesteps=args.timesteps,
            callback=eval_callback,
            tb_log_name="PPO_Carla",      # TensorBoard ä¸­çš„ run åç§°
            reset_num_timesteps=True,     # ä»é›¶å¼€å§‹è®¡æ•°ï¼ˆè‹¥ç»­è®­è®¾ä¸º Falseï¼‰
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nğŸ›‘ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰æ¨¡å‹...")
    finally:
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆéæœ€ä½³ï¼‰
        final_path = os.path.join(os.path.dirname(args.model_save_path), "final_model.zip")
        model.save(final_path)
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")
        env.close()
        eval_env.close()

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    print("\nğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿:")
    print("   tensorboard --logdir ./logs")
    print("\nğŸ§ª è¯„ä¼°æœ€ä½³æ¨¡å‹:")
    print("   python eval_agent.py --model_path ./checkpoints/best_model.zip")


if __name__ == "__main__":
    main()
