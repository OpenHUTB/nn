# å¼ºåˆ¶æ‰“å°è„šæœ¬æ ‡è¯†ï¼Œç¡®è®¤è¿è¡Œçš„æ˜¯æ–°ç‰ˆæœ¬
print("=====================================")
print("âœ… è¿è¡Œçš„æ˜¯æœ€ç»ˆç‰ˆè®­ç»ƒè„šæœ¬ï¼ˆtrain_final.pyï¼‰")
print("=====================================")

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import yaml
from models.dqn_agent import DQNAgent
from models.pruning import ModelPruner
from models.quantization import quantize_model
from envs.carla_environment import CarlaEnvironment 

def load_config(config_path='configs/config.yaml'):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r') as file:
            config = yaml.safe_load(file)
        print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶ï¼š{config_path}")
        return config
    except Exception as e:
        raise ValueError(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼š{e}")

def train_model(config):
    """è®­ç»ƒDQNæ¨¡å‹ï¼ˆé€‚é…CARLAçœŸå®å›¾åƒè¾“å…¥ï¼‰"""
    # 1. åˆå§‹åŒ–è®¾å¤‡ï¼ˆGPU/CPUï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # 2. åˆå§‹åŒ–CARLAç¯å¢ƒ
    print("ğŸ”§ åˆå§‹åŒ–CARLAç¯å¢ƒ...")
    env = CarlaEnvironment()
    state_shape = env.observation_space.shape  # (128, 128, 3) å®Œæ•´å›¾åƒå½¢çŠ¶
    action_size = env.action_space.n
    print(f"âœ… CARLAç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ | çŠ¶æ€å½¢çŠ¶ï¼š{state_shape} | åŠ¨ä½œç»´åº¦ï¼š{action_size}")

    # 3. åˆå§‹åŒ–DQNæ™ºèƒ½ä½“ï¼ˆä»…ä¼ state_shapeï¼Œç»å¯¹ä¸å«state_sizeï¼‰
    print("ğŸ”§ åˆå§‹åŒ–DQNæ™ºèƒ½ä½“...")
    agent = DQNAgent(
        state_shape=state_shape,  # å”¯ä¸€æ­£ç¡®çš„å‚æ•°å
        action_size=action_size,
        config=config
    )
    print("âœ… DQNæ™ºèƒ½ä½“åˆå§‹åŒ–æˆåŠŸ")

    # 4. è®­ç»ƒå‚æ•°
    episodes = config['train']['episodes']
    batch_size = config['train']['batch_size']
    reward_history = []  # è®°å½•å¥–åŠ±å†å²

    # 5. å¼€å§‹è®­ç»ƒ
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼šå…±{episodes}è½®Episode")
    for e in range(episodes):
        # é‡ç½®ç¯å¢ƒï¼Œè·å–åˆå§‹çŠ¶æ€
        state = env.reset()
        # å›¾åƒå½’ä¸€åŒ–ï¼ˆ0-255 â†’ 0-1ï¼‰
        state = state.astype(np.float32) / 255.0
        done = False
        total_reward = 0
        step = 0

        while not done:
            step += 1
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.act(state)
            # æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–ç¯å¢ƒåé¦ˆ
            next_state, reward, done, _ = env.step(action)
            
            # æ•°æ®é¢„å¤„ç†
            next_state = next_state.astype(np.float32) / 255.0  # å½’ä¸€åŒ–
            reward = np.clip(reward, -10, 10)  # å¥–åŠ±è£å‰ªï¼Œé¿å…æç«¯å€¼

            # å­˜å‚¨ç»éªŒ
            agent.remember(state, action, reward, next_state, done)
            # æ›´æ–°çŠ¶æ€
            state = next_state
            total_reward += reward

            # ç»éªŒå›æ”¾ï¼ˆæ‰¹é‡æ›´æ–°ï¼‰
            if len(agent.memory) > batch_size:
                agent.replay(batch_size)

            # é˜²æ­¢å•è½®æ­¥æ•°è¿‡å¤š
            if step > 500:
                done = True

        # è®°å½•å¥–åŠ±ï¼Œæ‰“å°è®­ç»ƒæ—¥å¿—
        reward_history.append(total_reward)
        avg_reward = np.mean(reward_history[-10:]) if len(reward_history) >= 10 else total_reward
        print(f"ğŸ“Š Episode {e+1}/{episodes} | æ€»å¥–åŠ±ï¼š{total_reward:.2f} | æœ€è¿‘10è½®å¹³å‡ï¼š{avg_reward:.2f} | æ¢ç´¢ç‡ï¼š{agent.epsilon:.4f}")

    # 6. æ¨¡å‹ä¼˜åŒ–ï¼ˆå‰ªæ+é‡åŒ–ï¼‰
    print("\nğŸ”§ å¼€å§‹æ¨¡å‹ä¼˜åŒ–ï¼ˆå‰ªæ+é‡åŒ–ï¼‰...")
    try:
        pruner = ModelPruner(agent.model)
        pruner.prune_model(amount=0.2)  # å‰ªæ20%å‚æ•°
        agent.model = quantize_model(agent.model)  # é‡åŒ–æ¨¡å‹
        print("âœ… æ¨¡å‹ä¼˜åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹ä¼˜åŒ–å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰ï¼š{e}")

    # 7. å¯¼å‡ºONNXæ¨¡å‹
    try:
        export_to_onnx(agent.model, state_shape, device)
    except Exception as e:
        print(f"âš ï¸  ONNXå¯¼å‡ºå¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰ï¼š{e}")
    
    # 8. ä¿å­˜æ¨¡å‹æƒé‡
    torch.save(agent.model.state_dict(), "dqn_carla_model_final.pth")
    print("âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜ï¼šdqn_carla_model_final.pth")

    # 9. æ¸…ç†ç¯å¢ƒ
    env.close()
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")

def export_to_onnx(model, state_shape, device, file_path='model_final.onnx'):
    """å¯¼å‡ºONNXæ¨¡å‹ï¼ˆé€‚é…CNNå›¾åƒè¾“å…¥ï¼‰"""
    # æ„å»ºdummy inputï¼š(1, 3, 128, 128)
    dummy_input = torch.randn(1, 3, state_shape[0], state_shape[1]).to(device)
    # å¯¼å‡ºONNX
    torch.onnx.export(
        model,
        dummy_input,
        file_path,
        opset_version=12,
        input_names=["input_image"],
        output_names=["action_q_values"],
        dynamic_axes={"input_image": {0: "batch_size"}, "action_q_values": {0: "batch_size"}}
    )
    print(f"âœ… ONNXæ¨¡å‹å·²å¯¼å‡ºï¼š{file_path}")

if __name__ == "__main__":
    # åŠ è½½é…ç½®
    config = load_config()
    # å¯åŠ¨è®­ç»ƒ
    try:
        train_model(config)
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™ï¼š{e}")
        import traceback
        traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯æ ˆ
        raise