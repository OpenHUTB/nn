import carla
import queue
import random
import cv2
import numpy as np
import math
import os
import time  # æ–°å¢ï¼šè®¡ç®—å¸§ç‡

# ä¿®å¤Deep SORTçš„APIå¼ƒç”¨é—®é¢˜
import scipy.optimize as opt
from deep_sort import nn_matching
from deep_sort.detection import Detection
from deep_sort.tracker import Tracker

# COCOç±»åˆ«åç§°
COCO_CLASS_NAMES = [
    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant',
    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',
    'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',
    'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',
    'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',
    'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
    'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',
    'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]


# ===================== Deep SORTä¿®å¤å‡½æ•° =====================
def linear_assignment(cost_matrix):
    x, y = opt.linear_sum_assignment(cost_matrix)
    return np.array(list(zip(x, y)))


import deep_sort.utils.linear_assignment as la

la.linear_assignment = linear_assignment


class SimpleBoxEncoder:
    def __init__(self):
        pass

    def __call__(self, image, boxes):
        features = []
        for box in boxes:
            x1, y1, w, h = box
            aspect_ratio = w / h if h != 0 else 1.0
            center_x = (x1 + w / 2) / image.shape[1]
            center_y = (y1 + h / 2) / image.shape[0]
            area = (w * h) / (image.shape[0] * image.shape[1])
            feature = np.array([aspect_ratio, center_x, center_y, area] + [0.0] * 124)
            features.append(feature)
        return np.array(features)


def create_box_encoder(model_filename=None, batch_size=32):
    return SimpleBoxEncoder()


# ===================== å·¥å…·å‡½æ•° =====================
def get_image_point(vertex, K, world_to_camera):
    point_3d = np.array([vertex.x, vertex.y, vertex.z, 1.0])
    point_camera = np.dot(world_to_camera, point_3d)
    point_img = np.dot(K, point_camera[:3])
    point_img = point_img / point_img[2]
    return (point_img[0], point_img[1])


def get_2d_box_from_3d_edges(points_2d, edges, image_h, image_w):
    x_coords = [p[0] for p in points_2d]
    y_coords = [p[1] for p in points_2d]
    x_min = max(0, min(x_coords))
    x_max = min(image_w, max(x_coords))
    y_min = max(0, min(y_coords))
    y_max = min(image_h, max(y_coords))
    return x_min, x_max, y_min, y_max


def point_in_canvas(point, image_h, image_w):
    x, y = point
    return 0 <= x <= image_w and 0 <= y <= image_h


def build_projection_matrix(w, h, fov, is_behind_camera=False):
    focal = w / (2.0 * math.tan(fov * math.pi / 360.0))
    K = np.identity(3)
    K[0, 0] = K[1, 1] = focal
    K[0, 2] = w / 2.0
    K[1, 2] = h / 2.0
    if is_behind_camera:
        K[0, 0] = -K[0, 0]
    return K


def clear_npc(world):
    for actor in world.get_actors().filter('*vehicle*'):
        if actor.attributes.get('role_name') != 'hero':
            actor.destroy()


def clear_static_vehicle(world):
    pass


def clear(world, camera):
    if camera:
        camera.destroy()
    for actor in world.get_actors().filter('*vehicle*'):
        actor.destroy()


# ===================== ã€å®Œå–„ã€‘å¯è§†åŒ–å‡½æ•° =====================
def draw_bounding_boxes(image, bboxes, labels, class_names, ids):
    """ä¼˜åŒ–æ¡†çš„é¢œè‰²ï¼Œä¸åŒIDä½¿ç”¨ä¸åŒé¢œè‰²"""

    # ç”Ÿæˆå›ºå®šçš„é¢œè‰²æ˜ å°„ï¼ˆåŸºäºIDçš„å“ˆå¸Œå€¼ï¼‰
    def get_color(track_id):
        np.random.seed(track_id)
        return tuple(np.random.randint(0, 255, 3).tolist())

    for bbox, label, track_id in zip(bboxes, labels, ids):
        x1, y1, x2, y2 = bbox.astype(int)
        color = get_color(track_id)
        # ç»˜åˆ¶æ¡†å’ŒèƒŒæ™¯
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
        class_name = class_names[label] if label < len(class_names) else 'car'
        text = f"ID:{track_id} | {class_name}"
        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
        cv2.rectangle(image, (x1, y1 - text_size[1] - 5), (x1 + text_size[0], y1), color, -1)
        cv2.putText(image, text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
    return image


def draw_info_text(image, speed_kmh, vehicle_count, map_name, fps):
    """æ–°å¢å¸§ç‡æ˜¾ç¤ºï¼Œä¼˜åŒ–ä¿¡æ¯æ’ç‰ˆ"""
    image_copy = image.copy()
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.6
    font_thickness = 2
    text_color = (255, 255, 255)
    bg_color = (0, 0, 0)
    padding = 5

    text_list = [
        f"Map: {map_name}",
        f"Speed: {speed_kmh:.1f} km/h",
        f"Tracked Vehicles: {vehicle_count}",
        f"FPS: {fps:.1f}"  # æ–°å¢å¸§ç‡
    ]

    y_offset = 30
    for text in text_list:
        text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]
        cv2.rectangle(
            image_copy,
            (10, y_offset - text_size[1] - padding),
            (10 + text_size[0] + padding * 2, y_offset + padding),
            bg_color, -1
        )
        cv2.putText(image_copy, text, (10 + padding, y_offset), font, font_scale, text_color, font_thickness)
        y_offset += text_size[1] + padding * 3
    return image_copy


def camera_callback(image, rgb_image_queue):
    rgb_image = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))
    rgb_image_queue.put(rgb_image)


# ===================== ã€æ–°å¢ã€‘çª—å£å·¥å…·å‡½æ•° =====================
def init_window(window_name, width, height):
    """åˆå§‹åŒ–çª—å£ï¼šç½®é¡¶ã€è‡ªé€‚åº”å¤§å°ã€æ˜¾ç¤ºæç¤º"""
    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)  # å…è®¸è°ƒæ•´å¤§å°
    cv2.resizeWindow(window_name, width, height)
    cv2.setWindowProperty(window_name, cv2.WND_PROP_TOPMOST, 1)  # çª—å£ç½®é¡¶
    # æ˜¾ç¤ºåˆå§‹æç¤ºæ–‡å­—
    init_img = np.zeros((height, width, 3), dtype=np.uint8)
    cv2.putText(init_img, "CARLA DeepSORT Tracking", (width // 4, height // 2), cv2.FONT_HERSHEY_SIMPLEX, 1,
                (255, 255, 255), 2)
    cv2.imshow(window_name, init_img)
    cv2.waitKey(1)


def confirm_exit():
    """é€€å‡ºå‰å¼¹å‡ºç¡®è®¤çª—å£"""
    confirm_img = np.zeros((200, 400, 3), dtype=np.uint8)
    cv2.putText(confirm_img, "Quit? (Y/N)", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    cv2.imshow("Confirm Exit", confirm_img)
    key = cv2.waitKey(0) & 0xFF
    cv2.destroyWindow("Confirm Exit")
    return key == ord('y') or key == ord('Y')


# ===================== ä¸»å‡½æ•° =====================
def main():
    # çª—å£é…ç½®
    WINDOW_NAME = "CARLA 2D Tracking (Enhanced Window)"
    CAMERA_WIDTH = 640
    CAMERA_HEIGHT = 480

    # åˆå§‹åŒ–CARLA
    client = carla.Client('localhost', 2000)
    client.set_timeout(10.0)
    world = client.get_world()

    settings = world.get_settings()
    settings.synchronous_mode = True
    settings.fixed_delta_seconds = 0.05
    world.apply_settings(settings)

    spectator = world.get_spectator()
    spawn_points = world.get_map().get_spawn_points()
    if not spawn_points:
        print("âŒ æ— å¯ç”¨ç”Ÿæˆç‚¹ï¼")
        return

    # ç”Ÿæˆè‡ªè½¦
    bp_lib = world.get_blueprint_library()
    vehicle_bp = bp_lib.find('vehicle.lincoln.mkz_2020') or bp_lib.filter('vehicle.*')[0]
    spawn_point = random.choice(spawn_points)
    vehicle = world.try_spawn_actor(vehicle_bp, spawn_point)
    if not vehicle:
        print("âŒ è½¦è¾†ç”Ÿæˆå¤±è´¥ï¼")
        return

    # ç”Ÿæˆç›¸æœº
    camera_bp = bp_lib.find('sensor.camera.rgb')
    camera_bp.set_attribute('image_size_x', str(CAMERA_WIDTH))
    camera_bp.set_attribute('image_size_y', str(CAMERA_HEIGHT))
    camera_bp.set_attribute('fov', '90')
    camera_init_trans = carla.Transform(carla.Location(x=1.2, z=2.0), carla.Rotation(pitch=-5))
    camera = world.spawn_actor(camera_bp, camera_init_trans, attach_to=vehicle)

    # åˆå§‹åŒ–çª—å£
    init_window(WINDOW_NAME, CAMERA_WIDTH, CAMERA_HEIGHT)

    image_queue = queue.Queue(maxsize=2)
    camera.listen(lambda image: camera_callback(image, image_queue))

    clear_npc(world)
    clear_static_vehicle(world)

    # è¿½è¸ªå‚æ•°
    edges = [[0, 1], [1, 3], [3, 2], [2, 0], [0, 4], [4, 5],
             [5, 1], [5, 7], [7, 6], [6, 4], [6, 2], [7, 3]]
    K = build_projection_matrix(CAMERA_WIDTH, CAMERA_HEIGHT, 90)
    K_b = build_projection_matrix(CAMERA_WIDTH, CAMERA_HEIGHT, 90, is_behind_camera=True)

    # ç”ŸæˆNPC
    npc_count = 20
    spawned_npcs = 0
    for i in range(npc_count):
        vehicle_bp_list = bp_lib.filter('vehicle')
        car_bp = [bp for bp in vehicle_bp_list if int(bp.get_attribute('number_of_wheels')) == 4]
        if not car_bp:
            continue
        random_spawn = random.choice(spawn_points)
        if random_spawn.location.distance(vehicle.get_location()) < 10.0:
            continue
        npc = world.try_spawn_actor(random.choice(car_bp), random_spawn)
        if npc:
            npc.set_autopilot(True)
            spawned_npcs += 1
    print(f"âœ… ç”Ÿæˆ{spawned_npcs}è¾†NPCè½¦è¾†")

    vehicle.set_autopilot(True)

    # åˆå§‹åŒ–è¿½è¸ªå™¨
    encoder = create_box_encoder()
    metric = nn_matching.NearestNeighborDistanceMetric("cosine", 0.2, None)
    tracker = Tracker(metric)
    map_name = world.get_map().name.split('/')[-1]

    # å¸§ç‡è®¡ç®—å˜é‡
    frame_count = 0
    start_time = time.time()
    fps = 0.0

    # ä¸»å¾ªç¯
    while True:
        try:
            world.tick()
            frame_count += 1

            # è®¡ç®—å¸§ç‡ï¼ˆæ¯10å¸§æ›´æ–°ä¸€æ¬¡ï¼‰
            if frame_count % 10 == 0:
                end_time = time.time()
                fps = 10 / (end_time - start_time)
                start_time = end_time

            # æ—è§‚è€…è§†è§’
            transform = carla.Transform(
                vehicle.get_transform().transform(carla.Location(x=-4, z=50)),
                carla.Rotation(yaw=-180, pitch=-90)
            )
            spectator.set_transform(transform)

            # è·å–å›¾åƒ
            if image_queue.empty():
                continue
            image = image_queue.get()
            image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)
            image = cv2.flip(image, 1)

            # 3Dè½¬2Dæ£€æµ‹æ¡†
            world_2_camera = np.array(camera.get_transform().get_inverse_matrix())
            boxes = []
            for npc in world.get_actors().filter('*vehicle*'):
                if npc.id != vehicle.id:
                    bb = npc.bounding_box
                    dist = npc.get_transform().location.distance(vehicle.get_transform().location)
                    if dist < 50:
                        forward_vec = vehicle.get_transform().get_forward_vector()
                        ray = npc.get_transform().location - vehicle.get_transform().location
                        if forward_vec.dot(ray) > 0:
                            verts = [v for v in bb.get_world_vertices(npc.get_transform())]
                            points_2d = []
                            for vert in verts:
                                ray0 = vert - camera.get_transform().location
                                cam_forward_vec = camera.get_transform().get_forward_vector()
                                p = get_image_point(vert, K, world_2_camera) if cam_forward_vec.dot(
                                    ray0) > 0 else get_image_point(vert, K_b, world_2_camera)
                                p = (CAMERA_WIDTH - p[0], p[1])
                                points_2d.append(p)
                            x_min, x_max, y_min, y_max = get_2d_box_from_3d_edges(points_2d, edges, CAMERA_HEIGHT,
                                                                                  CAMERA_WIDTH)
                            if (y_max - y_min) * (x_max - x_min) > 100 and (x_max - x_min) > 20:
                                if point_in_canvas((x_min, y_min), CAMERA_HEIGHT, CAMERA_WIDTH) and point_in_canvas(
                                        (x_max, y_max), CAMERA_HEIGHT, CAMERA_WIDTH):
                                    boxes.append(np.array([x_min, y_min, x_max, y_max]))

            boxes = np.array(boxes)
            detections = []
            if len(boxes) > 0:
                sort_boxes = boxes.copy()
                for i, box in enumerate(sort_boxes):
                    box[2] -= box[0]
                    box[3] -= box[1]
                    feature = encoder(image, box.reshape(1, -1).copy())
                    detections.append(Detection(box, 1.0, feature[0]))

            # æ›´æ–°è¿½è¸ªå™¨
            tracker.predict()
            tracker.update(detections)

            # ç»˜åˆ¶ç»“æœ
            bboxes, ids = [], []
            for track in tracker.tracks:
                if track.is_confirmed() and track.time_since_update <= 1:
                    bboxes.append(track.to_tlbr())
                    ids.append(track.track_id)
            bboxes = np.array(bboxes)
            tracked_vehicle_count = len(bboxes)

            if len(bboxes) > 0:
                labels = np.array([2] * len(bboxes))
                image = draw_bounding_boxes(image, bboxes, labels, COCO_CLASS_NAMES, ids)

            # ç»˜åˆ¶ä¿¡æ¯ï¼ˆå«å¸§ç‡ï¼‰
            velocity = vehicle.get_velocity()
            speed_ms = math.hypot(velocity.x, velocity.y)
            speed_kmh = speed_ms * 3.6
            image = draw_info_text(image, speed_kmh, tracked_vehicle_count, map_name, fps)

            # æ˜¾ç¤ºå›¾åƒ
            cv2.imshow(WINDOW_NAME, image)

            # æŒ‰é”®å¤„ç†ï¼ˆå®Œå–„é€€å‡ºé€»è¾‘ï¼‰
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                if confirm_exit():  # ç¡®è®¤é€€å‡º
                    break
            elif key == ord('f'):  # Fé”®åˆ‡æ¢å…¨å±
                current_flag = cv2.getWindowProperty(WINDOW_NAME, cv2.WND_PROP_FULLSCREEN)
                new_flag = cv2.WINDOW_FULLSCREEN if current_flag == 0 else cv2.WINDOW_NORMAL
                cv2.setWindowProperty(WINDOW_NAME, cv2.WND_PROP_FULLSCREEN, new_flag)
            elif key == ord('s'):  # Sé”®ä¿å­˜å½“å‰å¸§
                save_path = f"track_frame_{frame_count}.png"
                cv2.imwrite(save_path, image)
                print(f"ğŸ’¾ å¸§å·²ä¿å­˜è‡³ {save_path}")

        except KeyboardInterrupt:
            if confirm_exit():
                break
        except Exception as e:
            print(f"âš ï¸ è¿è¡Œé”™è¯¯ï¼š{e}")
            continue

    # æ¸…ç†èµ„æº
    clear(world, camera)
    settings.synchronous_mode = False
    world.apply_settings(settings)
    cv2.destroyAllWindows()
    print("âœ… ç¨‹åºæ­£å¸¸é€€å‡º")


if __name__ == '__main__':
    main()