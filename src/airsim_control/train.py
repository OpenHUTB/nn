import glob
import os
import torch as th
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from custom_env import AirSimMazeEnv
from typing import Callable

# === è·¯å¾„é…ç½® ===
MODELS_DIR = r"D:\Others\MyAirsimprojects\models"
LOG_DIR = r"D:\Others\MyAirsimprojects\airsim_logs"

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


# === ä¼˜åŒ– 1: çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨ ===
def linear_schedule(initial_value: float) -> Callable[[float], float]:
    """
    çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å‡½æ•°ã€‚
    :param initial_value: åˆå§‹å­¦ä¹ ç‡
    :return: å½“å‰æ­¥æ•°çš„å­¦ä¹ ç‡
    """

    def func(progress_remaining: float) -> float:
        # progress_remaining ä¼šä» 1.0 (å¼€å§‹) çº¿æ€§å‡å°‘åˆ° 0.0 (ç»“æŸ)
        return progress_remaining * initial_value

    return func


def get_latest_model_path(path_dir):
    list_of_files = glob.glob(os.path.join(path_dir, '*.zip'))
    if not list_of_files:
        return None
    return max(list_of_files, key=os.path.getctime)


def main():
    set_random_seed(42)  # å›ºå®šéšæœºç§å­ï¼Œæ–¹ä¾¿å¤ç°

    # å®ä¾‹åŒ–ç¯å¢ƒ
    # ä½¿ç”¨ DummyVecEnv åŒ…è£…ï¼Œè™½ç„¶åªæœ‰ä¸€ä¸ªç¯å¢ƒï¼Œä½†è¿™æ˜¯æ ‡å‡†åšæ³•ï¼Œæ–¹ä¾¿æœªæ¥æ‰©å±•
    env = DummyVecEnv([lambda: AirSimMazeEnv()])
    env = VecMonitor(env)  # æ·»åŠ  Monitor ä»¥è®°å½•è¯¦ç»†æ—¥å¿—

    latest_model_path = get_latest_model_path(MODELS_DIR)

    # === ä¼˜åŒ– 2: è‡ªå®šä¹‰ç½‘ç»œæ¶æ„ ===
    # net_arch=[256, 256]: ä½¿ç”¨ä¸¤ä¸ª 256 ç¥ç»å…ƒçš„éšè—å±‚ (æ¯”é»˜è®¤çš„ 64 å¼ºå¤§å¾ˆå¤š)
    # activation_fn: ä½¿ç”¨ Tanh æˆ– ReLU
    policy_kwargs = dict(
        activation_fn=th.nn.Tanh,
        net_arch=dict(pi=[256, 256], vf=[256, 256])
    )

    if latest_model_path:
        print(f"--- å‘ç°å­˜æ¡£: {latest_model_path}ï¼Œç»§ç»­è®­ç»ƒ ---")
        # åŠ è½½æ—§æ¨¡å‹
        # æ³¨æ„ï¼šåŠ è½½æ—§æ¨¡å‹æ—¶ï¼Œlearning_rate ä¼šè¢«è¦†ç›–ä¸ºæ—§æ¨¡å‹çš„è®¾ç½®
        # å¦‚æœä½ æƒ³åœ¨æ—§æ¨¡å‹ä¸Šå¼ºåˆ¶ä½¿ç”¨æ–°å‚æ•°ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®æ”¹ model.learning_rate
        model = PPO.load(latest_model_path, env=env, tensorboard_log=LOG_DIR)

        # å¼ºåˆ¶æ›´æ–°å­¦ä¹ ç‡ç­–ç•¥ (å¯é€‰ï¼Œå¦‚æœä½ æƒ³è®©æ—§æ¨¡å‹ä¹Ÿäº«å—çº¿æ€§è¡°å‡)
        # model.lr_schedule = linear_schedule(0.0003)

        reset_timesteps = False
    else:
        print(f"--- æœªå‘ç°å­˜æ¡£ï¼Œå¼€å§‹ã€ä»å¤´è®­ç»ƒã€‘(ä¼˜åŒ–ç‰ˆ) ---")
        model = PPO(
            "MultiInputPolicy",
            env,
            verbose=1,
            tensorboard_log=LOG_DIR,

            # --- æ ¸å¿ƒè¶…å‚æ•°ä¼˜åŒ– ---
            learning_rate=linear_schedule(0.0003),  # åŠ¨æ€å­¦ä¹ ç‡
            batch_size=256,  # å¤§æ‰¹é‡ï¼Œæ›´ç¨³
            n_steps=2048,  # æ¯æ¬¡æ›´æ–°å‰çš„é‡‡æ ·æ­¥æ•°
            gamma=0.99,  # æŠ˜æ‰£å› å­ (é•¿è¿œåˆ©ç›Š)
            gae_lambda=0.95,  # ä¼˜åŠ¿ä¼°è®¡
            clip_range=0.2,  # PPO ä¿®å‰ªèŒƒå›´
            ent_coef=0.01,  # ä¼˜åŒ– 3: ç†µç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢ï¼Œé˜²æ­¢è¿‡æ—©æ­»æ¿
            policy_kwargs=policy_kwargs,  # åº”ç”¨æ›´å¤§çš„ç½‘ç»œ
            device="auto"  # è‡ªåŠ¨ä½¿ç”¨ GPU
        )
        reset_timesteps = True

    checkpoint_callback = CheckpointCallback(
        save_freq=20000,  # æ¯ 2ä¸‡æ­¥ä¿å­˜ä¸€æ¬¡ï¼Œä¸ç”¨å¤ªé¢‘ç¹
        save_path=MODELS_DIR,
        name_prefix='drone_maze_opt'  # æ”¹ä¸ªåå­—åŒºåˆ†ä¸€ä¸‹
    )

    print("ğŸš€ ä¼˜åŒ–ç‰ˆè®­ç»ƒå¼•æ“å¯åŠ¨...")
    print("é…ç½®: Linear LR, Net=[256,256], Ent=0.01")

    model.learn(
        total_timesteps=1000000,  # å»ºè®®è·‘ 100ä¸‡æ­¥
        callback=checkpoint_callback,
        reset_num_timesteps=reset_timesteps
    )

    model.save(os.path.join(MODELS_DIR, "drone_maze_final_opt"))
    print("è®­ç»ƒç»“æŸã€‚")


if __name__ == "__main__":
    main()