import os
import torch as th
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from custom_env import AirSimMazeEnv  # å¯¼å…¥åˆšæ‰é‚£ä¸ªæ–‡ä»¶

# === è·¯å¾„é…ç½® (å·²æ”¹ä¸ºç›¸å¯¹è·¯å¾„) ===
# è·å–å½“å‰è„šæœ¬æ–‡ä»¶æ‰€åœ¨çš„ç»å¯¹ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# æ‹¼æ¥è·¯å¾„ï¼šåœ¨è„šæœ¬åŒçº§ç›®å½•ä¸‹ç”Ÿæˆ models å’Œ logs
MODELS_DIR = os.path.join(SCRIPT_DIR, "models")
LOG_DIR = os.path.join(SCRIPT_DIR, "logs")

os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


def main():
    # å®ä¾‹åŒ–ç¯å¢ƒ
    env = DummyVecEnv([lambda: AirSimMazeEnv()])

    # === ç½‘ç»œæ¶æ„é…ç½® ===
    # å› ä¸ºè¾“å…¥æ˜¯é›·è¾¾æ•°æ®(ä¸€ç»´æ•°å€¼)ï¼Œæ‰€ä»¥ä½¿ç”¨ MlpPolicy (å¤šå±‚æ„ŸçŸ¥æœº)
    policy_kwargs = dict(
        activation_fn=th.nn.Tanh,
        net_arch=dict(pi=[256, 256], vf=[256, 256])
    )

    print("ğŸš€ å¼€å§‹è®­ç»ƒ (ROS 2 ç‰ˆ)...")
    print(f"æ•°æ®ä¿å­˜è·¯å¾„: {SCRIPT_DIR}")

    # åˆå§‹åŒ– PPO æ¨¡å‹
    model = PPO(
        "MlpPolicy",  # å…³é”®ç‚¹ï¼šé›·è¾¾æ•°æ®å¿…é¡»ç”¨ MlpPolicy
        env,
        verbose=1,
        tensorboard_log=LOG_DIR,
        learning_rate=0.0003,
        batch_size=256,
        n_steps=2048,
        gamma=0.99,
        policy_kwargs=policy_kwargs,
        device="auto"
    )

    # è‡ªåŠ¨ä¿å­˜å›è°ƒ (æ¯ 10000 æ­¥ä¿å­˜ä¸€æ¬¡)
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path=MODELS_DIR,
        name_prefix='ros_drone'
    )

    # å¼€å§‹å­¦ä¹  (è®­ç»ƒ 10ä¸‡æ­¥è¯•è¯•)
    model.learn(
        total_timesteps=100000,
        callback=checkpoint_callback
    )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(os.path.join(MODELS_DIR, "ros_drone_final"))
    print("è®­ç»ƒç»“æŸã€‚")


if __name__ == "__main__":
    main()