# -*- coding: utf-8 -*-
"""
æ‰‹åŠ¿æ§åˆ¶AirSimæ— äººæœº - æ€§èƒ½æ¨¡å¼é€‰æ‹©ä¼˜åŒ–ç‰ˆ
æ–°å¢fastï¼ˆæœ€å¿«ï¼‰ã€balancedï¼ˆå¹³è¡¡ï¼‰ã€accurateï¼ˆæœ€å‡†ï¼‰ä¸‰ç§æ€§èƒ½æ¨¡å¼
ä½œè€…: xiaoshiyuan888
"""

import sys
import os
import time
import traceback
import json
import math
import threading
import tempfile
import pickle  # æ–°å¢ï¼šç”¨äºæ•°æ®åºåˆ—åŒ–
from datetime import datetime
from PIL import Image, ImageDraw, ImageFont
import cv2
import numpy as np
from collections import deque, Counter

print("=" * 60)
print("Gesture Controlled Drone - Performance Mode Selection")
print("æ€§èƒ½æ¨¡å¼é€‰æ‹©ä¼˜åŒ–ç‰ˆ!")
print("=" * 60)

# ========== ä¿®å¤å¯¼å…¥è·¯å¾„ ==========
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)


# ========== æ ¸å¿ƒæ¨¡å—å¯¼å…¥ ==========
def safe_import():
    """å®‰å…¨å¯¼å…¥æ‰€æœ‰æ¨¡å—"""
    modules_status = {}

    try:
        from PIL import Image, ImageDraw, ImageFont
        modules_status['PIL'] = True
        print("[PIL] âœ“ å›¾åƒå¤„ç†åº“å°±ç»ª")
    except Exception as e:
        modules_status['PIL'] = False
        print(f"[PIL] âœ— å¯¼å…¥å¤±è´¥: {e}")
        return None, modules_status

    try:
        import cv2
        import numpy as np
        modules_status['OpenCV'] = True
        print("[OpenCV] âœ“ è®¡ç®—æœºè§†è§‰åº“å°±ç»ª")
    except Exception as e:
        modules_status['OpenCV'] = False
        print(f"[OpenCV] âœ— å¯¼å…¥å¤±è´¥: {e}")
        return None, modules_status

    airsim_module = None
    try:
        airsim_module = __import__('airsim')
        modules_status['AirSim'] = True
        print(f"[AirSim] âœ“ æˆåŠŸå¯¼å…¥")
    except ImportError:
        print("\n" + "!" * 60)
        print("âš  AirSimåº“æœªæ‰¾åˆ°!")
        print("!" * 60)
        print("å®‰è£…AirSim:")
        print("1. é¦–å…ˆå®‰è£…: pip install msgpack-rpc-python")
        print("2. ç„¶åå®‰è£…: pip install airsim")
        print("\næˆ–ä»æºç å®‰è£…:")
        print("  pip install git+https://github.com/microsoft/AirSim.git")
        print("!" * 60)

        print("\næ— AirSimç»§ç»­è¿è¡Œ? (y/n)")
        choice = input().strip().lower()
        if choice != 'y':
            sys.exit(1)

    # å°è¯•å¯¼å…¥è¯­éŸ³åˆæˆåº“
    speech_module = None
    try:
        # å°è¯•å¯¼å…¥pyttsx3ï¼ˆç¦»çº¿TTSï¼‰
        import pyttsx3
        speech_module = pyttsx3
        modules_status['Speech'] = True
        print("[Speech] âœ“ pyttsx3è¯­éŸ³åº“å°±ç»ª (ç¦»çº¿)")
    except ImportError:
        print("\n" + "!" * 60)
        print("âš  pyttsx3è¯­éŸ³åº“æœªæ‰¾åˆ°!")
        print("!" * 60)
        print("å®‰è£…è¯­éŸ³åº“ (ä½¿ç”¨æ¸…åå¤§å­¦é•œåƒæº):")
        print("1. å®‰è£…ç¦»çº¿è¯­éŸ³åº“: pip install pyttsx3 -i https://pypi.tuna.tsinghua.edu.cn/simple")
        print("2. æˆ–è€…å®‰è£…åœ¨çº¿è¯­éŸ³åº“: pip install gtts pygame -i https://pypi.tuna.tsinghua.edu.cn/simple")
        print("!" * 60)

        # å°è¯•å…¶ä»–è¯­éŸ³åº“
        try:
            # å°è¯•ä½¿ç”¨gTTSï¼ˆéœ€è¦ç½‘ç»œï¼‰
            from gtts import gTTS
            speech_module = {'gTTS': gTTS, 'type': 'gtts'}
            modules_status['Speech'] = True
            print("[Speech] âœ“ gTTSè¯­éŸ³åº“å°±ç»ª (éœ€è¦ç½‘ç»œè¿æ¥)")

            # å°è¯•å¯¼å…¥éŸ³é¢‘æ’­æ”¾åº“
            try:
                import pygame
                pygame.mixer.init()
                speech_module['pygame'] = pygame
                print("[Speech] âœ“ pygameéŸ³é¢‘æ’­æ”¾åº“å°±ç»ª")
            except ImportError:
                # å°è¯•å…¶ä»–æ’­æ”¾æ–¹å¼
                try:
                    import pydub
                    from pydub import AudioSegment
                    from pydub.playback import play
                    speech_module['pydub'] = pydub
                    speech_module['AudioSegment'] = AudioSegment
                    speech_module['play'] = play
                    print("[Speech] âœ“ pydubéŸ³é¢‘æ’­æ”¾åº“å°±ç»ª")
                except ImportError:
                    # æœ€åå°è¯•ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤
                    if os.name == 'nt':  # Windows
                        speech_module['play_method'] = 'windows'
                        print("[Speech] âœ“ ä½¿ç”¨Windowsç³»ç»Ÿå‘½ä»¤æ’­æ”¾éŸ³é¢‘")
                    elif os.name == 'posix':  # Linux/Mac
                        speech_module['play_method'] = 'posix'
                        print("[Speech] âœ“ ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤æ’­æ”¾éŸ³é¢‘")
                    else:
                        print("[Speech] âœ— æ‰€æœ‰éŸ³é¢‘æ’­æ”¾åº“å¯¼å…¥å¤±è´¥ï¼Œè¯­éŸ³åŠŸèƒ½å°†ä¸å¯ç”¨")
                        speech_module = None
                        modules_status['Speech'] = False

        except ImportError:
            print("[Speech] âœ— æ‰€æœ‰è¯­éŸ³åº“å¯¼å…¥å¤±è´¥ï¼Œè¯­éŸ³åŠŸèƒ½å°†ä¸å¯ç”¨")
            speech_module = None
            modules_status['Speech'] = False

    return {
        'cv2': cv2,
        'np': np,
        'PIL': {'Image': Image, 'ImageDraw': ImageDraw, 'ImageFont': ImageFont},
        'airsim': airsim_module,
        'speech': speech_module
    }, modules_status


# æ‰§è¡Œå¯¼å…¥
libs, status = safe_import()
if not status.get('OpenCV', False) or not status.get('PIL', False):
    print("\nâŒ æ ¸å¿ƒåº“ç¼ºå¤±ï¼Œæ— æ³•å¯åŠ¨ã€‚")
    input("æŒ‰å›è½¦é”®é€€å‡º...")
    sys.exit(1)

print("-" * 60)
print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
print("-" * 60)

# è§£åŒ…åº“
cv2, np = libs['cv2'], libs['np']
Image, ImageDraw, ImageFont = libs['PIL']['Image'], libs['PIL']['ImageDraw'], libs['PIL']['ImageFont']


# ========== å¢å¼ºè¯­éŸ³åé¦ˆç®¡ç†å™¨ ==========
class EnhancedSpeechFeedbackManager:
    """å¢å¼ºçš„è¯­éŸ³åé¦ˆç®¡ç†å™¨"""

    def __init__(self, speech_lib):
        self.speech_lib = speech_lib
        self.enabled = True
        self.volume = 1.0
        self.rate = 150
        self.voice_id = None
        self.last_speech_time = {}
        self.min_interval = 1.5  # ç¼©çŸ­ç›¸åŒè¯­éŸ³çš„æœ€å°é—´éš”ï¼ˆç§’ï¼‰

        # è¯­éŸ³é˜Ÿåˆ—ï¼Œé¿å…è¯­éŸ³é‡å 
        self.speech_queue = []
        self.is_speaking = False
        self.queue_thread = None

        # éŸ³é¢‘æ’­æ”¾æ–¹æ³•
        self.audio_method = None

        # æ–°å¢ï¼šæ‰‹åŠ¿çŠ¶æ€è¿½è¸ª
        self.last_gesture_state = "none"  # è®°å½•ä¸Šæ¬¡æ‰‹åŠ¿çŠ¶æ€
        self.gesture_active_time = 0  # æ‰‹åŠ¿æŒç»­æ´»è·ƒæ—¶é—´

        # å¢å¼ºçš„è¯­éŸ³æ¶ˆæ¯æ˜ å°„
        self.messages = {
            # è¿æ¥ç›¸å…³
            'connecting': "æ­£åœ¨è¿æ¥æ— äººæœºï¼Œè¯·ç¨å€™",
            'connected': "æ— äººæœºè¿æ¥æˆåŠŸ",
            'connection_failed': "æ— äººæœºè¿æ¥å¤±è´¥ï¼Œè¿›å…¥æ¨¡æ‹Ÿæ¨¡å¼",

            # é£è¡Œç›¸å…³
            'taking_off': "æ— äººæœºæ­£åœ¨èµ·é£",
            'takeoff_success': "èµ·é£æˆåŠŸ",
            'takeoff_failed': "èµ·é£å¤±è´¥",
            'landing': "æ— äººæœºæ­£åœ¨é™è½",
            'land_success': "é™è½æˆåŠŸ",
            'emergency_stop': "ç´§æ€¥åœæ­¢ï¼Œæ— äººæœºå·²é™è½",
            'hovering': "æ— äººæœºæ‚¬åœä¸­",

            # æ‰‹åŠ¿ç›¸å…³ - å¢å¼º
            'gesture_detected': "æ‰‹åŠ¿è¯†åˆ«å°±ç»ªï¼Œè¯·å¼€å§‹æ‰‹åŠ¿",
            'gesture_start': "å¼€å§‹è¯†åˆ«æ‰‹åŠ¿",
            'gesture_end': "æ‰‹åŠ¿è¯†åˆ«ç»“æŸ",
            'gesture_stop': "åœæ­¢",
            'gesture_up': "å‘ä¸Š",
            'gesture_down': "å‘ä¸‹",
            'gesture_left': "å‘å·¦",
            'gesture_right': "å‘å³",
            'gesture_forward': "å‘å‰",
            'gesture_backward': "å‘å",
            'gesture_waiting': "ç­‰å¾…æ‰‹åŠ¿",
            'gesture_error': "æ‰‹åŠ¿è¯†åˆ«é”™è¯¯",
            'gesture_stable': "æ‰‹åŠ¿ç¨³å®š",
            'gesture_change': "æ‰‹åŠ¿å˜åŒ–",
            'gesture_low_confidence': "æ‰‹åŠ¿è¯†åˆ«ç½®ä¿¡åº¦ä½",
            'gesture_good_confidence': "æ‰‹åŠ¿è¯†åˆ«ç½®ä¿¡åº¦é«˜",
            'gesture_hover': "æ‚¬åœ",

            # ç³»ç»Ÿç›¸å…³
            'program_start': "æ‰‹åŠ¿æ§åˆ¶æ— äººæœºç³»ç»Ÿå·²å¯åŠ¨",
            'program_exit': "ç¨‹åºé€€å‡ºï¼Œæ„Ÿè°¢ä½¿ç”¨",
            'camera_error': "æ‘„åƒå¤´é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¿æ¥",
            'camera_ready': "æ‘„åƒå¤´å°±ç»ª",
            'system_ready': "ç³»ç»Ÿå‡†å¤‡å°±ç»ª",

            # æ¨¡å¼ç›¸å…³
            'simulation_mode': "è¿›å…¥æ¨¡æ‹Ÿæ¨¡å¼",
            'debug_mode_on': "è°ƒè¯•æ¨¡å¼å·²å¼€å¯",
            'debug_mode_off': "è°ƒè¯•æ¨¡å¼å·²å…³é—­",
            'display_mode_changed': "æ˜¾ç¤ºæ¨¡å¼å·²åˆ‡æ¢",
            'help_toggled': "å¸®åŠ©ä¿¡æ¯å·²åˆ‡æ¢",
            'performance_mode_fast': "åˆ‡æ¢åˆ°æœ€å¿«æ€§èƒ½æ¨¡å¼",
            'performance_mode_balanced': "åˆ‡æ¢åˆ°å¹³è¡¡æ€§èƒ½æ¨¡å¼",
            'performance_mode_accurate': "åˆ‡æ¢åˆ°æœ€å‡†ç¡®æ€§èƒ½æ¨¡å¼",

            # æ–°å¢ï¼šæ€§èƒ½ç›¸å…³
            'performance_good': "ç³»ç»Ÿè¿è¡Œæµç•…",
            'performance_warning': "ç³»ç»Ÿæ€§èƒ½è­¦å‘Š",

            # æ–°å¢ï¼šæ‰‹åŠ¿æŒ‡å¯¼
            'move_closer': "è¯·å°†æ‰‹é è¿‘æ‘„åƒå¤´",
            'move_away': "è¯·å°†æ‰‹ç§»è¿œä¸€äº›",
            'good_position': "æ‰‹éƒ¨ä½ç½®è‰¯å¥½",
            'hand_detected': "æ‰‹éƒ¨å·²æ£€æµ‹åˆ°",
            'hand_lost': "æ‰‹éƒ¨ä¸¢å¤±ï¼Œè¯·é‡æ–°æ”¾ç½®",

            # æ–°å¢ï¼šå½•åˆ¶ç›¸å…³
            'recording_start': "å¼€å§‹å½•åˆ¶æ‰‹åŠ¿è½¨è¿¹",
            'recording_stop': "åœæ­¢å½•åˆ¶",
            'recording_saved': "è½¨è¿¹å·²ä¿å­˜",
            'recording_loaded': "è½¨è¿¹å·²åŠ è½½",
            'recording_playback_start': "å¼€å§‹å›æ”¾æ‰‹åŠ¿è½¨è¿¹",
            'recording_playback_stop': "å›æ”¾ç»“æŸ",
            'recording_cleared': "è½¨è¿¹å·²æ¸…é™¤",
            'recording_paused': "å›æ”¾å·²æš‚åœ",
            'recording_resumed': "å›æ”¾ç»§ç»­",
            'recording_not_found': "æœªæ‰¾åˆ°è½¨è¿¹æ•°æ®",
            'recording_frame_count': "è½¨è¿¹å¸§æ•°",
        }

        # åˆå§‹åŒ–è¯­éŸ³å¼•æ“
        self.init_speech_engine()

    def init_speech_engine(self):
        """åˆå§‹åŒ–è¯­éŸ³å¼•æ“"""
        if self.speech_lib is None:
            print("âš  è¯­éŸ³åº“æœªæ‰¾åˆ°ï¼Œè¯­éŸ³åŠŸèƒ½ç¦ç”¨")
            self.enabled = False
            return

        try:
            if hasattr(self.speech_lib, 'init'):  # pyttsx3
                self.engine = self.speech_lib.init()
                self.audio_method = 'pyttsx3'

                # è®¾ç½®è¯­éŸ³å‚æ•°
                voices = self.engine.getProperty('voices')

                # å°è¯•å¯»æ‰¾ä¸­æ–‡è¯­éŸ³
                for voice in voices:
                    # æ£€æŸ¥è¯­éŸ³åç§°æ˜¯å¦åŒ…å«ä¸­æ–‡ç›¸å…³æ ‡è¯†
                    if 'chinese' in voice.name.lower() or 'zh' in voice.name.lower() or 'zh_CN' in voice.name.lower():
                        self.engine.setProperty('voice', voice.id)
                        self.voice_id = voice.id
                        print(f"[Speech] ä½¿ç”¨ä¸­æ–‡è¯­éŸ³: {voice.name}")
                        break

                # å¦‚æœæ²¡æ‰¾åˆ°ä¸­æ–‡è¯­éŸ³ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨è¯­éŸ³
                if self.voice_id is None and len(voices) > 0:
                    self.engine.setProperty('voice', voices[0].id)
                    print(f"[Speech] ä½¿ç”¨é»˜è®¤è¯­éŸ³: {voices[0].name}")

                # è®¾ç½®è¯­é€Ÿå’ŒéŸ³é‡
                self.engine.setProperty('rate', self.rate)
                self.engine.setProperty('volume', self.volume)

                print("âœ… è¯­éŸ³å¼•æ“åˆå§‹åŒ–æˆåŠŸ (pyttsx3)")

            elif isinstance(self.speech_lib, dict) and self.speech_lib.get('type') == 'gtts':
                print("âœ… è¯­éŸ³å¼•æ“åˆå§‹åŒ–æˆåŠŸ (gTTSï¼Œéœ€è¦ç½‘ç»œè¿æ¥)")
                self.audio_method = 'gtts'

                # ç¡®å®šæ’­æ”¾æ–¹æ³•
                if 'pygame' in self.speech_lib:
                    self.audio_method = 'gtts_pygame'
                    print("âœ… ä½¿ç”¨pygameæ’­æ”¾éŸ³é¢‘")
                elif 'pydub' in self.speech_lib:
                    self.audio_method = 'gtts_pydub'
                    print("âœ… ä½¿ç”¨pydubæ’­æ”¾éŸ³é¢‘")
                elif 'play_method' in self.speech_lib:
                    self.audio_method = f"gtts_{self.speech_lib['play_method']}"
                    print(f"âœ… ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤æ’­æ”¾éŸ³é¢‘")
                else:
                    self.audio_method = 'gtts_system'
                    print("âœ… ä½¿ç”¨é»˜è®¤ç³»ç»Ÿæ’­æ”¾å™¨")

            else:
                print("âš  æœªçŸ¥è¯­éŸ³åº“ç±»å‹ï¼Œè¯­éŸ³åŠŸèƒ½å¯èƒ½ä¸æ­£å¸¸")
                self.enabled = False

        except Exception as e:
            print(f"âš  è¯­éŸ³å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enabled = False

    def play_audio_file(self, audio_file):
        """æ’­æ”¾éŸ³é¢‘æ–‡ä»¶ï¼ˆæ ¹æ®å¯ç”¨åº“é€‰æ‹©æ–¹æ³•ï¼‰"""
        try:
            if self.audio_method == 'gtts_pygame' and 'pygame' in self.speech_lib:
                pygame = self.speech_lib['pygame']
                pygame.mixer.music.load(audio_file)
                pygame.mixer.music.play()

                # ç­‰å¾…æ’­æ”¾å®Œæˆ
                while pygame.mixer.music.get_busy():
                    pygame.time.Clock().tick(10)

            elif self.audio_method == 'gtts_pydub' and 'pydub' in self.speech_lib:
                AudioSegment = self.speech_lib['AudioSegment']
                play = self.speech_lib['play']

                audio = AudioSegment.from_mp3(audio_file)
                play(audio)

            elif self.audio_method == 'gtts_windows':
                # Windowsç³»ç»Ÿå‘½ä»¤
                os.startfile(audio_file)
                # ç­‰å¾…æ’­æ”¾å®Œæˆï¼ˆç®€å•ç­‰å¾…ï¼‰
                time.sleep(1.5)

            elif self.audio_method == 'gtts_posix':
                # Linux/Macç³»ç»Ÿå‘½ä»¤
                import subprocess
                if sys.platform == 'darwin':  # macOS
                    subprocess.call(['afplay', audio_file])
                else:  # Linux
                    subprocess.call(['xdg-open', audio_file])

            else:
                # é€šç”¨æ–¹æ³•ï¼šä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ’­æ”¾å™¨
                import subprocess
                if sys.platform == 'win32':
                    os.startfile(audio_file)
                elif sys.platform == 'darwin':
                    subprocess.call(['open', audio_file])
                else:
                    subprocess.call(['xdg-open', audio_file])

            return True

        except Exception as e:
            print(f"âš  éŸ³é¢‘æ’­æ”¾å¤±è´¥: {e}")
            return False

    def speak(self, message_key, force=False, immediate=False):
        """æ’­æ”¾è¯­éŸ³"""
        if not self.enabled:
            return

        # æ£€æŸ¥æ˜¯å¦åœ¨æœ€å°é—´éš”å†…
        current_time = time.time()
        if not force and message_key in self.last_speech_time:
            if current_time - self.last_speech_time[message_key] < self.min_interval:
                return

        # è·å–æ¶ˆæ¯æ–‡æœ¬
        if message_key in self.messages:
            text = self.messages[message_key]
        else:
            text = message_key  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ–‡æœ¬

        # å¦‚æœæ˜¯ç«‹å³æ’­æ”¾ï¼Œç›´æ¥åœ¨æ–°çº¿ç¨‹ä¸­æ’­æ”¾
        if immediate:
            self.speak_direct(text)
        else:
            # æ·»åŠ åˆ°è¯­éŸ³é˜Ÿåˆ—
            self.speech_queue.append(text)

            # å¦‚æœæ²¡æœ‰åœ¨æ’­æ”¾ï¼Œå¯åŠ¨æ’­æ”¾çº¿ç¨‹
            if not self.is_speaking and self.queue_thread is None:
                self.queue_thread = threading.Thread(target=self._process_speech_queue)
                self.queue_thread.daemon = True
                self.queue_thread.start()

        # æ›´æ–°æ—¶é—´æˆ³
        self.last_speech_time[message_key] = current_time

    def _process_speech_queue(self):
        """å¤„ç†è¯­éŸ³é˜Ÿåˆ—"""
        while self.speech_queue and self.enabled:
            self.is_speaking = True

            text = self.speech_queue.pop(0)

            try:
                if self.audio_method == 'pyttsx3':
                    # æ¸…é™¤ä¹‹å‰çš„è¯­éŸ³
                    self.engine.stop()
                    # æ’­æŠ¥æ–°è¯­éŸ³
                    self.engine.say(text)
                    self.engine.runAndWait()

                elif self.audio_method.startswith('gtts'):
                    # gTTSéœ€è¦ç½‘ç»œè¿æ¥
                    tts = self.speech_lib['gTTS'](text=text, lang='zh-cn')

                    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                    with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as f:
                        temp_file = f.name
                        tts.save(temp_file)

                    # æ’­æ”¾éŸ³é¢‘
                    self.play_audio_file(temp_file)

                    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.unlink(temp_file)
                    except:
                        pass  # å¿½ç•¥åˆ é™¤é”™è¯¯

            except Exception as e:
                print(f"âš  è¯­éŸ³æ’­æ”¾å¤±è´¥: {e}")

            time.sleep(0.05)  # å‡å°‘ç­‰å¾…æ—¶é—´ï¼Œé¿å…å¡é¡¿

        self.is_speaking = False
        self.queue_thread = None

    def speak_direct(self, text):
        """ç›´æ¥æ’­æ”¾æ–‡æœ¬ï¼ˆä¸é€šè¿‡æ¶ˆæ¯æ˜ å°„ï¼‰"""
        if not self.enabled:
            return

        # åœ¨æ–°çº¿ç¨‹ä¸­æ’­æ”¾
        thread = threading.Thread(target=self._speak_thread, args=(text,))
        thread.daemon = True
        thread.start()

    def _speak_thread(self, text):
        """è¯­éŸ³æ’­æ”¾çº¿ç¨‹"""
        try:
            if self.audio_method == 'pyttsx3':
                self.engine.say(text)
                self.engine.runAndWait()

            elif self.audio_method.startswith('gtts'):
                tts = self.speech_lib['gTTS'](text=text, lang='zh-cn')

                with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as f:
                    temp_file = f.name
                    tts.save(temp_file)

                self.play_audio_file(temp_file)

                try:
                    os.unlink(temp_file)
                except:
                    pass

        except Exception as e:
            print(f"âš  ç›´æ¥è¯­éŸ³æ’­æ”¾å¤±è´¥: {e}")

    def stop(self):
        """åœæ­¢æ‰€æœ‰è¯­éŸ³"""
        if hasattr(self, 'engine'):
            self.engine.stop()

        self.speech_queue.clear()
        self.is_speaking = False

    def set_enabled(self, enabled):
        """å¯ç”¨/ç¦ç”¨è¯­éŸ³"""
        self.enabled = enabled
        if not enabled:
            self.stop()

    def toggle_enabled(self):
        """åˆ‡æ¢è¯­éŸ³å¯ç”¨çŠ¶æ€"""
        self.enabled = not self.enabled
        status = "å¯ç”¨" if self.enabled else "ç¦ç”¨"
        self.speak_direct(f"è¯­éŸ³åé¦ˆå·²{status}")
        return self.enabled

    def get_status(self):
        """è·å–è¯­éŸ³çŠ¶æ€"""
        return {
            'enabled': self.enabled,
            'engine': 'pyttsx3' if self.audio_method == 'pyttsx3' else
            'gTTS' if self.audio_method.startswith('gtts') else
            'None',
            'queue_size': len(self.speech_queue),
            'is_speaking': self.is_speaking,
            'audio_method': self.audio_method
        }


# ========== æ‰‹åŠ¿è½¨è¿¹è®°å½•å™¨ ==========
class GestureTrajectoryRecorder:
    """æ‰‹åŠ¿è½¨è¿¹è®°å½•å™¨ - è®°å½•ã€ä¿å­˜ã€åŠ è½½å’Œå›æ”¾æ‰‹åŠ¿è½¨è¿¹"""

    def __init__(self, speech_manager=None):
        self.speech_manager = speech_manager
        self.trajectory_data = []
        self.is_recording = False
        self.is_playing = False
        self.playback_index = 0
        self.playback_paused = False
        self.max_trajectory_points = 1000  # æœ€å¤§è½¨è¿¹ç‚¹æ•°
        self.recording_start_time = 0
        self.last_save_time = 0
        self.save_interval = 5  # è‡ªåŠ¨ä¿å­˜é—´éš”ï¼ˆç§’ï¼‰

        # è½¨è¿¹æ–‡ä»¶è·¯å¾„
        self.trajectory_dir = os.path.join(current_dir, 'trajectories')
        if not os.path.exists(self.trajectory_dir):
            os.makedirs(self.trajectory_dir)

        # é»˜è®¤è½¨è¿¹æ–‡ä»¶å
        self.default_filename = os.path.join(self.trajectory_dir,
                                             f'trajectory_{datetime.now().strftime("%Y%m%d_%H%M%S")}.pkl')

        # è½¨è¿¹å¯è§†åŒ–è®¾ç½®
        self.trajectory_colors = [
            (255, 0, 0),  # çº¢è‰² - èµ·ç‚¹
            (0, 255, 0),  # ç»¿è‰² - æ­£å¸¸ç‚¹
            (0, 0, 255),  # è“è‰² - ç»ˆç‚¹
            (255, 255, 0),  # é’è‰² - ç‰¹æ®Šç‚¹
            (255, 0, 255)  # ç´«è‰² - ç‰¹æ®Šç‚¹
        ]

        self.show_trajectory = True
        self.trajectory_thickness = 2
        self.trajectory_max_length = 100  # æ˜¾ç¤ºçš„æœ€å¤§è½¨è¿¹é•¿åº¦

        print("âœ“ æ‰‹åŠ¿è½¨è¿¹è®°å½•å™¨å·²åˆå§‹åŒ–")

    def start_recording(self):
        """å¼€å§‹å½•åˆ¶æ‰‹åŠ¿è½¨è¿¹"""
        if self.is_recording:
            return False

        self.trajectory_data = []
        self.is_recording = True
        self.recording_start_time = time.time()
        self.last_save_time = time.time()

        print("ğŸ¬ å¼€å§‹å½•åˆ¶æ‰‹åŠ¿è½¨è¿¹")

        # è¯­éŸ³æç¤º
        if self.speech_manager and self.speech_manager.enabled:
            self.speech_manager.speak('recording_start', immediate=True)

        return True

    def stop_recording(self):
        """åœæ­¢å½•åˆ¶æ‰‹åŠ¿è½¨è¿¹"""
        if not self.is_recording:
            return False

        self.is_recording = False
        recording_duration = time.time() - self.recording_start_time

        print(f"â¹ï¸ åœæ­¢å½•åˆ¶æ‰‹åŠ¿è½¨è¿¹")
        print(f"   å½•åˆ¶æ—¶é•¿: {recording_duration:.1f}ç§’")
        print(f"   è½¨è¿¹ç‚¹æ•°: {len(self.trajectory_data)}")

        # è¯­éŸ³æç¤º
        if self.speech_manager and self.speech_manager.enabled:
            self.speech_manager.speak('recording_stop', immediate=True)
            if len(self.trajectory_data) > 0:
                self.speech_manager.speak_direct(f"å½•åˆ¶äº†{len(self.trajectory_data)}ä¸ªè½¨è¿¹ç‚¹")

        return True

    def add_trajectory_point(self, hand_data, gesture, confidence, frame_shape):
        """æ·»åŠ è½¨è¿¹ç‚¹"""
        if not self.is_recording or len(self.trajectory_data) >= self.max_trajectory_points:
            return False

        if hand_data is None:
            return False

        # åˆ›å»ºè½¨è¿¹ç‚¹æ•°æ®
        trajectory_point = {
            'timestamp': time.time(),
            'hand_position': hand_data['position'] if 'position' in hand_data else (0.5, 0.5),
            'hand_center': hand_data['center'] if 'center' in hand_data else (0, 0),
            'gesture': gesture,
            'confidence': confidence,
            'fingertips': hand_data.get('fingertips', []),
            'frame_shape': frame_shape
        }

        self.trajectory_data.append(trajectory_point)

        # è‡ªåŠ¨ä¿å­˜æ£€æŸ¥
        current_time = time.time()
        if current_time - self.last_save_time >= self.save_interval and len(self.trajectory_data) > 10:
            self.auto_save()
            self.last_save_time = current_time

        return True

    def auto_save(self):
        """è‡ªåŠ¨ä¿å­˜è½¨è¿¹ï¼ˆä¸´æ—¶æ–‡ä»¶ï¼‰"""
        if len(self.trajectory_data) == 0:
            return

        temp_file = os.path.join(self.trajectory_dir, 'trajectory_temp.pkl')
        try:
            with open(temp_file, 'wb') as f:
                pickle.dump(self.trajectory_data, f)
            print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜è½¨è¿¹åˆ°ä¸´æ—¶æ–‡ä»¶ ({len(self.trajectory_data)}ä¸ªç‚¹)")
        except Exception as e:
            print(f"âš  è‡ªåŠ¨ä¿å­˜è½¨è¿¹å¤±è´¥: {e}")

    def save_trajectory(self, filename=None):
        """ä¿å­˜è½¨è¿¹åˆ°æ–‡ä»¶"""
        if len(self.trajectory_data) == 0:
            print("âš  æ²¡æœ‰è½¨è¿¹æ•°æ®å¯ä¿å­˜")
            return False

        if filename is None:
            filename = self.default_filename

        try:
            with open(filename, 'wb') as f:
                pickle.dump(self.trajectory_data, f)

            print(f"ğŸ’¾ è½¨è¿¹å·²ä¿å­˜åˆ°: {filename}")
            print(f"   è½¨è¿¹ç‚¹æ•°: {len(self.trajectory_data)}")

            # è¯­éŸ³æç¤º
            if self.speech_manager and self.speech_manager.enabled:
                self.speech_manager.speak('recording_saved', immediate=True)
                self.speech_manager.speak_direct(f"ä¿å­˜äº†{len(self.trajectory_data)}ä¸ªè½¨è¿¹ç‚¹")

            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜è½¨è¿¹å¤±è´¥: {e}")
            return False

    def load_trajectory(self, filename):
        """ä»æ–‡ä»¶åŠ è½½è½¨è¿¹"""
        try:
            if not os.path.exists(filename):
                print(f"âŒ è½¨è¿¹æ–‡ä»¶ä¸å­˜åœ¨: {filename}")

                # è¯­éŸ³æç¤º
                if self.speech_manager and self.speech_manager.enabled:
                    self.speech_manager.speak('recording_not_found', immediate=True)

                return False

            with open(filename, 'rb') as f:
                self.trajectory_data = pickle.load(f)

            print(f"ğŸ“‚ è½¨è¿¹å·²ä»æ–‡ä»¶åŠ è½½: {filename}")
            print(f"   è½¨è¿¹ç‚¹æ•°: {len(self.trajectory_data)}")

            # è¯­éŸ³æç¤º
            if self.speech_manager and self.speech_manager.enabled:
                self.speech_manager.speak('recording_loaded', immediate=True)
                self.speech_manager.speak_direct(f"åŠ è½½äº†{len(self.trajectory_data)}ä¸ªè½¨è¿¹ç‚¹")

            return True
        except Exception as e:
            print(f"âŒ åŠ è½½è½¨è¿¹å¤±è´¥: {e}")
            return False

    def start_playback(self):
        """å¼€å§‹å›æ”¾è½¨è¿¹"""
        if len(self.trajectory_data) == 0:
            print("âš  æ²¡æœ‰è½¨è¿¹æ•°æ®å¯å›æ”¾")

            # è¯­éŸ³æç¤º
            if self.speech_manager and self.speech_manager.enabled:
                self.speech_manager.speak('recording_not_found', immediate=True)

            return False

        self.is_playing = True
        self.playback_index = 0
        self.playback_paused = False

        print(f"â–¶ï¸ å¼€å§‹å›æ”¾æ‰‹åŠ¿è½¨è¿¹")
        print(f"   æ€»å¸§æ•°: {len(self.trajectory_data)}")

        # è¯­éŸ³æç¤º
        if self.speech_manager and self.speech_manager.enabled:
            self.speech_manager.speak('recording_playback_start', immediate=True)

        return True

    def stop_playback(self):
        """åœæ­¢å›æ”¾è½¨è¿¹"""
        if not self.is_playing:
            return False

        self.is_playing = False
        self.playback_paused = False

        print("â¹ï¸ åœæ­¢å›æ”¾æ‰‹åŠ¿è½¨è¿¹")

        # è¯­éŸ³æç¤º
        if self.speech_manager and self.speech_manager.enabled:
            self.speech_manager.speak('recording_playback_stop', immediate=True)

        return True

    def pause_playback(self):
        """æš‚åœ/ç»§ç»­å›æ”¾"""
        self.playback_paused = not self.playback_paused

        status = "æš‚åœ" if self.playback_paused else "ç»§ç»­"
        print(f"â¸ï¸ å›æ”¾å·²{status}")

        # è¯­éŸ³æç¤º
        if self.speech_manager and self.speech_manager.enabled:
            if self.playback_paused:
                self.speech_manager.speak('recording_paused', immediate=True)
            else:
                self.speech_manager.speak('recording_resumed', immediate=True)

        return self.playback_paused

    def get_next_playback_point(self):
        """è·å–ä¸‹ä¸€ä¸ªå›æ”¾ç‚¹"""
        if not self.is_playing or self.playback_paused or len(self.trajectory_data) == 0:
            return None

        if self.playback_index >= len(self.trajectory_data):
            self.stop_playback()
            return None

        point = self.trajectory_data[self.playback_index]
        self.playback_index += 1

        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æœ«å°¾
        if self.playback_index >= len(self.trajectory_data):
            self.stop_playback()

        return point

    def clear_trajectory(self):
        """æ¸…é™¤è½¨è¿¹æ•°æ®"""
        self.trajectory_data = []
        self.is_recording = False
        self.is_playing = False
        self.playback_index = 0

        print("ğŸ—‘ï¸ è½¨è¿¹æ•°æ®å·²æ¸…é™¤")

        # è¯­éŸ³æç¤º
        if self.speech_manager and self.speech_manager.enabled:
            self.speech_manager.speak('recording_cleared', immediate=True)

        return True

    def draw_trajectory(self, frame):
        """åœ¨å¸§ä¸Šç»˜åˆ¶è½¨è¿¹"""
        if not self.show_trajectory or len(self.trajectory_data) == 0:
            return frame

        h, w = frame.shape[:2]

        # é™åˆ¶æ˜¾ç¤ºçš„è½¨è¿¹ç‚¹æ•°
        display_points = min(len(self.trajectory_data), self.trajectory_max_length)
        start_idx = max(0, len(self.trajectory_data) - display_points)

        # ç»˜åˆ¶è½¨è¿¹çº¿
        for i in range(start_idx, len(self.trajectory_data) - 1):
            point1 = self.trajectory_data[i]
            point2 = self.trajectory_data[i + 1]

            # è·å–æ‰‹éƒ¨ä¸­å¿ƒä½ç½®ï¼ˆè½¬æ¢ä¸ºå›¾åƒåæ ‡ï¼‰
            if 'hand_center' in point1 and 'hand_center' in point2:
                x1, y1 = point1['hand_center']
                x2, y2 = point2['hand_center']

                # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                x1 = max(0, min(w - 1, x1))
                y1 = max(0, min(h - 1, y1))
                x2 = max(0, min(w - 1, x2))
                y2 = max(0, min(h - 1, y2))

                # æ ¹æ®ç´¢å¼•è®¡ç®—é¢œè‰²ï¼ˆæ¸å˜è‰²ï¼‰
                color_idx = int((i - start_idx) / display_points * (len(self.trajectory_colors) - 1))
                color = self.trajectory_colors[color_idx]

                # ç»˜åˆ¶çº¿æ¡
                cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)),
                         color, self.trajectory_thickness)

        # ç»˜åˆ¶å½“å‰ç‚¹ï¼ˆå¦‚æœæ­£åœ¨å½•åˆ¶æˆ–å›æ”¾ï¼‰
        if self.is_recording or self.is_playing:
            current_idx = len(self.trajectory_data) - 1 if self.is_recording else self.playback_index - 1
            if 0 <= current_idx < len(self.trajectory_data):
                point = self.trajectory_data[current_idx]
                if 'hand_center' in point:
                    x, y = point['hand_center']
                    x = max(0, min(w - 1, x))
                    y = max(0, min(h - 1, y))

                    # ç»˜åˆ¶å½“å‰ç‚¹
                    cv2.circle(frame, (int(x), int(y)), 8, (0, 255, 255), -1)
                    cv2.circle(frame, (int(x), int(y)), 8, (0, 0, 0), 2)

        return frame

    def get_status(self):
        """è·å–å½•åˆ¶çŠ¶æ€"""
        return {
            'is_recording': self.is_recording,
            'is_playing': self.is_playing,
            'playback_paused': self.playback_paused,
            'trajectory_points': len(self.trajectory_data),
            'playback_index': self.playback_index,
            'playback_total': len(self.trajectory_data),
            'recording_duration': time.time() - self.recording_start_time if self.is_recording else 0
        }

    def list_saved_trajectories(self):
        """åˆ—å‡ºä¿å­˜çš„è½¨è¿¹æ–‡ä»¶"""
        try:
            files = [f for f in os.listdir(self.trajectory_dir) if f.endswith('.pkl')]
            return sorted(files, reverse=True)
        except:
            return []


# ========== é…ç½®ç®¡ç†å™¨ ==========
class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""

    def __init__(self):
        self.config_file = os.path.join(current_dir, 'gesture_config.json')

        # æ€§èƒ½æ¨¡å¼é…ç½®
        self.performance_modes = {
            'fast': {
                'name': 'æœ€å¿«',
                'description': 'æ€§èƒ½ä¼˜å…ˆï¼Œé™ä½è¯†åˆ«ç²¾åº¦æ¢å–æ›´é«˜å¸§ç‡',
                'detection_interval': 2,  # æ¯2å¸§æ£€æµ‹ä¸€æ¬¡
                'smooth_frames': 3,  # å¹³æ»‘å¸§æ•°
                'min_confidence': 0.5,  # æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
                'resize_factor': 0.5,  # å›¾åƒç¼©æ”¾å› å­
                'skin_detection_enabled': True,
                'background_subtraction_enabled': False,
                'contour_simplify_epsilon': 0.03,
                'history_size': 10,
                'gesture_stability_threshold': 3,
                'color': (0, 255, 0),  # ç»¿è‰²
            },
            'balanced': {
                'name': 'å¹³è¡¡',
                'description': 'å¹³è¡¡æ€§èƒ½ä¸ç²¾åº¦ï¼Œé€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯',
                'detection_interval': 1,  # æ¯å¸§æ£€æµ‹
                'smooth_frames': 5,  # å¹³æ»‘å¸§æ•°
                'min_confidence': 0.6,  # æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
                'resize_factor': 0.75,  # å›¾åƒç¼©æ”¾å› å­
                'skin_detection_enabled': True,
                'background_subtraction_enabled': True,
                'contour_simplify_epsilon': 0.02,
                'history_size': 15,
                'gesture_stability_threshold': 5,
                'color': (255, 165, 0),  # æ©™è‰²
            },
            'accurate': {
                'name': 'æœ€å‡†',
                'description': 'ç²¾åº¦ä¼˜å…ˆï¼Œæä¾›æœ€å‡†ç¡®çš„æ‰‹åŠ¿è¯†åˆ«',
                'detection_interval': 1,  # æ¯å¸§æ£€æµ‹
                'smooth_frames': 7,  # å¹³æ»‘å¸§æ•°
                'min_confidence': 0.7,  # æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
                'resize_factor': 1.0,  # ä¸ç¼©æ”¾
                'skin_detection_enabled': True,
                'background_subtraction_enabled': True,
                'contour_simplify_epsilon': 0.01,
                'history_size': 20,
                'gesture_stability_threshold': 7,
                'color': (255, 0, 0),  # çº¢è‰²
            }
        }

        self.default_config = {
            'camera': {
                'index': 0,
                'width': 640,
                'height': 480,
                'fps': 30
            },
            'gesture': {
                'skin_lower_h': 0,
                'skin_upper_h': 25,
                'skin_lower_s': 30,
                'skin_upper_s': 255,
                'skin_lower_v': 60,
                'skin_upper_v': 255,
                'min_hand_area': 2000,
                'max_hand_area': 30000,
                'hand_ratio_threshold': 1.5,
                'defect_distance_threshold': 20,
                'palm_circle_radius_ratio': 0.3,
                'transition_threshold': 0.3,
                'position_stability_weight': 0.4,
                'gesture_cooldown': 0.5,
            },
            'drone': {
                'velocity': 2.5,
                'duration': 0.3,
                'altitude': -10.0,
                'control_interval': 0.3
            },
            'display': {
                'show_fps': True,
                'show_confidence': True,
                'show_help': True,
                'show_contours': True,
                'show_bbox': True,
                'show_fingertips': True,
                'show_palm_center': True,
                'show_hand_direction': True,
                'show_debug_info': False,
                'show_speech_status': True,
                'show_gesture_history': True,
                'show_stability_indicator': True,
                'show_trajectory': True,
                'show_recording_status': True,
                'show_performance_mode': True,  # æ–°å¢ï¼šæ˜¾ç¤ºæ€§èƒ½æ¨¡å¼
            },
            'performance': {
                'target_fps': 30,
                'resize_factor': 1.0,
                'enable_multiprocessing': False,
                'mode': 'balanced',  # æ–°å¢ï¼šæ€§èƒ½æ¨¡å¼é€‰æ‹©
                'current_mode_index': 1,  # å½“å‰æ¨¡å¼ç´¢å¼•
                'modes': ['fast', 'balanced', 'accurate'],  # å¯ç”¨æ¨¡å¼åˆ—è¡¨
            },
            'calibration': {
                'auto_calibrate_skin': True,
                'skin_calibration_frames': 30,
                'hand_size_calibration': True
            },
            'speech': {
                'enabled': True,
                'volume': 1.0,
                'rate': 150,
                'announce_gestures': True,
                'announce_connections': True,
                'announce_flight_events': True,
                'announce_gesture_changes': True,
                'announce_hand_status': True,
                'announce_performance': True,
                'announce_recording_events': True,
                'announce_performance_mode': True,  # æ–°å¢ï¼šæ’­æŠ¥æ€§èƒ½æ¨¡å¼åˆ‡æ¢
                'min_gesture_confidence': 0.7,
                'gesture_start_threshold': 3,
                'gesture_end_threshold': 10,
            },
            'recording': {
                'auto_save_interval': 5,
                'max_trajectory_points': 1000,
                'show_trajectory': True,
                'trajectory_thickness': 2,
                'trajectory_max_length': 100,
                'default_save_dir': 'trajectories',
            }
        }
        self.config = self.load_config()
        self.skin_calibration_data = []
        self.hand_size_calibration_done = False
        self.reference_hand_size = 0

    def load_config(self):
        """åŠ è½½é…ç½®"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)
                    config = self.default_config.copy()
                    self._merge_config(config, loaded_config)
                    print("âœ“ ä»æ–‡ä»¶åŠ è½½é…ç½®")
                    return config
            except Exception as e:
                print(f"âš  åŠ è½½é…ç½®å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é…ç½®")
                return self.default_config.copy()
        else:
            print("âœ“ ä½¿ç”¨é»˜è®¤é…ç½®")
            return self.default_config.copy()

    def _merge_config(self, base, update):
        """é€’å½’åˆå¹¶é…ç½®"""
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._merge_config(base[key], value)
            else:
                base[key] = value

    def save_config(self):
        """ä¿å­˜é…ç½®"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            print("âœ“ é…ç½®å·²ä¿å­˜")
        except Exception as e:
            print(f"âš  ä¿å­˜é…ç½®å¤±è´¥: {e}")

    def get(self, *keys):
        """è·å–é…ç½®å€¼"""
        value = self.config
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return None
        return value

    def set(self, *keys, value):
        """è®¾ç½®é…ç½®å€¼"""
        if len(keys) == 0:
            return

        config = self.config
        for key in keys[:-1]:
            if key not in config:
                config[key] = {}
            config = config[key]

        config[keys[-1]] = value
        self.save_config()

    def get_performance_mode_config(self, mode=None):
        """è·å–æ€§èƒ½æ¨¡å¼é…ç½®"""
        if mode is None:
            mode = self.get('performance', 'mode')

        if mode in self.performance_modes:
            return self.performance_modes[mode]
        else:
            return self.performance_modes['balanced']

    def get_current_performance_mode(self):
        """è·å–å½“å‰æ€§èƒ½æ¨¡å¼"""
        mode = self.get('performance', 'mode')
        if mode in self.performance_modes:
            return mode
        return 'balanced'

    def set_performance_mode(self, mode):
        """è®¾ç½®æ€§èƒ½æ¨¡å¼"""
        if mode in self.performance_modes:
            self.set('performance', 'mode', value=mode)

            # æ›´æ–°å½“å‰æ¨¡å¼ç´¢å¼•
            modes = self.get('performance', 'modes')
            if modes and mode in modes:
                index = modes.index(mode)
                self.set('performance', 'current_mode_index', value=index)

            print(f"âœ“ æ€§èƒ½æ¨¡å¼è®¾ç½®ä¸º: {self.performance_modes[mode]['name']}")
            return True
        return False

    def cycle_performance_mode(self):
        """å¾ªç¯åˆ‡æ¢æ€§èƒ½æ¨¡å¼"""
        modes = self.get('performance', 'modes')
        if not modes:
            modes = ['fast', 'balanced', 'accurate']

        current_index = self.get('performance', 'current_mode_index')
        if current_index is None:
            current_index = 0

        # è®¡ç®—ä¸‹ä¸€ä¸ªæ¨¡å¼ç´¢å¼•
        next_index = (current_index + 1) % len(modes)
        next_mode = modes[next_index]

        # è®¾ç½®æ–°æ¨¡å¼
        self.set('performance', 'current_mode_index', value=next_index)
        return self.set_performance_mode(next_mode)

    def calibrate_skin_color(self, frame, hand_mask):
        """è‡ªåŠ¨æ ¡å‡†è‚¤è‰²èŒƒå›´"""
        if not self.get('calibration', 'auto_calibrate_skin'):
            return

        if len(self.skin_calibration_data) < self.get('calibration', 'skin_calibration_frames'):
            # è½¬æ¢åˆ°HSV
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

            # è·å–è‚¤è‰²åŒºåŸŸçš„HSVå€¼
            skin_pixels = hsv[hand_mask > 0]

            if len(skin_pixels) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åƒç´ 
                self.skin_calibration_data.append(skin_pixels)

        if len(self.skin_calibration_data) == self.get('calibration', 'skin_calibration_frames'):
            # è®¡ç®—è‚¤è‰²èŒƒå›´
            all_skin_pixels = np.vstack(self.skin_calibration_data)

            h_min, h_max = np.percentile(all_skin_pixels[:, 0], [2, 98])
            s_min, s_max = np.percentile(all_skin_pixels[:, 1], [2, 98])
            v_min, v_max = np.percentile(all_skin_pixels[:, 2], [2, 98])

            # æ›´æ–°é…ç½®
            self.set('gesture', 'skin_lower_h', value=int(max(0, h_min - 5)))
            self.set('gesture', 'skin_upper_h', value=int(min(180, h_max + 5)))
            self.set('gesture', 'skin_lower_s', value=int(max(0, s_min - 10)))
            self.set('gesture', 'skin_upper_s', value=int(min(255, s_max + 10)))
            self.set('gesture', 'skin_lower_v', value=int(max(0, v_min - 10)))
            self.set('gesture', 'skin_upper_v', value=int(min(255, v_max + 10)))

            print("âœ“ è‚¤è‰²æ ¡å‡†å®Œæˆ")
            print(f"  è‚¤è‰²èŒƒå›´: H[{self.get('gesture', 'skin_lower_h')}-{self.get('gesture', 'skin_upper_h')}], "
                  f"S[{self.get('gesture', 'skin_lower_s')}-{self.get('gesture', 'skin_upper_s')}], "
                  f"V[{self.get('gesture', 'skin_lower_v')}-{self.get('gesture', 'skin_upper_v')}]")

    def calibrate_hand_size(self, hand_area):
        """æ ¡å‡†æ‰‹éƒ¨å¤§å°"""
        if not self.get('calibration', 'hand_size_calibration') or self.hand_size_calibration_done:
            return

        if hand_area > 0:
            self.reference_hand_size = hand_area
            self.hand_size_calibration_done = True
            print(f"âœ“ æ‰‹éƒ¨å¤§å°æ ¡å‡†å®Œæˆ: {self.reference_hand_size:.0f} åƒç´ ")


config = ConfigManager()


# ========== æ”¹è¿›çš„æ‰‹åŠ¿è¯†åˆ«å™¨ï¼ˆæ”¯æŒæ€§èƒ½æ¨¡å¼ï¼‰ ==========
class EnhancedGestureRecognizer:
    """å¢å¼ºçš„æ‰‹åŠ¿è¯†åˆ«å™¨ - æ”¯æŒæ€§èƒ½æ¨¡å¼é€‰æ‹©"""

    def __init__(self, speech_manager=None):
        self.speech_manager = speech_manager

        # åŠ è½½æ€§èƒ½æ¨¡å¼é…ç½®
        self.performance_mode = config.get_current_performance_mode()
        self.mode_config = config.get_performance_mode_config(self.performance_mode)

        # æ ¹æ®æ€§èƒ½æ¨¡å¼åˆå§‹åŒ–å‚æ•°
        self.history_size = self.mode_config['history_size']
        self.detection_interval = self.mode_config['detection_interval']
        self.smooth_frames = self.mode_config['smooth_frames']
        self.min_confidence = self.mode_config['min_confidence']
        self.resize_factor = self.mode_config['resize_factor']

        # å¢å¼ºçš„æ‰‹åŠ¿å†å²å’Œå¹³æ»‘
        self.gesture_history = deque(maxlen=self.history_size)
        self.confidence_history = deque(maxlen=self.history_size)
        self.position_history = deque(maxlen=self.history_size)
        self.current_gesture = "Waiting"
        self.current_confidence = 0.0

        # æ–°å¢ï¼šæ‰‹åŠ¿çŠ¶æ€è¿½è¸ª
        self.gesture_state = "none"
        self.gesture_stability_counter = 0
        self.last_stable_gesture = "Waiting"
        self.gesture_active_frames = 0
        self.last_gesture_change_time = 0

        # è®°å½•ä¸Šæ¬¡æ’­æŠ¥çš„æ‰‹åŠ¿
        self.last_announced_gesture = None
        self.last_announced_time = 0
        self.last_hand_status_time = 0
        self.gesture_announce_interval = 2.0

        # æ‰‹éƒ¨è·Ÿè¸ªå’ŒçŠ¶æ€
        self.last_hand_position = None
        self.hand_tracking = False
        self.track_window = None
        self.hand_states = deque(maxlen=15)
        self.hand_detected_frames = 0
        self.hand_lost_frames = 0

        # æ€§èƒ½ç»Ÿè®¡
        self.process_times = deque(maxlen=30)
        self.frame_counter = 0
        self.last_performance_report = 0

        # æ‰‹åŠ¿é¢œè‰²æ˜ å°„
        self.gesture_colors = {
            "Stop": (0, 0, 255),
            "Forward": (0, 255, 0),
            "Up": (255, 255, 0),
            "Down": (255, 0, 255),
            "Left": (255, 165, 0),
            "Right": (0, 165, 255),
            "Waiting": (200, 200, 200),
            "Error": (255, 0, 0),
            "Hover": (255, 255, 255)
        }

        # æ‰‹åŠ¿åˆ°è¯­éŸ³çš„æ˜ å°„
        self.gesture_speech_map = {
            "Stop": "gesture_stop",
            "Forward": "gesture_forward",
            "Up": "gesture_up",
            "Down": "gesture_down",
            "Left": "gesture_left",
            "Right": "gesture_right",
            "Waiting": "gesture_waiting",
            "Error": "gesture_error",
            "Hover": "gesture_hover",
        }

        # æ‰‹åŠ¿çŠ¶æ€é¢œè‰²
        self.state_colors = {
            "none": (100, 100, 100),
            "starting": (255, 165, 0),
            "active": (0, 255, 0),
            "ending": (255, 0, 0),
        }

        # æ ¹æ®æ€§èƒ½æ¨¡å¼åˆå§‹åŒ–èƒŒæ™¯å‡é™¤å™¨
        self.bg_subtractor = None
        if self.mode_config['background_subtraction_enabled']:
            self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
                history=100, varThreshold=25, detectShadows=True
            )

        # å½¢æ€å­¦æ“ä½œæ ¸
        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))

        # æ€§èƒ½ç›‘æ§
        self.avg_process_time = 0
        self.frame_rate = 0
        self.last_fps_check = time.time()

        # å­˜å‚¨æ‰‹éƒ¨æ•°æ®ç”¨äºè½¨è¿¹è®°å½•
        self.last_hand_data = None

        # æ€§èƒ½æ¨¡å¼ä¿¡æ¯
        self.performance_mode_color = self.mode_config['color']
        self.performance_mode_name = self.mode_config['name']

        print(f"âœ“ å¢å¼ºçš„æ‰‹åŠ¿è¯†åˆ«å™¨å·²åˆå§‹åŒ– - æ€§èƒ½æ¨¡å¼: {self.performance_mode_name}")

    def set_performance_mode(self, mode):
        """è®¾ç½®æ€§èƒ½æ¨¡å¼"""
        self.performance_mode = mode
        self.mode_config = config.get_performance_mode_config(mode)

        # æ›´æ–°å‚æ•°
        self.history_size = self.mode_config['history_size']
        self.detection_interval = self.mode_config['detection_interval']
        self.smooth_frames = self.mode_config['smooth_frames']
        self.min_confidence = self.mode_config['min_confidence']
        self.resize_factor = self.mode_config['resize_factor']

        # æ›´æ–°èƒŒæ™¯å‡é™¤å™¨
        if self.mode_config['background_subtraction_enabled'] and self.bg_subtractor is None:
            self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
                history=100, varThreshold=25, detectShadows=True
            )
        elif not self.mode_config['background_subtraction_enabled']:
            self.bg_subtractor = None

        # æ›´æ–°é˜Ÿåˆ—å¤§å°
        self.gesture_history = deque(maxlen=self.history_size)
        self.confidence_history = deque(maxlen=self.history_size)
        self.position_history = deque(maxlen=self.history_size)

        # æ›´æ–°æ˜¾ç¤ºä¿¡æ¯
        self.performance_mode_color = self.mode_config['color']
        self.performance_mode_name = self.mode_config['name']

        print(f"âœ“ åˆ‡æ¢åˆ°æ€§èƒ½æ¨¡å¼: {self.performance_mode_name}")

    def get_skin_mask(self, frame):
        """è·å–è‚¤è‰²æ©ç """
        h_low = config.get('gesture', 'skin_lower_h')
        h_high = config.get('gesture', 'skin_upper_h')
        s_low = config.get('gesture', 'skin_lower_s')
        s_high = config.get('gesture', 'skin_upper_s')
        v_low = config.get('gesture', 'skin_lower_v')
        v_high = config.get('gesture', 'skin_upper_v')

        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        lower_skin = np.array([h_low, s_low, v_low], dtype=np.uint8)
        upper_skin = np.array([h_high, s_high, v_high], dtype=np.uint8)
        skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)

        return skin_mask, hsv

    def enhance_skin_detection(self, frame, skin_mask):
        """å¢å¼ºè‚¤è‰²æ£€æµ‹"""
        if not self.mode_config['skin_detection_enabled']:
            return skin_mask

        if self.bg_subtractor is not None:
            fg_mask = self.bg_subtractor.apply(frame)
            combined_mask = cv2.bitwise_and(skin_mask, fg_mask)
        else:
            combined_mask = skin_mask

        # æ ¹æ®æ€§èƒ½æ¨¡å¼å†³å®šå½¢æ€å­¦æ“ä½œæ¬¡æ•°
        if self.performance_mode == 'accurate':
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, self.kernel, iterations=2)
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, self.kernel, iterations=1)
            combined_mask = cv2.GaussianBlur(combined_mask, (5, 5), 0)
        elif self.performance_mode == 'balanced':
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, self.kernel, iterations=1)
            combined_mask = cv2.GaussianBlur(combined_mask, (3, 3), 0)
        # fastæ¨¡å¼ä¸è¿›è¡Œé¢å¤–çš„å½¢æ€å­¦æ“ä½œ

        return combined_mask

    def preprocess_frame(self, frame):
        """é¢„å¤„ç†å¸§ï¼ˆæ ¹æ®æ€§èƒ½æ¨¡å¼è°ƒæ•´å¤§å°ï¼‰"""
        if self.resize_factor != 1.0:
            new_width = int(frame.shape[1] * self.resize_factor)
            new_height = int(frame.shape[0] * self.resize_factor)
            resized_frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)
            return resized_frame
        return frame

    def find_best_hand_contour(self, mask, frame):
        """æ‰¾åˆ°æœ€ä½³çš„æ‰‹éƒ¨è½®å»“"""
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if not contours:
            return None, 0.0

        contours = sorted(contours, key=cv2.contourArea, reverse=True)
        best_contour = None
        best_score = 0.0
        min_area = config.get('gesture', 'min_hand_area') * (self.resize_factor ** 2)
        max_area = config.get('gesture', 'max_hand_area') * (self.resize_factor ** 2)

        # æ ¹æ®æ€§èƒ½æ¨¡å¼è°ƒæ•´æ£€æŸ¥çš„è½®å»“æ•°é‡
        max_contours = 3 if self.performance_mode != 'fast' else 1

        for contour in contours[:max_contours]:
            area = cv2.contourArea(contour)

            if area < min_area or area > max_area:
                continue

            score = self.rate_contour(contour, frame.shape)

            if score > best_score:
                best_score = score
                best_contour = contour

        return best_contour, best_score

    def rate_contour(self, contour, frame_shape):
        """è¯„ä¼°è½®å»“ä½œä¸ºæ‰‹éƒ¨çš„å¯èƒ½æ€§"""
        score = 0.0
        area = cv2.contourArea(contour)
        min_area = config.get('gesture', 'min_hand_area') * (self.resize_factor ** 2)
        max_area = config.get('gesture', 'max_hand_area') * (self.resize_factor ** 2)

        if min_area < area < max_area:
            area_ratio = min(area / max_area, 1.0)
            score += area_ratio * 0.3

        perimeter = cv2.arcLength(contour, True)
        if perimeter > 100:
            score += 0.2

        if area > 0:
            compactness = perimeter ** 2 / area
            if 12 < compactness < 25:
                compactness_score = 1.0 - abs(compactness - 18) / 6
                score += compactness_score * 0.3

        x, y, w, h = cv2.boundingRect(contour)
        if h > 0:
            aspect_ratio = w / h
            if 0.4 < aspect_ratio < 2.5:
                aspect_score = 1.0 - abs(aspect_ratio - 1.0) / 1.5
                score += aspect_score * 0.2

        return score

    def analyze_hand_features(self, contour, frame_shape):
        """åˆ†ææ‰‹éƒ¨ç‰¹å¾"""
        if contour is None:
            return None, 0.0

        area = cv2.contourArea(contour)
        M = cv2.moments(contour)
        if M["m00"] == 0:
            return None, 0.0

        cx = int(M["m10"] / M["m00"])
        cy = int(M["m01"] / M["m00"])
        x, y, w, h = cv2.boundingRect(contour)
        bbox_area = w * h
        palm_center = (cx, cy)
        palm_radius = int(w * config.get('gesture', 'palm_circle_radius_ratio'))

        # æ ¹æ®æ€§èƒ½æ¨¡å¼è°ƒæ•´è½®å»“ç®€åŒ–ç¨‹åº¦
        epsilon = self.mode_config['contour_simplify_epsilon'] * cv2.arcLength(contour, True)
        fingers, fingertips, defects = self.analyze_fingers(contour, palm_center, palm_radius, epsilon)

        direction = self.calculate_hand_direction(contour, cx, cy)
        h_img, w_img = frame_shape[:2]
        norm_x = cx / w_img
        norm_y = cy / h_img
        confidence = self.calculate_confidence(area, len(fingers), len(contour), bbox_area)

        result = {
            'contour': contour,
            'center': (cx, cy),
            'bbox': (x, y, x + w, y + h),
            'fingers': fingers,
            'fingertips': fingertips,
            'defects': defects,
            'palm_center': palm_center,
            'palm_radius': palm_radius,
            'direction': direction,
            'area': area,
            'position': (norm_x, norm_y),
            'bbox_size': (w, h),
            'confidence': confidence
        }

        return result, confidence

    def analyze_fingers(self, contour, palm_center, palm_radius, epsilon):
        """åˆ†ææ‰‹æŒ‡"""
        approx = cv2.approxPolyDP(contour, epsilon, True)
        hull = cv2.convexHull(approx, returnPoints=False)

        if hull is None or len(hull) < 3:
            return [], [], []

        defects = cv2.convexityDefects(approx, hull)
        fingers = []
        fingertips = []
        defect_points = []

        if defects is not None:
            for i in range(defects.shape[0]):
                s, e, f, d = defects[i, 0]
                start = tuple(approx[s][0])
                end = tuple(approx[e][0])
                far = tuple(approx[f][0])

                start_dist = np.linalg.norm(np.array(start) - np.array(palm_center))
                end_dist = np.linalg.norm(np.array(end) - np.array(palm_center))
                far_dist = np.linalg.norm(np.array(far) - np.array(palm_center))

                if (start_dist > palm_radius * 1.2 and
                        end_dist > palm_radius * 1.2 and
                        d > config.get('gesture', 'defect_distance_threshold') * 256):

                    a = math.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)
                    b = math.sqrt((far[0] - start[0]) ** 2 + (far[1] - start[1]) ** 2)
                    c = math.sqrt((end[0] - far[0]) ** 2 + (end[1] - far[1]) ** 2)

                    if b * c != 0:
                        angle = math.acos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c))
                        angle_degrees = math.degrees(angle)

                        if angle_degrees < 90:
                            finger = {
                                'start': start,
                                'end': end,
                                'far': far,
                                'depth': d,
                                'angle': angle_degrees
                            }
                            fingers.append(finger)

                            for point in [start, end]:
                                if point not in fingertips:
                                    point_dist = np.linalg.norm(np.array(point) - np.array(palm_center))
                                    if point_dist > palm_radius * 1.5:
                                        fingertips.append(point)

                            defect_points.append((start, end, far, d))

        if len(fingertips) == 0:
            hull_points = cv2.convexHull(approx, returnPoints=True)
            if len(hull_points) > 0:
                hull_points = hull_points.reshape(-1, 2)

                for point in hull_points:
                    point_tuple = tuple(point)
                    point_dist = np.linalg.norm(point - np.array(palm_center))
                    if point_dist > palm_radius * 1.5 and point_tuple not in fingertips:
                        fingertips.append(point_tuple)

        fingertips = fingertips[:5]

        return fingers, fingertips, defect_points

    def calculate_hand_direction(self, contour, cx, cy):
        """è®¡ç®—æ‰‹éƒ¨æ–¹å‘"""
        if len(contour) < 5:
            return 0.0

        points = contour.reshape(-1, 2).astype(np.float32)
        mean = np.empty((0))
        mean, eigenvectors, eigenvalues = cv2.PCACompute2(points, mean)
        direction = math.degrees(math.atan2(eigenvectors[0, 1], eigenvectors[0, 0]))

        return direction

    def calculate_confidence(self, area, finger_count, contour_length, bbox_area):
        """è®¡ç®—æ‰‹åŠ¿ç½®ä¿¡åº¦"""
        confidence = 0.5
        min_area = config.get('gesture', 'min_hand_area') * (self.resize_factor ** 2)
        max_area = config.get('gesture', 'max_hand_area') * (self.resize_factor ** 2)

        if min_area < area < max_area:
            area_norm = (area - min_area) / (max_area - min_area)
            confidence += area_norm * 0.2

        if 0 <= finger_count <= 5:
            confidence += 0.2

        if contour_length > 200:
            confidence += 0.1

        if bbox_area > 0:
            fill_ratio = area / bbox_area
            if 0.2 < fill_ratio < 0.8:
                fill_score = 1.0 - abs(fill_ratio - 0.5) / 0.3
                confidence += fill_score * 0.1

        return min(confidence, 1.0)

    def recognize_gesture_improved(self, hand_data):
        """æ”¹è¿›çš„æ‰‹åŠ¿è¯†åˆ«é€»è¾‘"""
        if hand_data is None:
            return "Waiting", 0.3

        finger_count = len(hand_data.get('fingers', []))
        fingertips = hand_data.get('fingertips', [])
        norm_x, norm_y = hand_data['position']
        direction = hand_data.get('direction', 0.0)
        confidence = hand_data['confidence']
        w, h = hand_data['bbox_size']
        aspect_ratio = w / h if h > 0 else 1.0

        # æ ¹æ®æ‰‹æŒ‡æ•°é‡åˆ†ç±»
        if finger_count == 0:
            if len(fingertips) == 0:
                return "Stop", confidence * 0.9
            else:
                return "Stop", confidence * 0.7

        elif finger_count == 1:
            if len(fingertips) >= 1:
                cx, cy = hand_data['center']
                fingertip = fingertips[0]
                dx = fingertip[0] - cx
                dy = fingertip[1] - cy

                if abs(dx) > abs(dy):
                    if dx > 0:
                        return "Right", confidence * 0.8
                    else:
                        return "Left", confidence * 0.8
                else:
                    if dy < 0:
                        return "Up", confidence * 0.8
                    else:
                        return "Forward", confidence * 0.8
            return "Forward", confidence * 0.7

        elif finger_count == 2:
            return "Forward", confidence * 0.7

        elif finger_count == 3:
            if -45 <= direction <= 45:
                if direction > 0:
                    return "Right", confidence * 0.7
                else:
                    return "Left", confidence * 0.7
            else:
                if direction > 0:
                    return "Down", confidence * 0.7
                else:
                    return "Up", confidence * 0.7

        elif finger_count >= 4:
            if norm_x < 0.4:
                return "Left", confidence * 0.8
            elif norm_x > 0.6:
                return "Right", confidence * 0.8
            elif norm_y < 0.4:
                return "Up", confidence * 0.8
            elif norm_y > 0.6:
                return "Down", confidence * 0.8
            else:
                if -45 <= direction <= 45:
                    return "Forward", confidence * 0.7
                else:
                    return "Stop", confidence * 0.7

        return "Waiting", confidence * 0.5

    def smooth_gesture_enhanced(self, new_gesture, new_confidence, hand_data):
        """å¢å¼ºçš„æ‰‹åŠ¿å¹³æ»‘å¤„ç†"""
        current_time = time.time()

        # æ£€æŸ¥æ‰‹åŠ¿å†·å´æ—¶é—´
        if current_time - self.last_gesture_change_time < config.get('gesture', 'gesture_cooldown'):
            return self.current_gesture, self.current_confidence

        # æ·»åŠ åˆ°å†å²
        self.gesture_history.append(new_gesture)
        self.confidence_history.append(new_confidence)

        if hand_data is not None:
            self.position_history.append(hand_data['position'])

        # è®¡ç®—æ‰‹åŠ¿ç¨³å®šæ€§
        if len(self.gesture_history) >= 3:
            # æ£€æŸ¥æœ€è¿‘Nä¸ªæ‰‹åŠ¿æ˜¯å¦ä¸€è‡´
            recent_gestures = list(self.gesture_history)[-3:]
            gesture_counter = Counter(recent_gestures)
            most_common_gesture, most_common_count = gesture_counter.most_common(1)[0]

            # è®¡ç®—ä½ç½®ç¨³å®šæ€§
            position_stability = 1.0
            if len(self.position_history) >= 2 and hand_data is not None:
                current_pos = hand_data['position']
                prev_pos = self.position_history[-2] if len(self.position_history) >= 2 else current_pos
                position_diff = math.sqrt((current_pos[0] - prev_pos[0]) ** 2 + (current_pos[1] - prev_pos[1]) ** 2)
                position_stability = max(0, 1.0 - position_diff * 5.0)

            # å¢å¼ºçš„ç¨³å®šæ€§æ£€æŸ¥
            stability_threshold = self.mode_config['gesture_stability_threshold']
            transition_threshold = config.get('gesture', 'transition_threshold')
            position_weight = config.get('gesture', 'position_stability_weight')

            # è®¡ç®—ç»¼åˆç¨³å®šæ€§å¾—åˆ†
            gesture_stability = most_common_count / 3.0
            overall_stability = gesture_stability * (1.0 - position_weight) + position_stability * position_weight

            # æ‰‹åŠ¿çŠ¶æ€è½¬æ¢é€»è¾‘
            if overall_stability >= transition_threshold:
                if most_common_gesture != self.last_stable_gesture:
                    self.gesture_stability_counter += 1
                else:
                    self.gesture_stability_counter = max(0, self.gesture_stability_counter - 1)

                if self.gesture_stability_counter >= stability_threshold:
                    # æ‰‹åŠ¿ç¨³å®šï¼Œæ›´æ–°å½“å‰æ‰‹åŠ¿
                    self.current_gesture = most_common_gesture
                    self.current_confidence = np.mean(list(self.confidence_history)[-3:])
                    self.last_stable_gesture = most_common_gesture
                    self.gesture_stability_counter = 0
                    self.last_gesture_change_time = current_time
            else:
                # æ‰‹åŠ¿ä¸ç¨³å®šï¼Œé‡ç½®è®¡æ•°å™¨
                self.gesture_stability_counter = max(0, self.gesture_stability_counter - 2)

        return self.current_gesture, self.current_confidence

    def update_gesture_state(self, hand_data, gesture, confidence):
        """æ›´æ–°æ‰‹åŠ¿çŠ¶æ€"""
        current_time = time.time()

        if hand_data is None:
            # æ‰‹éƒ¨ä¸¢å¤±
            self.hand_lost_frames += 1
            self.hand_detected_frames = max(0, self.hand_detected_frames - 1)

            if self.hand_lost_frames > 10 and self.gesture_state != "none":
                self.gesture_state = "none"
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_hand_status') and
                        current_time - self.last_hand_status_time > 3.0):
                    self.speech_manager.speak('hand_lost', immediate=True)
                    self.last_hand_status_time = current_time
            return

        # æ‰‹éƒ¨æ£€æµ‹åˆ°
        self.hand_detected_frames += 1
        self.hand_lost_frames = 0

        # æ£€æŸ¥æ‰‹éƒ¨å¤§å°å’Œä½ç½®
        hand_area = hand_data['area']
        min_area = config.get('gesture', 'min_hand_area') * (self.resize_factor ** 2)
        max_area = config.get('gesture', 'max_hand_area') * (self.resize_factor ** 2)

        # æä¾›æ‰‹éƒ¨ä½ç½®åé¦ˆ
        if (self.speech_manager and
                config.get('speech', 'enabled') and
                config.get('speech', 'announce_hand_status') and
                current_time - self.last_hand_status_time > 5.0):

            if hand_area < min_area * 0.8:
                self.speech_manager.speak('move_closer', immediate=True)
                self.last_hand_status_time = current_time
            elif hand_area > max_area * 1.2:
                self.speech_manager.speak('move_away', immediate=True)
                self.last_hand_status_time = current_time
            elif self.hand_detected_frames == 5:  # é¦–æ¬¡ç¨³å®šæ£€æµ‹
                self.speech_manager.speak('hand_detected', immediate=True)
                self.last_hand_status_time = current_time

        # æ‰‹åŠ¿çŠ¶æ€è½¬æ¢
        if self.gesture_state == "none" and gesture != "Waiting" and confidence > 0.6:
            # æ‰‹åŠ¿å¼€å§‹
            self.gesture_state = "starting"
            self.gesture_active_frames = 0
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_gesture_changes')):
                self.speech_manager.speak('gesture_start', immediate=True)

        elif self.gesture_state == "starting":
            self.gesture_active_frames += 1
            if self.gesture_active_frames >= config.get('speech', 'gesture_start_threshold'):
                self.gesture_state = "active"
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_gesture_changes')):
                    self.speech_manager.speak('gesture_stable', immediate=True)

        elif self.gesture_state == "active":
            if gesture == "Waiting" or confidence < 0.5:
                self.gesture_active_frames = max(0, self.gesture_active_frames - 2)
                if self.gesture_active_frames <= 0:
                    self.gesture_state = "ending"
            else:
                self.gesture_active_frames = min(20, self.gesture_active_frames + 1)

        elif self.gesture_state == "ending":
            self.gesture_active_frames -= 1
            if self.gesture_active_frames <= 0:
                self.gesture_state = "none"
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_gesture_changes')):
                    self.speech_manager.speak('gesture_end', immediate=True)

    def visualize_detection(self, frame, hand_data, gesture, confidence):
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        if hand_data is None:
            return frame

        show_contours = config.get('display', 'show_contours')
        show_bbox = config.get('display', 'show_bbox')
        show_fingertips = config.get('display', 'show_fingertips')
        show_palm_center = config.get('display', 'show_palm_center')
        show_hand_direction = config.get('display', 'show_hand_direction')
        show_debug_info = config.get('display', 'show_debug_info')
        show_gesture_history = config.get('display', 'show_gesture_history')
        show_stability_indicator = config.get('display', 'show_stability_indicator')

        # ç»˜åˆ¶è½®å»“
        if show_contours and 'contour' in hand_data:
            cv2.drawContours(frame, [hand_data['contour']], -1, (0, 255, 0), 2)

        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        if show_bbox and 'bbox' in hand_data:
            x1, y1, x2, y2 = hand_data['bbox']
            color = self.gesture_colors.get(gesture, (255, 255, 255))

            # æ ¹æ®æ‰‹åŠ¿çŠ¶æ€è°ƒæ•´è¾¹ç•Œæ¡†é¢œè‰²
            state_color = self.state_colors.get(self.gesture_state, color)
            if self.gesture_state != "none":
                color = state_color

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

            # æ˜¾ç¤ºæ‰‹åŠ¿æ ‡ç­¾
            label = f"{gesture}"
            if config.get('display', 'show_confidence'):
                label += f" ({confidence:.0%})"

            # æ˜¾ç¤ºæ‰‹åŠ¿çŠ¶æ€
            if self.gesture_state != "none":
                state_text = {"starting": "å¼€å§‹", "active": "æ´»è·ƒ", "ending": "ç»“æŸ"}
                label += f" [{state_text.get(self.gesture_state, '')}]"

            # è®¡ç®—æ–‡æœ¬å¤§å°
            (text_width, text_height), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
            )

            # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
            cv2.rectangle(frame,
                          (x1, y1 - text_height - 10),
                          (x1 + text_width, y1),
                          color, -1)

            # ç»˜åˆ¶æ–‡æœ¬
            cv2.putText(frame, label, (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # ç»˜åˆ¶æ‰‹æŒä¸­å¿ƒ
        if show_palm_center and 'palm_center' in hand_data:
            cx, cy = hand_data['palm_center']
            palm_radius = hand_data.get('palm_radius', 20)
            cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)
            cv2.circle(frame, (cx, cy), palm_radius, (0, 0, 255), 1)
            cv2.putText(frame, "Palm", (cx + 10, cy),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)

        # ç»˜åˆ¶æŒ‡å°–
        if show_fingertips and 'fingertips' in hand_data:
            for i, point in enumerate(hand_data['fingertips']):
                cv2.circle(frame, point, 4, (255, 0, 0), -1)
                cv2.putText(frame, f"F{i + 1}", (point[0] + 5, point[1]),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)

        # ç»˜åˆ¶æ‰‹éƒ¨æ–¹å‘
        if show_hand_direction and 'direction' in hand_data and 'center' in hand_data:
            cx, cy = hand_data['center']
            direction = hand_data['direction']
            length = 50

            dx = length * math.cos(math.radians(direction))
            dy = length * math.sin(math.radians(direction))

            end_point = (int(cx + dx), int(cy + dy))
            cv2.arrowedLine(frame, (cx, cy), end_point, (255, 255, 0), 2)

            angle_text = f"Dir: {direction:.0f}Â°"
            cv2.putText(frame, angle_text, (cx, cy - 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)

        # ç»˜åˆ¶ç¨³å®šæ€§æŒ‡ç¤ºå™¨
        if show_stability_indicator:
            h, w = frame.shape[:2]
            indicator_x = w - 100
            indicator_y = 30

            # ç»˜åˆ¶ç¨³å®šæ€§èƒŒæ™¯
            cv2.rectangle(frame, (indicator_x, indicator_y),
                          (indicator_x + 80, indicator_y + 15), (50, 50, 50), -1)

            # è®¡ç®—ç¨³å®šæ€§æŒ‡ç¤ºæ¡é•¿åº¦
            stability_level = min(1.0,
                                  self.gesture_stability_counter / self.mode_config['gesture_stability_threshold'])
            bar_length = int(70 * stability_level)

            # æ ¹æ®ç¨³å®šæ€§çº§åˆ«é€‰æ‹©é¢œè‰²
            if stability_level > 0.7:
                bar_color = (0, 255, 0)  # ç»¿è‰²
            elif stability_level > 0.4:
                bar_color = (255, 165, 0)  # æ©™è‰²
            else:
                bar_color = (255, 0, 0)  # çº¢è‰²

            # ç»˜åˆ¶ç¨³å®šæ€§æŒ‡ç¤ºæ¡
            cv2.rectangle(frame, (indicator_x + 5, indicator_y + 5),
                          (indicator_x + 5 + bar_length, indicator_y + 10), bar_color, -1)

            # ç»˜åˆ¶ç¨³å®šæ€§æ–‡æœ¬
            cv2.putText(frame, "ç¨³å®šåº¦", (indicator_x, indicator_y - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        # ç»˜åˆ¶æ‰‹åŠ¿å†å²ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if show_gesture_history and len(self.gesture_history) > 0:
            h, w = frame.shape[:2]
            history_y = h - 50

            # ç»˜åˆ¶å†å²èƒŒæ™¯
            cv2.rectangle(frame, (10, history_y - 20), (200, history_y + 10), (0, 0, 0), -1)
            cv2.putText(frame, "æ‰‹åŠ¿å†å²:", (15, history_y),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # æ˜¾ç¤ºæœ€è¿‘å‡ ä¸ªæ‰‹åŠ¿
            recent_gestures = list(self.gesture_history)[-5:] if len(self.gesture_history) >= 5 else list(
                self.gesture_history)
            for i, gest in enumerate(recent_gestures):
                color = self.gesture_colors.get(gest, (255, 255, 255))
                cv2.putText(frame, gest[0], (85 + i * 20, history_y),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
        if show_debug_info:
            # æ˜¾ç¤ºæ‰‹æŒ‡æ•°é‡
            finger_count = len(hand_data.get('fingers', []))
            finger_text = f"Fingers: {finger_count}"
            cv2.putText(frame, finger_text, (10, frame.shape[0] - 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # æ˜¾ç¤ºæ‰‹éƒ¨ä½ç½®
            pos_text = f"Pos: ({hand_data['position'][0]:.2f}, {hand_data['position'][1]:.2f})"
            cv2.putText(frame, pos_text, (10, frame.shape[0] - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # æ˜¾ç¤ºæ‰‹åŠ¿çŠ¶æ€
            state_text = f"State: {self.gesture_state}"
            cv2.putText(frame, state_text, (150, frame.shape[0] - 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # æ˜¾ç¤ºç¨³å®šæ€§è®¡æ•°å™¨
            stability_text = f"Stability: {self.gesture_stability_counter}"
            cv2.putText(frame, stability_text, (150, frame.shape[0] - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        return frame

    def recognize(self, frame):
        """è¯†åˆ«æ‰‹åŠ¿"""
        start_time = time.time()

        try:
            # é¢„å¤„ç†å¸§
            processed_frame = cv2.flip(frame, 1)

            # æ ¹æ®æ€§èƒ½æ¨¡å¼è°ƒæ•´å›¾åƒå¤§å°
            original_frame = processed_frame.copy()
            if self.resize_factor != 1.0:
                processed_frame = self.preprocess_frame(processed_frame)

            # æ¯éš”å‡ å¸§æ£€æµ‹ä¸€æ¬¡ä»¥æé«˜æ€§èƒ½
            if self.frame_counter % self.detection_interval != 0:
                self.frame_counter += 1
                return self.current_gesture, self.current_confidence, original_frame

            # è·å–è‚¤è‰²æ©ç 
            skin_mask, hsv = self.get_skin_mask(processed_frame)

            # å¢å¼ºè‚¤è‰²æ£€æµ‹
            enhanced_mask = self.enhance_skin_detection(processed_frame, skin_mask)

            # æ‰¾åˆ°æœ€ä½³çš„æ‰‹éƒ¨è½®å»“
            hand_contour, contour_score = self.find_best_hand_contour(enhanced_mask, processed_frame)

            # åˆ†ææ‰‹éƒ¨ç‰¹å¾
            hand_data, confidence = self.analyze_hand_features(hand_contour, processed_frame.shape)

            # ä¿å­˜æ‰‹éƒ¨æ•°æ®ç”¨äºè½¨è¿¹è®°å½•
            self.last_hand_data = hand_data

            # è¯†åˆ«æ‰‹åŠ¿
            if hand_data is not None:
                # æ ¡å‡†è‚¤è‰²ï¼ˆå¦‚æœéœ€è¦ï¼‰
                config.calibrate_skin_color(processed_frame, enhanced_mask)

                # æ ¡å‡†æ‰‹éƒ¨å¤§å°ï¼ˆå¦‚æœéœ€è¦ï¼‰
                config.calibrate_hand_size(hand_data['area'])

                # è¯†åˆ«æ‰‹åŠ¿
                gesture, raw_confidence = self.recognize_gesture_improved(hand_data)
                confidence = max(confidence, raw_confidence)

                # æ›´æ–°æ‰‹åŠ¿çŠ¶æ€
                self.update_gesture_state(hand_data, gesture, confidence)

                # å¢å¼ºçš„æ‰‹åŠ¿å¹³æ»‘
                final_gesture, final_confidence = self.smooth_gesture_enhanced(gesture, confidence, hand_data)
            else:
                gesture, confidence = "Waiting", 0.3
                self.update_gesture_state(None, gesture, confidence)
                final_gesture, final_confidence = gesture, confidence

            # æ‰‹åŠ¿è¯­éŸ³æ’­æŠ¥
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_gestures')):

                current_time = time.time()

                # æ ¹æ®ç½®ä¿¡åº¦æä¾›åé¦ˆ
                if confidence >= 0.8 and current_time - self.last_hand_status_time > 5.0:
                    self.speech_manager.speak('gesture_good_confidence', immediate=True)
                    self.last_hand_status_time = current_time
                elif confidence < 0.5 and current_time - self.last_hand_status_time > 5.0:
                    self.speech_manager.speak('gesture_low_confidence', immediate=True)
                    self.last_hand_status_time = current_time

                # æ‰‹åŠ¿è¯­éŸ³æ’­æŠ¥
                if (final_gesture != "Waiting" and
                        final_gesture != "Error" and
                        final_gesture != "æ‘„åƒå¤´é”™è¯¯" and
                        final_confidence >= self.min_confidence and
                        current_time - self.last_announced_time > self.gesture_announce_interval):

                    if final_gesture in self.gesture_speech_map:
                        speech_key = self.gesture_speech_map[final_gesture]
                        self.speech_manager.speak(speech_key)
                    else:
                        self.speech_manager.speak_direct(f"æ‰‹åŠ¿{final_gesture}")

                    self.last_announced_gesture = final_gesture
                    self.last_announced_time = current_time

            # æ€§èƒ½æŠ¥å‘Š
            current_time = time.time()
            if current_time - self.last_performance_report > 30.0:
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_performance')):

                    if self.avg_process_time < 20:
                        self.speech_manager.speak('performance_good', immediate=True)
                    elif self.avg_process_time > 50:
                        self.speech_manager.speak('performance_warning', immediate=True)

                    self.last_performance_report = current_time

            # å¯è§†åŒ–ç»“æœï¼ˆåœ¨åŸå›¾ä¸Šï¼‰
            if hand_data is not None:
                # éœ€è¦å°†åæ ‡è½¬æ¢å›åŸå§‹å›¾åƒå¤§å°
                if self.resize_factor != 1.0:
                    # ç¼©æ”¾åæ ‡
                    scale_factor = 1.0 / self.resize_factor
                    if 'center' in hand_data:
                        hand_data['center'] = (int(hand_data['center'][0] * scale_factor),
                                               int(hand_data['center'][1] * scale_factor))
                    if 'bbox' in hand_data:
                        x1, y1, x2, y2 = hand_data['bbox']
                        hand_data['bbox'] = (int(x1 * scale_factor), int(y1 * scale_factor),
                                             int(x2 * scale_factor), int(y2 * scale_factor))
                    if 'fingertips' in hand_data:
                        hand_data['fingertips'] = [(int(x * scale_factor), int(y * scale_factor))
                                                   for (x, y) in hand_data['fingertips']]
                    if 'palm_center' in hand_data:
                        hand_data['palm_center'] = (int(hand_data['palm_center'][0] * scale_factor),
                                                    int(hand_data['palm_center'][1] * scale_factor))

                original_frame = self.visualize_detection(
                    original_frame, hand_data, final_gesture, final_confidence
                )
            else:
                original_frame = self.visualize_detection(
                    original_frame, None, final_gesture, final_confidence
                )

            # æ›´æ–°è®¡æ•°å™¨
            self.frame_counter += 1

            # è®¡ç®—å¤„ç†æ—¶é—´
            process_time = (time.time() - start_time) * 1000
            self.process_times.append(process_time)

            # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
            if len(self.process_times) > 0:
                self.avg_process_time = np.mean(list(self.process_times))

            # æ›´æ–°å¸§ç‡
            current_time = time.time()
            if current_time - self.last_fps_check >= 1.0:
                self.frame_rate = self.frame_counter
                self.frame_counter = 0
                self.last_fps_check = current_time

            return final_gesture, final_confidence, original_frame

        except Exception as e:
            print(f"âš  æ‰‹åŠ¿è¯†åˆ«é”™è¯¯: {e}")
            return "Error", 0.0, frame

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if len(self.process_times) == 0:
            return 0.0, self.frame_rate

        return np.mean(list(self.process_times)), self.frame_rate

    def get_performance_mode_info(self):
        """è·å–æ€§èƒ½æ¨¡å¼ä¿¡æ¯"""
        return {
            'name': self.performance_mode_name,
            'mode': self.performance_mode,
            'color': self.performance_mode_color,
            'detection_interval': self.detection_interval,
            'resize_factor': self.resize_factor,
            'smooth_frames': self.smooth_frames,
            'min_confidence': self.min_confidence
        }

    def set_simulated_gesture(self, gesture):
        """è®¾ç½®æ¨¡æ‹Ÿçš„æ‰‹åŠ¿"""
        self.current_gesture = gesture
        self.current_confidence = 0.9

        # æ¨¡æ‹Ÿæ‰‹åŠ¿ä¹Ÿè§¦å‘è¯­éŸ³æç¤º
        if (self.speech_manager and
                config.get('speech', 'enabled') and
                config.get('speech', 'announce_gestures')):

            if gesture in self.gesture_speech_map:
                self.speech_manager.speak(self.gesture_speech_map[gesture])
            else:
                self.speech_manager.speak_direct(f"æ‰‹åŠ¿{gesture}")

            self.last_announced_gesture = gesture
            self.last_announced_time = time.time()


# ========== ç®€å•çš„æ— äººæœºæ§åˆ¶å™¨ ==========
class SimpleDroneController:
    """ç®€å•çš„æ— äººæœºæ§åˆ¶å™¨"""

    def __init__(self, airsim_module, speech_manager=None):
        self.airsim = airsim_module
        self.client = None
        self.connected = False
        self.flying = False
        self.speech_manager = speech_manager

        # æ§åˆ¶å‚æ•°
        self.velocity = config.get('drone', 'velocity')
        self.duration = config.get('drone', 'duration')
        self.altitude = config.get('drone', 'altitude')
        self.control_interval = config.get('drone', 'control_interval')

        # æ§åˆ¶çŠ¶æ€
        self.last_control_time = 0
        self.last_gesture = None

        # ä¸Šæ¬¡è¯­éŸ³æç¤ºçŠ¶æ€
        self.last_connection_announced = False
        self.last_takeoff_announced = False
        self.last_land_announced = False

        print("âœ“ ç®€å•çš„æ— äººæœºæ§åˆ¶å™¨å·²åˆå§‹åŒ–")

    def connect(self):
        """è¿æ¥AirSimæ— äººæœº"""
        if self.connected:
            return True

        # è¯­éŸ³æç¤ºï¼šæ­£åœ¨è¿æ¥
        if (self.speech_manager and
                config.get('speech', 'enabled') and
                config.get('speech', 'announce_connections')):
            self.speech_manager.speak('connecting')

        if self.airsim is None:
            print("âš  AirSimä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

            # è¯­éŸ³æç¤ºï¼šæ¨¡æ‹Ÿæ¨¡å¼
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_connections')):
                self.speech_manager.speak('simulation_mode')

            self.connected = True
            return True

        print("è¿æ¥AirSim...")

        try:
            self.client = self.airsim.MultirotorClient()
            self.client.confirmConnection()
            print("âœ… å·²è¿æ¥AirSim!")

            # è¯­éŸ³æç¤ºï¼šè¿æ¥æˆåŠŸ
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_connections')):
                self.speech_manager.speak('connected')

            self.client.enableApiControl(True)
            print("âœ… APIæ§åˆ¶å·²å¯ç”¨")

            self.client.armDisarm(True)
            print("âœ… æ— äººæœºå·²æ­¦è£…")

            self.connected = True
            return True

        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")

            # è¯­éŸ³æç¤ºï¼šè¿æ¥å¤±è´¥
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_connections')):
                self.speech_manager.speak('connection_failed')

            print("\nä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ç»§ç»­? (y/n)")
            choice = input().strip().lower()
            if choice == 'y':
                self.connected = True
                print("âœ… ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

                # è¯­éŸ³æç¤ºï¼šæ¨¡æ‹Ÿæ¨¡å¼
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_connections')):
                    self.speech_manager.speak('simulation_mode')

                return True

            return False

    def takeoff(self):
        """èµ·é£"""
        if not self.connected:
            return False

        # è¯­éŸ³æç¤ºï¼šæ­£åœ¨èµ·é£
        if (self.speech_manager and
                config.get('speech', 'enabled') and
                config.get('speech', 'announce_flight_events') and
                not self.last_takeoff_announced):
            self.speech_manager.speak('taking_off')
            self.last_takeoff_announced = True
            self.last_land_announced = False

        try:
            if self.airsim is None or self.client is None:
                print("âœ… æ¨¡æ‹Ÿèµ·é£")
                self.flying = True

                # è¯­éŸ³æç¤ºï¼šèµ·é£æˆåŠŸ
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_flight_events')):
                    self.speech_manager.speak('takeoff_success')

                return True

            print("èµ·é£ä¸­...")
            self.client.takeoffAsync().join()
            time.sleep(1)

            # ä¸Šå‡åˆ°æŒ‡å®šé«˜åº¦
            self.client.moveToZAsync(self.altitude, 3).join()

            self.flying = True
            print("âœ… æ— äººæœºæˆåŠŸèµ·é£")

            # è¯­éŸ³æç¤ºï¼šèµ·é£æˆåŠŸ
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_flight_events')):
                self.speech_manager.speak('takeoff_success')

            return True
        except Exception as e:
            print(f"âŒ èµ·é£å¤±è´¥: {e}")

            # è¯­éŸ³æç¤ºï¼šèµ·é£å¤±è´¥
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_flight_events')):
                self.speech_manager.speak('takeoff_failed')

            return False

    def land(self):
        """é™è½"""
        if not self.connected:
            return False

        # è¯­éŸ³æç¤ºï¼šæ­£åœ¨é™è½
        if (self.speech_manager and
                config.get('speech', 'enabled') and
                config.get('speech', 'announce_flight_events') and
                not self.last_land_announced):
            self.speech_manager.speak('landing')
            self.last_land_announced = True
            self.last_takeoff_announced = False

        try:
            if self.airsim is None or self.client is None:
                print("âœ… æ¨¡æ‹Ÿé™è½")
                self.flying = False

                # è¯­éŸ³æç¤ºï¼šé™è½æˆåŠŸ
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_flight_events')):
                    self.speech_manager.speak('land_success')

                return True

            print("é™è½ä¸­...")
            self.client.landAsync().join()
            self.flying = False
            print("âœ… æ— äººæœºå·²é™è½")

            # è¯­éŸ³æç¤ºï¼šé™è½æˆåŠŸ
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_flight_events')):
                self.speech_manager.speak('land_success')

            return True
        except Exception as e:
            print(f"é™è½å¤±è´¥: {e}")
            return False

    def move_by_gesture(self, gesture, confidence):
        """æ ¹æ®æ‰‹åŠ¿ç§»åŠ¨"""
        if not self.connected or not self.flying:
            return False

        # æ£€æŸ¥æ§åˆ¶é—´éš”
        current_time = time.time()
        if current_time - self.last_control_time < self.control_interval:
            return False

        # æ£€æŸ¥ç½®ä¿¡åº¦é˜ˆå€¼
        min_confidence = config.get('gesture', 'min_confidence')
        if confidence < min_confidence:
            # ä½ç½®ä¿¡åº¦è¯­éŸ³æç¤º
            if (self.speech_manager and
                    config.get('speech', 'enabled') and
                    config.get('speech', 'announce_gestures') and
                    confidence < min_confidence * 0.8):
                self.speech_manager.speak('gesture_low_confidence')
            return False

        try:
            if self.airsim is None or self.client is None:
                print(f"æ¨¡æ‹Ÿç§»åŠ¨: {gesture}")
                self.last_control_time = current_time
                self.last_gesture = gesture
                return True

            success = False

            if gesture == "Up":
                self.client.moveByVelocityZAsync(0, 0, -self.velocity, self.duration)
                success = True
            elif gesture == "Down":
                self.client.moveByVelocityZAsync(0, 0, self.velocity, self.duration)
                success = True
            elif gesture == "Left":
                self.client.moveByVelocityAsync(-self.velocity, 0, 0, self.duration)
                success = True
            elif gesture == "Right":
                self.client.moveByVelocityAsync(self.velocity, 0, 0, self.duration)
                success = True
            elif gesture == "Forward":
                self.client.moveByVelocityAsync(0, -self.velocity, 0, self.duration)
                success = True
            elif gesture == "Stop":
                self.client.hoverAsync()
                success = True
                # æ‚¬åœè¯­éŸ³æç¤º
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_flight_events')):
                    self.speech_manager.speak('hovering')
            elif gesture == "Hover":
                self.client.hoverAsync()
                success = True
                if (self.speech_manager and
                        config.get('speech', 'enabled') and
                        config.get('speech', 'announce_flight_events')):
                    self.speech_manager.speak('hovering')

            if success:
                self.last_control_time = current_time
                self.last_gesture = gesture

            return success
        except Exception as e:
            print(f"æ§åˆ¶å‘½ä»¤å¤±è´¥: {e}")
            return False

    def emergency_stop(self):
        """ç´§æ€¥åœæ­¢"""
        if self.connected:
            try:
                if self.flying and self.client is not None:
                    print("ç´§æ€¥é™è½...")

                    # è¯­éŸ³æç¤ºï¼šç´§æ€¥åœæ­¢
                    if (self.speech_manager and
                            config.get('speech', 'enabled') and
                            config.get('speech', 'announce_flight_events')):
                        self.speech_manager.speak('emergency_stop')

                    self.land()
                if self.client is not None:
                    self.client.armDisarm(False)
                    self.client.enableApiControl(False)
                    print("âœ… ç´§æ€¥åœæ­¢å®Œæˆ")
            except:
                pass

        self.connected = False
        self.flying = False


# ========== ä¸­æ–‡UIæ¸²æŸ“å™¨ ==========
class ChineseUIRenderer:
    """ä¸­æ–‡UIæ¸²æŸ“å™¨"""

    def __init__(self, speech_manager=None):
        self.fonts = {}
        self.speech_manager = speech_manager
        self.load_fonts()

        # é¢œè‰²å®šä¹‰
        self.colors = {
            'title': (0, 255, 255),
            'connected': (0, 255, 0),
            'disconnected': (0, 0, 255),
            'flying': (0, 255, 0),
            'landed': (255, 165, 0),
            'warning': (0, 165, 255),
            'info': (255, 255, 255),
            'help': (255, 200, 100),
            'speech_enabled': (0, 255, 0),
            'speech_disabled': (255, 0, 0),
            'performance_good': (0, 255, 0),
            'performance_warning': (255, 165, 0),
            'performance_bad': (255, 0, 0),
            'recording': (255, 50, 50),
            'playback': (50, 50, 255),
            'paused': (255, 255, 0),
            'performance_fast': (0, 255, 0),  # ç»¿è‰²
            'performance_balanced': (255, 165, 0),  # æ©™è‰²
            'performance_accurate': (255, 0, 0),  # çº¢è‰²
        }

        print("âœ“ ä¸­æ–‡UIæ¸²æŸ“å™¨å·²åˆå§‹åŒ–")

    def load_fonts(self):
        """åŠ è½½å­—ä½“"""
        font_paths = [
            'simhei.ttf',
            'C:/Windows/Fonts/simhei.ttf',
            'C:/Windows/Fonts/msyh.ttc',
            '/System/Library/Fonts/PingFang.ttc',
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'
        ]

        for path in font_paths:
            try:
                self.fonts[14] = ImageFont.truetype(path, 14)
                self.fonts[16] = ImageFont.truetype(path, 16)
                self.fonts[18] = ImageFont.truetype(path, 18)
                self.fonts[20] = ImageFont.truetype(path, 20)
                self.fonts[24] = ImageFont.truetype(path, 24)
                print(f"âœ“ å­—ä½“å·²åŠ è½½: {path}")
                return
            except:
                continue

        print("âš  æœªæ‰¾åˆ°å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤")

    def draw_text(self, frame, text, pos, size=16, color=(255, 255, 255)):
        """åœ¨å›¾åƒä¸Šç»˜åˆ¶æ–‡æœ¬"""
        try:
            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(img_rgb)
            draw = ImageDraw.Draw(pil_img)

            font = self.fonts.get(size, self.fonts.get(16))

            # ç»˜åˆ¶é˜´å½±
            shadow_color = (0, 0, 0)
            shadow_pos = (pos[0] + 1, pos[1] + 1)
            draw.text(shadow_pos, text, font=font, fill=shadow_color)

            # ç»˜åˆ¶æ–‡å­—
            rgb_color = color[::-1]  # BGR to RGB
            draw.text(pos, text, font=font, fill=rgb_color)

            return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
        except:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨OpenCVç»˜åˆ¶è‹±æ–‡
            cv2.putText(frame, text, pos, cv2.FONT_HERSHEY_SIMPLEX,
                        size / 25, color, 1)
            return frame

    def draw_status_bar(self, frame, drone_controller, gesture, confidence, fps, process_time, trajectory_recorder=None,
                        gesture_recognizer=None):
        """ç»˜åˆ¶çŠ¶æ€æ """
        h, w = frame.shape[:2]

        # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, 0), (w, 120), (0, 0, 0), -1)
        frame = cv2.addWeighted(overlay, 0.7, frame, 0.3, 0)

        # æ ‡é¢˜
        title = "æ‰‹åŠ¿æ§åˆ¶æ— äººæœºç³»ç»Ÿ - æ€§èƒ½æ¨¡å¼ç‰ˆ"
        frame = self.draw_text(frame, title, (10, 10), size=20, color=self.colors['title'])

        # è¿æ¥çŠ¶æ€
        status_color = self.colors['connected'] if drone_controller.connected else self.colors['disconnected']
        status_text = f"æ— äººæœº: {'å·²è¿æ¥' if drone_controller.connected else 'æœªè¿æ¥'}"
        frame = self.draw_text(frame, status_text, (10, 40), size=16, color=status_color)

        # é£è¡ŒçŠ¶æ€
        flight_color = self.colors['flying'] if drone_controller.flying else self.colors['landed']
        flight_text = f"é£è¡ŒçŠ¶æ€: {'é£è¡Œä¸­' if drone_controller.flying else 'å·²é™è½'}"
        frame = self.draw_text(frame, flight_text, (10, 65), size=16, color=flight_color)

        # æ‰‹åŠ¿ä¿¡æ¯
        if confidence > 0.7:
            gesture_color = (0, 255, 0)
        elif confidence > 0.5:
            gesture_color = (255, 165, 0)
        else:
            gesture_color = (200, 200, 200)

        gesture_text = f"å½“å‰æ‰‹åŠ¿: {gesture}"
        if config.get('display', 'show_confidence'):
            gesture_text += f" ({confidence:.0%})"

        frame = self.draw_text(frame, gesture_text, (w // 2, 40), size=16, color=gesture_color)

        # æ€§èƒ½æ¨¡å¼æ˜¾ç¤º
        if gesture_recognizer and config.get('display', 'show_performance_mode'):
            mode_info = gesture_recognizer.get_performance_mode_info()

            # æ ¹æ®æ¨¡å¼é€‰æ‹©é¢œè‰²
            if mode_info['mode'] == 'fast':
                mode_color = self.colors['performance_fast']
            elif mode_info['mode'] == 'balanced':
                mode_color = self.colors['performance_balanced']
            else:  # accurate
                mode_color = self.colors['performance_accurate']

            mode_text = f"æ€§èƒ½æ¨¡å¼: {mode_info['name']}"
            frame = self.draw_text(frame, mode_text, (w // 2, 65), size=14, color=mode_color)

        # å½•åˆ¶/å›æ”¾çŠ¶æ€
        elif trajectory_recorder and config.get('display', 'show_recording_status'):
            recorder_status = trajectory_recorder.get_status()

            if recorder_status['is_recording']:
                status_color = self.colors['recording']
                status_text = f"å½•åˆ¶ä¸­: {recorder_status['trajectory_points']}ç‚¹"
                frame = self.draw_text(frame, status_text, (w // 2, 65), size=14, color=status_color)
            elif recorder_status['is_playing']:
                if recorder_status['playback_paused']:
                    status_color = self.colors['paused']
                    status_text = f"å›æ”¾æš‚åœ: {recorder_status['playback_index']}/{recorder_status['playback_total']}"
                else:
                    status_color = self.colors['playback']
                    status_text = f"å›æ”¾ä¸­: {recorder_status['playback_index']}/{recorder_status['playback_total']}"
                frame = self.draw_text(frame, status_text, (w // 2, 65), size=14, color=status_color)
            else:
                status_text = f"è½¨è¿¹ç‚¹: {recorder_status['trajectory_points']}"
                frame = self.draw_text(frame, status_text, (w // 2, 65), size=12, color=self.colors['info'])

        # è¯­éŸ³çŠ¶æ€
        if config.get('display', 'show_speech_status') and self.speech_manager:
            speech_status = self.speech_manager.get_status()
            speech_color = self.colors['speech_enabled'] if speech_status['enabled'] else self.colors['speech_disabled']
            speech_text = f"è¯­éŸ³: {'å¯ç”¨' if speech_status['enabled'] else 'ç¦ç”¨'}"
            frame = self.draw_text(frame, speech_text, (w // 2, 90), size=12, color=speech_color)

        # æ€§èƒ½ä¿¡æ¯
        if config.get('display', 'show_fps'):
            perf_text = f"å¸§ç‡: {fps:.1f}"
            if process_time > 0:
                perf_text += f" | å»¶è¿Ÿ: {process_time:.1f}ms"

                # æ ¹æ®å¤„ç†æ—¶é—´é€‰æ‹©é¢œè‰²
                if process_time < 20:
                    perf_color = self.colors['performance_good']
                elif process_time < 50:
                    perf_color = self.colors['performance_warning']
                else:
                    perf_color = self.colors['performance_bad']
            else:
                perf_color = self.colors['info']

            frame = self.draw_text(frame, perf_text, (w - 200, 65), size=12, color=perf_color)

        # æ§åˆ¶æç¤º
        control_text = "æç¤º: ç¡®ä¿æ‰‹éƒ¨å®Œå…¨è¿›å…¥ç”»é¢ï¼Œä¿æŒç¨³å®šæ‰‹åŠ¿"
        frame = self.draw_text(frame, control_text, (10, 90), size=12, color=self.colors['info'])

        return frame

    def draw_help_bar(self, frame):
        """ç»˜åˆ¶å¸®åŠ©æ """
        if not config.get('display', 'show_help'):
            return frame

        h, w = frame.shape[:2]

        # ç»˜åˆ¶åº•éƒ¨å¸®åŠ©æ 
        cv2.rectangle(frame, (0, h - 100), (w, h), (0, 0, 0), -1)

        # å¸®åŠ©æ–‡æœ¬
        help_lines = [
            "C:è¿æ¥  ç©ºæ ¼:èµ·é£/é™è½  ESC:é€€å‡º  W/A/S/D/F/X:é”®ç›˜æ§åˆ¶",
            "H:åˆ‡æ¢å¸®åŠ©  R:é‡ç½®è¯†åˆ«  T:åˆ‡æ¢æ˜¾ç¤ºæ¨¡å¼  D:è°ƒè¯•ä¿¡æ¯",
            "V:åˆ‡æ¢è¯­éŸ³åé¦ˆ  M:æµ‹è¯•è¯­éŸ³  P:æ€§èƒ½æŠ¥å‘Š  O:åˆ‡æ¢æ€§èƒ½æ¨¡å¼",
            "1:å¼€å§‹å½•åˆ¶ 2:åœæ­¢å½•åˆ¶ 3:ä¿å­˜è½¨è¿¹ 4:å›æ”¾è½¨è¿¹ 5:æ¸…é™¤è½¨è¿¹ 6:æš‚åœ/ç»§ç»­"
        ]

        for i, line in enumerate(help_lines):
            y_pos = h - 85 + i * 20
            frame = self.draw_text(frame, line, (10, y_pos), size=14, color=self.colors['help'])

        return frame

    def draw_warning(self, frame, message):
        """ç»˜åˆ¶è­¦å‘Šä¿¡æ¯"""
        h, w = frame.shape[:2]

        # åœ¨é¡¶éƒ¨ç»˜åˆ¶è­¦å‘Š
        warning_bg = np.zeros((40, w, 3), dtype=np.uint8)
        warning_bg[:, :] = (0, 69, 255)

        frame[120:160, 0:w] = cv2.addWeighted(
            frame[120:160, 0:w], 0.3,
            warning_bg, 0.7, 0
        )

        # ç»˜åˆ¶è­¦å‘Šæ–‡æœ¬
        frame = self.draw_text(frame, message, (10, 135),
                               size=16, color=self.colors['warning'])

        return frame


# ========== æ€§èƒ½ç›‘æ§å™¨ ==========
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.frame_times = deque(maxlen=60)
        self.last_update = time.time()
        self.fps = 0
        self.frame_count = 0

    def update(self):
        """æ›´æ–°æ€§èƒ½æ•°æ®"""
        current_time = time.time()
        self.frame_times.append(current_time)
        self.frame_count += 1

        # æ¯ç§’è®¡ç®—ä¸€æ¬¡FPS
        if current_time - self.last_update >= 1.0:
            if len(self.frame_times) > 1:
                time_diff = self.frame_times[-1] - self.frame_times[0]
                if time_diff > 0:
                    self.fps = len(self.frame_times) / time_diff
                else:
                    self.fps = 0
            self.last_update = current_time

    def get_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            'fps': self.fps,
            'frame_count': self.frame_count
        }


# ========== ä¸»ç¨‹åº ==========
def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–è¯­éŸ³ç®¡ç†å™¨
    print("åˆå§‹åŒ–è¯­éŸ³åé¦ˆç³»ç»Ÿ...")
    speech_manager = EnhancedSpeechFeedbackManager(libs['speech'])

    # ç¨‹åºå¯åŠ¨è¯­éŸ³æç¤º
    if speech_manager.enabled:
        speech_manager.speak('program_start', force=True, immediate=True)
        speech_manager.speak('system_ready', immediate=True)

    # åˆå§‹åŒ–ç»„ä»¶
    print("åˆå§‹åŒ–ç»„ä»¶...")

    gesture_recognizer = EnhancedGestureRecognizer(speech_manager)
    drone_controller = SimpleDroneController(libs['airsim'], speech_manager)
    ui_renderer = ChineseUIRenderer(speech_manager)
    performance_monitor = PerformanceMonitor()

    # æ‰‹åŠ¿è½¨è¿¹è®°å½•å™¨
    print("åˆå§‹åŒ–æ‰‹åŠ¿è½¨è¿¹è®°å½•å™¨...")
    trajectory_recorder = GestureTrajectoryRecorder(speech_manager)

    # åˆå§‹åŒ–æ‘„åƒå¤´
    cap = None
    try:
        cap = cv2.VideoCapture(0)
        if cap.isOpened():
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, config.get('camera', 'width'))
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, config.get('camera', 'height'))
            cap.set(cv2.CAP_PROP_FPS, config.get('camera', 'fps'))

            # è·å–å®é™…å‚æ•°
            actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            actual_fps = int(cap.get(cv2.CAP_PROP_FPS))

            print(f"âœ“ æ‘„åƒå¤´å·²åˆå§‹åŒ–")
            print(f"  åˆ†è¾¨ç‡: {actual_width}x{actual_height}")
            print(f"  å¸§ç‡: {actual_fps}")

            # æ‘„åƒå¤´å°±ç»ªè¯­éŸ³æç¤º
            if speech_manager.enabled:
                speech_manager.speak('camera_ready', immediate=True)
        else:
            print("âŒ æ‘„åƒå¤´ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

            # æ‘„åƒå¤´é”™è¯¯è¯­éŸ³æç¤º
            if speech_manager.enabled:
                speech_manager.speak('camera_error', immediate=True)

            cap = None
    except Exception as e:
        print(f"âš  æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥: {e}")

        # æ‘„åƒå¤´é”™è¯¯è¯­éŸ³æç¤º
        if speech_manager.enabled:
            speech_manager.speak('camera_error', immediate=True)

        cap = None

    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 60)
    print("æ‰‹åŠ¿æ§åˆ¶æ— äººæœºç³»ç»Ÿ - æ€§èƒ½æ¨¡å¼ç‰ˆ")
    print("=" * 60)
    print("ç³»ç»ŸçŠ¶æ€:")
    print(f"  æ‘„åƒå¤´: {'å·²è¿æ¥' if cap else 'æ¨¡æ‹Ÿæ¨¡å¼'}")
    print(f"  æ‰‹åŠ¿è¯†åˆ«: å¢å¼ºçš„å¹³æ»‘å¤„ç†ç®—æ³•")
    print(f"  è¯­éŸ³åé¦ˆ: {'å·²å¯ç”¨' if speech_manager.enabled else 'å·²ç¦ç”¨'}")
    print(f"  æ€§èƒ½æ¨¡å¼: {gesture_recognizer.performance_mode_name}")
    print(f"  è½¨è¿¹è®°å½•: æ”¯æŒå½•åˆ¶/å›æ”¾åŠŸèƒ½")
    print(f"  AirSim: {'å¯ç”¨' if libs['airsim'] else 'æ¨¡æ‹Ÿæ¨¡å¼'}")
    print("=" * 60)

    # æ˜¾ç¤ºæ“ä½œè¯´æ˜
    print("\næ“ä½œè¯´æ˜:")
    print("1. æŒ‰ [C] è¿æ¥æ— äººæœº (AirSimæ¨¡æ‹Ÿå™¨)")
    print("2. æŒ‰ [ç©ºæ ¼é”®] èµ·é£/é™è½")
    print("3. æ€§èƒ½æ¨¡å¼é€‰æ‹©:")
    print("   - æŒ‰ [O] é”®å¾ªç¯åˆ‡æ¢æ€§èƒ½æ¨¡å¼: æœ€å¿«(fast) â†’ å¹³è¡¡(balanced) â†’ æœ€å‡†(accurate)")
    print("   - æœ€å¿«æ¨¡å¼: æ€§èƒ½ä¼˜å…ˆï¼Œå¸§ç‡æœ€é«˜ï¼Œè¯†åˆ«ç²¾åº¦è¾ƒä½")
    print("   - å¹³è¡¡æ¨¡å¼: æ€§èƒ½ä¸ç²¾åº¦å¹³è¡¡ï¼Œé€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯")
    print("   - æœ€å‡†æ¨¡å¼: ç²¾åº¦ä¼˜å…ˆï¼Œæä¾›æœ€å‡†ç¡®çš„æ‰‹åŠ¿è¯†åˆ«")
    print("4. æ‰‹åŠ¿æ§åˆ¶:")
    print("   - æ‰‹åŠ¿è¯†åˆ«åä¼šæœ‰è¯­éŸ³æç¤º: å‘ä¸Šã€å‘ä¸‹ã€å‘å·¦ã€å‘å³ã€å‘å‰ã€åœæ­¢")
    print("   - æ‰‹åŠ¿ç¨³å®šæ€§è¶Šé«˜ï¼Œè¯†åˆ«è¶Šå‡†ç¡®")
    print("   - æ‰‹éƒ¨è·ç¦»æ‘„åƒå¤´é€‚ä¸­æ—¶æ•ˆæœæœ€ä½³")
    print("   * æ‰‹åŠ¿è¯†åˆ«ç½®ä¿¡åº¦ > 60% æ—¶æ‰ä¼šæ‰§è¡Œ")
    print("5. è½¨è¿¹è®°å½•åŠŸèƒ½:")
    print("   [1]å¼€å§‹å½•åˆ¶ [2]åœæ­¢å½•åˆ¶ [3]ä¿å­˜è½¨è¿¹ [4]å›æ”¾è½¨è¿¹ [5]æ¸…é™¤è½¨è¿¹ [6]æš‚åœ/ç»§ç»­")
    print("   - å½•åˆ¶æ—¶ç³»ç»Ÿä¼šè®°å½•æ‰‹éƒ¨ä½ç½®å’Œæ‰‹åŠ¿")
    print("   - å›æ”¾æ—¶å¯ä»¥æŸ¥çœ‹å½•åˆ¶çš„è½¨è¿¹")
    print("   - è½¨è¿¹æ•°æ®è‡ªåŠ¨ä¿å­˜åˆ° trajectories/ ç›®å½•")
    print("6. é”®ç›˜æ§åˆ¶:")
    print("   [W]å‘ä¸Š [S]å‘ä¸‹ [A]å‘å·¦ [D]å‘å³ [F]å‘å‰ [X]åœæ­¢")
    print("7. è°ƒè¯•åŠŸèƒ½:")
    print("   [H]åˆ‡æ¢å¸®åŠ©æ˜¾ç¤º [R]é‡ç½®æ‰‹åŠ¿è¯†åˆ« [T]åˆ‡æ¢æ˜¾ç¤ºæ¨¡å¼ [D]è°ƒè¯•ä¿¡æ¯")
    print("8. è¯­éŸ³æ§åˆ¶:")
    print("   [V]åˆ‡æ¢è¯­éŸ³åé¦ˆ [M]æµ‹è¯•è¯­éŸ³ [P]æ€§èƒ½æŠ¥å‘Š")
    print("9. æŒ‰ [ESC] å®‰å…¨é€€å‡º")
    print("=" * 60)
    print("ç¨‹åºå¯åŠ¨æˆåŠŸ!")
    print("-" * 60)

    # é”®ç›˜æ‰‹åŠ¿æ˜ å°„
    key_to_gesture = {
        ord('w'): "Up", ord('W'): "Up",
        ord('s'): "Down", ord('S'): "Down",
        ord('a'): "Left", ord('A'): "Left",
        ord('d'): "Right", ord('D'): "Right",
        ord('f'): "Forward", ord('F'): "Forward",
        ord('x'): "Stop", ord('X'): "Stop",
        ord('h'): "Hover", ord('H'): "Hover",
    }

    # æ˜¾ç¤ºæ¨¡å¼
    display_modes = ['normal', 'detailed', 'minimal']
    current_display_mode = 0

    # ä¸»å¾ªç¯
    print("\nè¿›å…¥ä¸»å¾ªç¯ï¼ŒæŒ‰ESCé€€å‡º...")

    try:
        while True:
            # æ›´æ–°æ€§èƒ½ç›‘æ§
            performance_monitor.update()

            # è¯»å–æ‘„åƒå¤´å¸§
            if cap:
                ret, frame = cap.read()
                if not ret:
                    # åˆ›å»ºç©ºç™½å¸§
                    frame = np.zeros((480, 640, 3), dtype=np.uint8)
                    gesture, confidence = "æ‘„åƒå¤´é”™è¯¯", 0.0
                else:
                    # æ‰‹åŠ¿è¯†åˆ«
                    gesture, confidence, frame = gesture_recognizer.recognize(frame)
            else:
                # æ¨¡æ‹Ÿæ¨¡å¼
                frame = np.zeros((480, 640, 3), dtype=np.uint8)
                gesture, confidence = gesture_recognizer.current_gesture, gesture_recognizer.current_confidence

            # è·å–æ€§èƒ½ç»Ÿè®¡
            perf_stats = performance_monitor.get_stats()
            process_time, frame_rate = gesture_recognizer.get_performance_stats()

            # æ ¹æ®æ˜¾ç¤ºæ¨¡å¼è°ƒæ•´æ˜¾ç¤ºé€‰é¡¹
            if display_modes[current_display_mode] == 'normal':
                config.set('display', 'show_contours', value=True)
                config.set('display', 'show_bbox', value=True)
                config.set('display', 'show_fingertips', value=True)
                config.set('display', 'show_gesture_history', value=True)
                config.set('display', 'show_stability_indicator', value=True)
                config.set('display', 'show_trajectory', value=True)
                config.set('display', 'show_recording_status', value=True)
                config.set('display', 'show_performance_mode', value=True)
                config.set('display', 'show_debug_info', value=False)
            elif display_modes[current_display_mode] == 'detailed':
                config.set('display', 'show_contours', value=True)
                config.set('display', 'show_bbox', value=True)
                config.set('display', 'show_fingertips', value=True)
                config.set('display', 'show_palm_center', value=True)
                config.set('display', 'show_hand_direction', value=True)
                config.set('display', 'show_gesture_history', value=True)
                config.set('display', 'show_stability_indicator', value=True)
                config.set('display', 'show_trajectory', value=True)
                config.set('display', 'show_recording_status', value=True)
                config.set('display', 'show_performance_mode', value=True)
                config.set('display', 'show_debug_info', value=True)
            elif display_modes[current_display_mode] == 'minimal':
                config.set('display', 'show_contours', value=False)
                config.set('display', 'show_bbox', value=True)
                config.set('display', 'show_fingertips', value=False)
                config.set('display', 'show_gesture_history', value=False)
                config.set('display', 'show_stability_indicator', value=False)
                config.set('display', 'show_trajectory', value=True)
                config.set('display', 'show_recording_status', value=True)
                config.set('display', 'show_performance_mode', value=True)
                config.set('display', 'show_debug_info', value=False)

            # ç»˜åˆ¶è½¨è¿¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if config.get('display', 'show_trajectory'):
                frame = trajectory_recorder.draw_trajectory(frame)

            # ç»˜åˆ¶UI
            frame = ui_renderer.draw_status_bar(
                frame, drone_controller, gesture, confidence,
                perf_stats['fps'], process_time, trajectory_recorder, gesture_recognizer
            )

            frame = ui_renderer.draw_help_bar(frame)

            # æ˜¾ç¤ºè¿æ¥æç¤º
            if not drone_controller.connected:
                warning_msg = "âš  æŒ‰Cé”®è¿æ¥æ— äººæœºï¼Œæˆ–ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼"
                frame = ui_renderer.draw_warning(frame, warning_msg)

            # æ˜¾ç¤ºå›¾åƒï¼ˆçª—å£æ ‡é¢˜ç”¨è‹±æ–‡ï¼‰
            cv2.imshow('Gesture Controlled Drone - Performance Mode', frame)

            # ========== é”®ç›˜æ§åˆ¶ ==========
            key = cv2.waitKey(1) & 0xFF

            if key == 27:  # ESCé”®
                print("\né€€å‡ºç¨‹åº...")
                break

            elif key == ord('c') or key == ord('C'):
                if not drone_controller.connected:
                    drone_controller.connect()

            elif key == 32:  # ç©ºæ ¼é”®
                if drone_controller.connected:
                    if drone_controller.flying:
                        drone_controller.land()
                    else:
                        drone_controller.takeoff()
                    time.sleep(0.5)

            elif key == ord('h') or key == ord('H'):
                # åˆ‡æ¢å¸®åŠ©æ˜¾ç¤º
                current = config.get('display', 'show_help')
                config.set('display', 'show_help', value=not current)
                print(f"å¸®åŠ©æ˜¾ç¤º: {'å¼€å¯' if not current else 'å…³é—­'}")

                # è¯­éŸ³æç¤º
                if speech_manager.enabled:
                    speech_manager.speak('help_toggled', immediate=True)

            elif key == ord('r') or key == ord('R'):
                # é‡ç½®æ‰‹åŠ¿è¯†åˆ«
                print("é‡ç½®æ‰‹åŠ¿è¯†åˆ«...")
                gesture_recognizer = EnhancedGestureRecognizer(speech_manager)
                print("âœ“ æ‰‹åŠ¿è¯†åˆ«å·²é‡ç½®")

                # è¯­éŸ³æç¤º
                if speech_manager.enabled:
                    speech_manager.speak_direct("æ‰‹åŠ¿è¯†åˆ«å·²é‡ç½®")

            elif key == ord('t') or key == ord('T'):
                # åˆ‡æ¢æ˜¾ç¤ºæ¨¡å¼
                current_display_mode = (current_display_mode + 1) % len(display_modes)
                mode_name = display_modes[current_display_mode]
                print(f"æ˜¾ç¤ºæ¨¡å¼: {mode_name}")

                # è¯­éŸ³æç¤º
                if speech_manager.enabled:
                    speech_manager.speak('display_mode_changed', immediate=True)

            elif key == ord('d') or key == ord('D'):
                # åˆ‡æ¢è°ƒè¯•ä¿¡æ¯
                current = config.get('display', 'show_debug_info')
                config.set('display', 'show_debug_info', value=not current)
                status = 'å¼€å¯' if not current else 'å…³é—­'
                print(f"è°ƒè¯•ä¿¡æ¯: {status}")

                # è¯­éŸ³æç¤º
                if speech_manager.enabled:
                    if not current:
                        speech_manager.speak('debug_mode_on', immediate=True)
                    else:
                        speech_manager.speak('debug_mode_off', immediate=True)

            elif key == ord('v') or key == ord('V'):
                # åˆ‡æ¢è¯­éŸ³åé¦ˆ
                new_status = speech_manager.toggle_enabled()
                status = 'å¯ç”¨' if new_status else 'ç¦ç”¨'
                print(f"è¯­éŸ³åé¦ˆ: {status}")
                config.set('speech', 'enabled', value=new_status)

            elif key == ord('m') or key == ord('M'):
                # æµ‹è¯•è¯­éŸ³
                if speech_manager.enabled:
                    print("æµ‹è¯•è¯­éŸ³...")
                    speech_manager.speak_direct("è¯­éŸ³åé¦ˆæµ‹è¯•ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸")
                else:
                    print("è¯­éŸ³åé¦ˆå·²ç¦ç”¨ï¼ŒæŒ‰Vé”®å¯ç”¨")

            elif key == ord('p') or key == ord('P'):
                # æ€§èƒ½æŠ¥å‘Š
                if speech_manager.enabled:
                    print("ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
                    if process_time < 20:
                        speech_manager.speak_direct("ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼Œè¿è¡Œæµç•…")
                    elif process_time < 50:
                        speech_manager.speak_direct("ç³»ç»Ÿæ€§èƒ½è‰¯å¥½")
                    else:
                        speech_manager.speak_direct("ç³»ç»Ÿæ€§èƒ½è­¦å‘Šï¼Œè¯·æ£€æŸ¥")

            elif key == ord('o') or key == ord('O'):
                # åˆ‡æ¢æ€§èƒ½æ¨¡å¼
                if config.cycle_performance_mode():
                    new_mode = config.get_current_performance_mode()
                    gesture_recognizer.set_performance_mode(new_mode)

                    # è¯­éŸ³æç¤º
                    if speech_manager.enabled:
                        if new_mode == 'fast':
                            speech_manager.speak('performance_mode_fast', immediate=True)
                        elif new_mode == 'balanced':
                            speech_manager.speak('performance_mode_balanced', immediate=True)
                        else:  # accurate
                            speech_manager.speak('performance_mode_accurate', immediate=True)

                    print(f"âœ“ å·²åˆ‡æ¢åˆ°æ€§èƒ½æ¨¡å¼: {gesture_recognizer.performance_mode_name}")

            # æ–°å¢ï¼šè½¨è¿¹è®°å½•æ§åˆ¶
            elif key == ord('1'):
                # å¼€å§‹å½•åˆ¶è½¨è¿¹
                if trajectory_recorder.start_recording():
                    print("âœ… å¼€å§‹å½•åˆ¶æ‰‹åŠ¿è½¨è¿¹")
                else:
                    print("âš  å·²ç»åœ¨å½•åˆ¶ä¸­")

            elif key == ord('2'):
                # åœæ­¢å½•åˆ¶è½¨è¿¹
                if trajectory_recorder.stop_recording():
                    print("âœ… åœæ­¢å½•åˆ¶æ‰‹åŠ¿è½¨è¿¹")
                else:
                    print("âš  å½“å‰æ²¡æœ‰åœ¨å½•åˆ¶")

            elif key == ord('3'):
                # ä¿å­˜è½¨è¿¹
                if trajectory_recorder.save_trajectory():
                    print("âœ… è½¨è¿¹ä¿å­˜æˆåŠŸ")
                else:
                    print("âš  æ²¡æœ‰è½¨è¿¹æ•°æ®å¯ä¿å­˜")

            elif key == ord('4'):
                # å›æ”¾è½¨è¿¹
                if trajectory_recorder.start_playback():
                    print("âœ… å¼€å§‹å›æ”¾æ‰‹åŠ¿è½¨è¿¹")
                else:
                    print("âš  æ²¡æœ‰è½¨è¿¹æ•°æ®å¯å›æ”¾")

            elif key == ord('5'):
                # æ¸…é™¤è½¨è¿¹
                if trajectory_recorder.clear_trajectory():
                    print("âœ… è½¨è¿¹æ•°æ®å·²æ¸…é™¤")
                else:
                    print("âš  æ¸…é™¤è½¨è¿¹å¤±è´¥")

            elif key == ord('6'):
                # æš‚åœ/ç»§ç»­å›æ”¾
                if trajectory_recorder.pause_playback():
                    print("âœ… åˆ‡æ¢å›æ”¾æš‚åœçŠ¶æ€")
                else:
                    print("âš  å½“å‰æ²¡æœ‰åœ¨å›æ”¾")

            elif key in key_to_gesture:
                # é”®ç›˜æ§åˆ¶
                simulated_gesture = key_to_gesture[key]
                gesture_recognizer.set_simulated_gesture(simulated_gesture)
                gesture = simulated_gesture
                confidence = 0.9
                if drone_controller.connected and drone_controller.flying:
                    drone_controller.move_by_gesture(gesture, confidence)

            # çœŸå®æ‰‹åŠ¿æ§åˆ¶
            current_time = time.time()
            if (gesture and gesture != "Waiting" and
                    gesture != "æ‘„åƒå¤´é”™è¯¯" and gesture != "Error" and
                    drone_controller.connected and drone_controller.flying):
                drone_controller.move_by_gesture(gesture, confidence)

            # å¤„ç†è½¨è¿¹è®°å½•
            if cap and ret:
                # å¦‚æœæ­£åœ¨å½•åˆ¶ï¼Œæ·»åŠ è½¨è¿¹ç‚¹
                if trajectory_recorder.is_recording:
                    # è·å–æ‰‹åŠ¿è¯†åˆ«çš„æ‰‹éƒ¨æ•°æ®
                    if hasattr(gesture_recognizer, 'last_hand_data'):
                        hand_data = gesture_recognizer.last_hand_data
                        trajectory_recorder.add_trajectory_point(
                            hand_data, gesture, confidence, frame.shape
                        )

                # å¦‚æœæ­£åœ¨å›æ”¾ï¼Œè·å–å›æ”¾ç‚¹
                if trajectory_recorder.is_playing and not trajectory_recorder.playback_paused:
                    playback_point = trajectory_recorder.get_next_playback_point()
                    if playback_point:
                        # è¿™é‡Œå¯ä»¥æ·»åŠ å›æ”¾ç‚¹çš„å¯è§†åŒ–æˆ–å¤„ç†
                        pass

    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nç¨‹åºé”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        print("\næ¸…ç†èµ„æº...")
        if cap:
            cap.release()
        cv2.destroyAllWindows()

        # ç¨‹åºé€€å‡ºè¯­éŸ³æç¤º
        if speech_manager.enabled:
            speech_manager.speak('program_exit', force=True, immediate=True)
            time.sleep(1)

        drone_controller.emergency_stop()

        # è‡ªåŠ¨ä¿å­˜è½¨è¿¹
        if trajectory_recorder and len(trajectory_recorder.trajectory_data) > 0:
            print("è‡ªåŠ¨ä¿å­˜è½¨è¿¹æ•°æ®...")
            trajectory_recorder.save_trajectory()

        config.save_config()

        print("ç¨‹åºå®‰å…¨é€€å‡º")
        print("=" * 60)
        print("\næ„Ÿè°¢ä½¿ç”¨æ‰‹åŠ¿æ§åˆ¶æ— äººæœºç³»ç»Ÿ!")
        input("æŒ‰å›è½¦é”®é€€å‡º...")


# ========== ç¨‹åºå…¥å£ ==========
if __name__ == "__main__":
    main()