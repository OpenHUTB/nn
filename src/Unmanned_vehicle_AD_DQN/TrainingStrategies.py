# TrainingStrategies.py
import os
import math
import pickle
import numpy as np
from datetime import datetime
from collections import deque
import tensorflow as tf
from tensorflow.keras.optimizers import Adam


# é™æ€éšœç¢ç‰©æ£€æµ‹å™¨
class StaticObstacleDetector:
    def __init__(self):
        self.static_obstacle_history = deque(maxlen=50)
        self.collision_patterns = []
        
    def detect_pattern(self, location, heading, action, reward):
        """æ£€æµ‹é™æ€éšœç¢ç‰©ç¢°æ’æ¨¡å¼"""
        if reward < -20:
            pattern = {
                'location': location,
                'heading': heading,
                'action': action,
                'timestamp': datetime.now()
            }
            self.static_obstacle_history.append(pattern)
            
            if len(self.static_obstacle_history) >= 10:
                self.analyze_collision_patterns()
                
    def analyze_collision_patterns(self):
        """åˆ†æç¢°æ’æ¨¡å¼"""
        if len(self.static_obstacle_history) == 0:
            return
            
        print(f"é™æ€éšœç¢ç‰©ç¢°æ’åˆ†æ: æ€»æ¬¡æ•°={len(self.static_obstacle_history)}")
        
    def get_safe_action_suggestion(self, current_location, current_heading):
        """è·å–å®‰å…¨åŠ¨ä½œå»ºè®®"""
        suggestions = []
        
        if len(self.static_obstacle_history) > 0:
            for pattern in list(self.static_obstacle_history)[-5:]:
                loc = pattern['location']
                distance = math.sqrt(
                    (current_location[0] - loc[0])**2 + 
                    (current_location[1] - loc[1])**2
                )
                
                if distance < 10.0:
                    dangerous_action = pattern['action']
                    suggestions.append({
                        'avoid_action': dangerous_action,
                        'suggested_actions': [0, 1, 2] if dangerous_action in [3, 4] else [3, 4],
                        'reason': 'å†å²ç¢°æ’åŒºåŸŸ'
                    })
        
        return suggestions


# è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨
class CurriculumManager:
    def __init__(self, env):
        self.env = env
        self.current_stage = 0
        self.stage_thresholds = [0.3, 0.5, 0.7, 0.85, 0.9]
        
        self.stage_configs = [
            # é˜¶æ®µ0: å…¥é—¨
            {
                'pedestrian_cross': 2,
                'pedestrian_normal': 1,
                'static_obstacle_penalty': 0.5,
                'max_episode_steps': 800,
                'success_threshold': 0.3,
                'difficulty_name': 'å…¥é—¨'
            },
            # é˜¶æ®µ1: ç®€å•
            {
                'pedestrian_cross': 4,
                'pedestrian_normal': 2,
                'static_obstacle_penalty': 1.0,
                'max_episode_steps': 1000,
                'success_threshold': 0.4,
                'difficulty_name': 'ç®€å•'
            },
            # é˜¶æ®µ2: ä¸­ç­‰
            {
                'pedestrian_cross': 6,
                'pedestrian_normal': 3,
                'static_obstacle_penalty': 2.0,
                'max_episode_steps': 1200,
                'success_threshold': 0.5,
                'difficulty_name': 'ä¸­ç­‰'
            },
            # é˜¶æ®µ3: å›°éš¾
            {
                'pedestrian_cross': 8,
                'pedestrian_normal': 4,
                'static_obstacle_penalty': 3.0,
                'max_episode_steps': 1500,
                'success_threshold': 0.6,
                'difficulty_name': 'å›°éš¾'
            },
            # é˜¶æ®µ4: ä¸“å®¶
            {
                'pedestrian_cross': 10,
                'pedestrian_normal': 6,
                'static_obstacle_penalty': 4.0,
                'max_episode_steps': 1800,
                'success_threshold': 0.7,
                'difficulty_name': 'ä¸“å®¶'
            },
            # é˜¶æ®µ5: å¤§å¸ˆ
            {
                'pedestrian_cross': 12,
                'pedestrian_normal': 8,
                'static_obstacle_penalty': 5.0,
                'max_episode_steps': 2400,
                'success_threshold': 0.8,
                'difficulty_name': 'å¤§å¸ˆ'
            }
        ]
        
        # è®­ç»ƒå†å²
        self.success_history = deque(maxlen=20)
        self.reward_history = deque(maxlen=50)
        self.reaction_time_history = deque(maxlen=50)
        self.static_collision_history = deque(maxlen=20)
        
        # é™æ€éšœç¢ç‰©æ£€æµ‹å™¨
        self.static_detector = StaticObstacleDetector()
        
    def update_stage(self, success, reward, reaction_time=None, static_collision=False):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µ"""
        self.success_history.append(1 if success else 0)
        self.reward_history.append(reward)
        if reaction_time is not None:
            self.reaction_time_history.append(reaction_time)
        if static_collision:
            self.static_collision_history.append(1)
        else:
            self.static_collision_history.append(0)
        
        if len(self.success_history) >= 10:
            success_rate = sum(self.success_history) / len(self.success_history)
            avg_reward = np.mean(self.reward_history) if self.reward_history else 0
            
            static_collision_rate = sum(self.static_collision_history) / len(self.static_collision_history) if self.static_collision_history else 0
            
            if len(self.success_history) % 20 == 0:
                stage_info = self.get_current_config()
                print(f"è¯¾ç¨‹å­¦ä¹  - é˜¶æ®µ: {self.current_stage}({stage_info['difficulty_name']})")
                print(f"  æˆåŠŸç‡: {success_rate:.2f}, å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
                print(f"  é™æ€ç¢°æ’ç‡: {static_collision_rate:.2f}")
                if self.reaction_time_history:
                    avg_rt = np.mean(self.reaction_time_history)
                    print(f"  å¹³å‡ååº”æ—¶é—´: {avg_rt:.2f}ç§’")
            
            if self.current_stage < len(self.stage_configs) - 1:
                next_stage_threshold = self.stage_configs[self.current_stage]['success_threshold']
                
                can_advance = (
                    success_rate >= next_stage_threshold and 
                    avg_reward > 3 and
                    static_collision_rate < 0.2
                )
                
                if can_advance:
                    self.current_stage += 1
                    print(f"ğŸ‰ è¯¾ç¨‹å­¦ä¹ : è¿›é˜¶åˆ°é˜¶æ®µ {self.current_stage}!")
                    print(f"   æ–°é…ç½®: {self.stage_configs[self.current_stage]['difficulty_name']}")
                    return True
                    
            if self.current_stage > 0 and (
                success_rate < 0.2 or 
                static_collision_rate > 0.4 or
                (self.reaction_time_history and np.mean(self.reaction_time_history) > 2.0)
            ):
                self.current_stage -= 1
                print(f"âš ï¸ è¯¾ç¨‹å­¦ä¹ : é€€å›é˜¶æ®µ {self.current_stage}")
                return True
        
        return False
    
    def get_current_config(self):
        """è·å–å½“å‰é˜¶æ®µçš„é…ç½®"""
        return self.stage_configs[min(self.current_stage, len(self.stage_configs) - 1)]
    
    def apply_to_environment(self):
        """å°†å½“å‰é˜¶æ®µé…ç½®åº”ç”¨åˆ°ç¯å¢ƒ"""
        config = self.get_current_config()
        return config


# å¤šç›®æ ‡ä¼˜åŒ–å™¨
class MultiObjectiveOptimizer:
    def __init__(self):
        self.objectives = {
            'reaction_time': {
                'weight': 0.20,
                'description': 'å¿«é€Ÿååº”é¿éšœ',
                'metrics': ['reaction_time', 'proactive_actions']
            },
            'safety': {
                'weight': 0.35,
                'description': 'å®‰å…¨é¿éšœå’Œé¿å…ç¢°æ’',
                'metrics': ['collision_avoidance', 'pedestrian_distance', 'static_obstacle_distance']
            },
            'efficiency': {
                'weight': 0.20,
                'description': 'å¿«é€Ÿåˆ°è¾¾ç›®çš„åœ°',
                'metrics': ['progress_speed', 'total_time']
            },
            'comfort': {
                'weight': 0.15,
                'description': 'å¹³ç¨³é©¾é©¶ä½“éªŒ',
                'metrics': ['smoothness', 'steering_changes']
            },
            'static_avoidance': {
                'weight': 0.10,
                'description': 'é¿å…é™æ€éšœç¢ç‰©',
                'metrics': ['static_collision', 'static_distance']
            }
        }
        
        self.metrics_history = {
            'reaction_time': [],
            'safety': [],
            'efficiency': [],
            'comfort': [],
            'static_avoidance': []
        }
        
    def compute_composite_reward(self, metrics):
        """è®¡ç®—ç»¼åˆå¥–åŠ±å€¼"""
        composite = 0
        
        for obj_name, obj_info in self.objectives.items():
            if obj_name in metrics:
                normalized_value = self._normalize_metric(metrics[obj_name], obj_name)
                composite += normalized_value * obj_info['weight']
                
                self.metrics_history[obj_name].append(normalized_value)
        
        # ç‰¹æ®Šå¥–åŠ±/æƒ©ç½šé¡¹
        if metrics.get('collision', False):
            composite -= 12
        if metrics.get('static_collision', False):
            composite -= 15
        if metrics.get('off_road', False):
            composite -= 8
            
        if 'reaction_time' in metrics:
            rt = metrics['reaction_time']
            if rt < 0.3:
                composite += 3
            elif rt > 1.2:
                composite -= 4
        
        if metrics.get('proactive_action', False):
            composite += 2.0
            
        if metrics.get('static_distance', 100) > 15:
            composite += 1.0
        elif metrics.get('static_distance', 100) < 5:
            composite -= 3.0
            
        return composite
    
    def _normalize_metric(self, value, metric_name):
        """å½’ä¸€åŒ–æŒ‡æ ‡å€¼åˆ°[0, 1]èŒƒå›´"""
        normalization_rules = {
            'reaction_time': lambda x: max(0, 1 - x/3),
            'safety': lambda x: min(max(x / 10, 0), 1),
            'efficiency': lambda x: min(max(x / 100, 0), 1),
            'comfort': lambda x: min(max((x + 5) / 10, 0), 1),
            'static_avoidance': lambda x: min(max(1 - x/5, 0), 1)
        }
        
        if metric_name in normalization_rules:
            return normalization_rules[metric_name](value)
        return min(max(value, 0), 1)
    
    def adjust_weights(self, performance_feedback):
        """æ ¹æ®æ€§èƒ½åé¦ˆåŠ¨æ€è°ƒæ•´æƒé‡"""
        recent_performance = {}
        for obj in self.objectives:
            if len(self.metrics_history[obj]) >= 10:
                recent_avg = np.mean(self.metrics_history[obj][-10:])
                recent_performance[obj] = recent_avg
        
        if recent_performance:
            worst_obj = min(recent_performance, key=recent_performance.get)
            best_obj = max(recent_performance, key=recent_performance.get)
            
            if recent_performance[worst_obj] < 0.3:
                adjustment = 0.04
                self.objectives[worst_obj]['weight'] += adjustment
                self.objectives[best_obj]['weight'] -= adjustment
                
                total = sum(obj['weight'] for obj in self.objectives.values())
                for obj in self.objectives:
                    self.objectives[obj]['weight'] /= total
                
                if adjustment != 0:
                    print(f"åŠ¨æ€æƒé‡è°ƒæ•´: {worst_obj}æƒé‡â†‘ {adjustment:.3f}, {best_obj}æƒé‡â†“ {adjustment:.3f}")
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = "å¤šç›®æ ‡ä¼˜åŒ–æ€§èƒ½æŠ¥å‘Š:\n"
        report += "=" * 50 + "\n"
        
        for obj_name, obj_info in self.objectives.items():
            history = self.metrics_history[obj_name]
            if history:
                avg = np.mean(history[-20:]) if len(history) >= 20 else np.mean(history)
                report += f"{obj_name}(æƒé‡:{obj_info['weight']:.2f}): å¹³å‡å¾—åˆ†={avg:.3f}\n"
                report += f"  æè¿°: {obj_info['description']}\n"
        
        return report


# æ¨¡ä»¿å­¦ä¹ ç®¡ç†å™¨
class ImitationLearningManager:
    def __init__(self, expert_data_path=None):
        self.expert_data_path = expert_data_path
        self.expert_data = []
        self.is_pretrained = False
        
    def load_expert_data(self, path):
        """åŠ è½½ä¸“å®¶ç¤ºèŒƒæ•°æ®"""
        try:
            if os.path.exists(path):
                with open(path, 'rb') as f:
                    self.expert_data = pickle.load(f)
                print(f"å·²åŠ è½½ {len(self.expert_data)} æ¡ä¸“å®¶ç¤ºèŒƒæ•°æ®")
                return True
            else:
                print(f"ä¸“å®¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False
        except Exception as e:
            print(f"åŠ è½½ä¸“å®¶æ•°æ®å¤±è´¥: {e}")
            return False


# ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒº
class PrioritizedReplayBuffer:
    def __init__(self, max_size=20000, alpha=0.7, beta_start=0.5, beta_frames=50000):
        self.max_size = max_size
        self.alpha = alpha
        self.beta_start = beta_start
        self.beta_frames = beta_frames
        self.frame = 1
        
        self.buffer = deque(maxlen=max_size)
        self.priorities = deque(maxlen=max_size)
        
    def __len__(self):
        return len(self.buffer)
    
    def beta(self):
        """çº¿æ€§é€’å¢çš„betaå€¼ï¼Œç”¨äºé‡è¦æ€§é‡‡æ ·æƒé‡"""
        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)
    
    def add(self, experience, error=None):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        if error is None:
            priority = max(self.priorities) if self.priorities else 1.0
        else:
            priority = (abs(error) + 1e-5) ** self.alpha
            
        state, action, reward, next_state, done = experience
        
        # é™æ€ç¢°æ’æ£€æµ‹
        if reward < -20:
            priority *= 2.0
            
        elif reward < -5:
            priority *= 1.5
            
        self.buffer.append(experience)
        self.priorities.append(priority)
        
    def sample(self, batch_size):
        """ä»ç¼“å†²åŒºä¸­é‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        if len(self.buffer) == 0:
            return [], [], []
            
        priorities = np.array(self.priorities, dtype=np.float32)
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), p=probs, replace=False)
        
        samples = [self.buffer[i] for i in indices]
        
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta())
        weights /= weights.max()
        
        self.frame += 1
        
        return indices, samples, weights
    
    def update_priorities(self, indices, errors):
        """æ›´æ–°é‡‡æ ·ç»éªŒçš„ä¼˜å…ˆçº§"""
        for idx, error in zip(indices, errors):
            if 0 <= idx < len(self.priorities):
                self.priorities[idx] = (abs(error) + 1e-5) ** self.alpha