# TrainingStrategies.py
import os
import math
import pickle
import numpy as np
from datetime import datetime
from collections import deque
import tensorflow as tf
from tensorflow.keras.optimizers import Adam


# è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨
class CurriculumManager:
    def __init__(self, env):
        self.env = env
        self.current_stage = 0
        self.stage_thresholds = [0.3, 0.5, 0.7, 0.85]  # æˆåŠŸç‡é˜ˆå€¼
        self.stage_configs = [
            # é˜¶æ®µ0: å…¥é—¨
            {
                'pedestrian_cross': 2,      # åå­—è·¯å£è¡Œäººæ•°é‡ï¼ˆå‡å°‘ï¼‰
                'pedestrian_normal': 1,     # æ™®é€šè·¯æ®µè¡Œäººæ•°é‡ï¼ˆå‡å°‘ï¼‰
                'pedestrian_speed_min': 0.5,  # è¡Œäººæœ€ä½é€Ÿåº¦
                'pedestrian_speed_max': 1.0,  # è¡Œäººæœ€é«˜é€Ÿåº¦
                'max_episode_steps': 1200,   # æœ€å¤§æ­¥æ•° (20ç§’ * 60FPS)
                'success_threshold': 0.3     # è¿›å…¥ä¸‹ä¸€é˜¶æ®µæˆåŠŸç‡
            },
            # é˜¶æ®µ1: åˆçº§
            {
                'pedestrian_cross': 4,      # é€æ­¥å¢åŠ 
                'pedestrian_normal': 2,
                'pedestrian_speed_min': 0.7,
                'pedestrian_speed_max': 1.3,
                'max_episode_steps': 1800,   # 30ç§’
                'success_threshold': 0.5
            },
            # é˜¶æ®µ2: ä¸­çº§
            {
                'pedestrian_cross': 6,
                'pedestrian_normal': 3,
                'pedestrian_speed_min': 0.8,
                'pedestrian_speed_max': 1.5,
                'max_episode_steps': 2400,   # 40ç§’
                'success_threshold': 0.7
            },
            # é˜¶æ®µ3: é«˜çº§ (æ­£å¸¸éš¾åº¦)
            {
                'pedestrian_cross': 8,
                'pedestrian_normal': 4,
                'pedestrian_speed_min': 1.0,
                'pedestrian_speed_max': 2.0,
                'max_episode_steps': 3600,   # 60ç§’
                'success_threshold': 0.85
            },
            # é˜¶æ®µ4: ä¸“å®¶ (æŒ‘æˆ˜)
            {
                'pedestrian_cross': 10,     # é€‚å½“å‡å°‘
                'pedestrian_normal': 5,
                'pedestrian_speed_min': 1.2,
                'pedestrian_speed_max': 2.5,
                'max_episode_steps': 3600,
                'success_threshold': 0.9
            }
        ]
        
        # è®­ç»ƒå†å²
        self.success_history = deque(maxlen=20)  # è®°å½•æœ€è¿‘20è½®çš„æˆåŠŸæƒ…å†µ
        self.reward_history = deque(maxlen=50)   # è®°å½•æœ€è¿‘50è½®çš„å¥–åŠ±
        
    def update_stage(self, success, reward):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µ"""
        # è®°å½•å†å²
        self.success_history.append(1 if success else 0)
        self.reward_history.append(reward)
        
        # è®¡ç®—æœ€è¿‘æˆåŠŸç‡
        if len(self.success_history) >= 10:
            success_rate = sum(self.success_history) / len(self.success_history)
            avg_reward = np.mean(self.reward_history) if self.reward_history else 0
            
            # å‡å°‘æ‰“å°é¢‘ç‡
            if len(self.success_history) % 10 == 0:
                print(f"è¯¾ç¨‹å­¦ä¹  - å½“å‰é˜¶æ®µ: {self.current_stage}, æˆåŠŸç‡: {success_rate:.2f}, å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            if self.current_stage < len(self.stage_configs) - 1:
                next_stage_threshold = self.stage_configs[self.current_stage]['success_threshold']
                if success_rate >= next_stage_threshold and avg_reward > 5:
                    self.current_stage += 1
                    print(f"ğŸ‰ è¯¾ç¨‹å­¦ä¹ : è¿›é˜¶åˆ°é˜¶æ®µ {self.current_stage}!")
                    return True
                    
            # å¦‚æœè¡¨ç°å¤ªå·®ï¼Œé€€å›ä¸Šä¸€é˜¶æ®µ
            if self.current_stage > 0 and success_rate < 0.2:
                self.current_stage -= 1
                print(f"âš ï¸ è¯¾ç¨‹å­¦ä¹ : é€€å›é˜¶æ®µ {self.current_stage}")
                return True
        
        return False
    
    def get_current_config(self):
        """è·å–å½“å‰é˜¶æ®µçš„é…ç½®"""
        return self.stage_configs[min(self.current_stage, len(self.stage_configs) - 1)]
    
    def apply_to_environment(self):
        """å°†å½“å‰é˜¶æ®µé…ç½®åº”ç”¨åˆ°ç¯å¢ƒ"""
        config = self.get_current_config()
        return config


# å¤šç›®æ ‡ä¼˜åŒ–å™¨
class MultiObjectiveOptimizer:
    def __init__(self):
        # å®šä¹‰ä¼˜åŒ–ç›®æ ‡åŠå…¶æƒé‡ï¼ˆå¯åŠ¨æ€è°ƒæ•´ï¼‰
        self.objectives = {
            'safety': {
                'weight': 0.35,  # ç¨å¾®é™ä½å®‰å…¨æƒé‡
                'description': 'å®‰å…¨é¿éšœå’Œé¿å…ç¢°æ’',
                'metrics': ['collision_avoidance', 'pedestrian_distance']
            },
            'efficiency': {
                'weight': 0.30,  # æé«˜æ•ˆç‡æƒé‡
                'description': 'å¿«é€Ÿåˆ°è¾¾ç›®çš„åœ°',
                'metrics': ['progress_speed', 'total_time']
            },
            'comfort': {
                'weight': 0.20,
                'description': 'å¹³ç¨³é©¾é©¶ä½“éªŒ',
                'metrics': ['smoothness', 'steering_changes']
            },
            'rule_following': {
                'weight': 0.15,
                'description': 'éµå®ˆäº¤é€šè§„åˆ™',
                'metrics': ['lane_keeping', 'speed_limit']
            }
        }
        
        # æŒ‡æ ‡è·Ÿè¸ª
        self.metrics_history = {
            'safety': [],
            'efficiency': [],
            'comfort': [],
            'rule_following': []
        }
        
    def compute_composite_reward(self, metrics):
        """è®¡ç®—ç»¼åˆå¥–åŠ±å€¼"""
        composite = 0
        
        for obj_name, obj_info in self.objectives.items():
            if obj_name in metrics:
                # å½’ä¸€åŒ–å¤„ç†æ¯ä¸ªç›®æ ‡çš„è´¡çŒ®
                normalized_value = self._normalize_metric(metrics[obj_name], obj_name)
                composite += normalized_value * obj_info['weight']
                
                # è®°å½•æŒ‡æ ‡å†å²
                self.metrics_history[obj_name].append(normalized_value)
        
        # ç‰¹æ®Šæƒ©ç½šé¡¹
        if metrics.get('collision', False):
            composite -= 8  # å‡å°‘ç¢°æ’æƒ©ç½š
        if metrics.get('off_road', False):
            composite -= 3  # å‡å°‘åç¦»é“è·¯æƒ©ç½š
        if metrics.get('dangerous_action', False):
            composite -= 2  # å‡å°‘å±é™©åŠ¨ä½œæƒ©ç½š
            
        return composite
    
    def _normalize_metric(self, value, metric_name):
        """å½’ä¸€åŒ–æŒ‡æ ‡å€¼åˆ°[0, 1]èŒƒå›´"""
        # ä¸åŒæŒ‡æ ‡çš„å½’ä¸€åŒ–æ–¹å¼ä¸åŒ
        normalization_rules = {
            'safety': lambda x: min(max(x / 10, 0), 1),  # å‡è®¾å®‰å…¨åˆ†æ»¡åˆ†10
            'efficiency': lambda x: min(max(x / 100, 0), 1),  # æ•ˆç‡åˆ†æ»¡åˆ†100
            'comfort': lambda x: min(max((x + 5) / 10, 0), 1),  # èˆ’é€‚åº¦[-5, 5] -> [0, 1]
            'rule_following': lambda x: min(max(x, 0), 1)  # è§„åˆ™éµå¾ªåº¦[0, 1]
        }
        
        if metric_name in normalization_rules:
            return normalization_rules[metric_name](value)
        return min(max(value, 0), 1)  # é»˜è®¤æˆªæ–­åˆ°[0, 1]
    
    def adjust_weights(self, performance_feedback):
        """æ ¹æ®æ€§èƒ½åé¦ˆåŠ¨æ€è°ƒæ•´æƒé‡"""
        # å¦‚æœæŸä¸ªç›®æ ‡è¡¨ç°æŒç»­è¾ƒå·®ï¼Œå¢åŠ å…¶æƒé‡
        recent_performance = {}
        for obj in self.objectives:
            if len(self.metrics_history[obj]) >= 10:
                recent_avg = np.mean(self.metrics_history[obj][-10:])
                recent_performance[obj] = recent_avg
        
        if recent_performance:
            # æ‰¾åˆ°è¡¨ç°æœ€å·®çš„ç›®æ ‡
            worst_obj = min(recent_performance, key=recent_performance.get)
            best_obj = max(recent_performance, key=recent_performance.get)
            
            # å¦‚æœæœ€å·®ç›®æ ‡è¡¨ç°ä½äºé˜ˆå€¼ï¼Œå¢åŠ å…¶æƒé‡
            if recent_performance[worst_obj] < 0.3:
                adjustment = 0.02  # å‡å°‘è°ƒæ•´å¹…åº¦
                self.objectives[worst_obj]['weight'] += adjustment
                self.objectives[best_obj]['weight'] -= adjustment
                
                # ç¡®ä¿æƒé‡æ€»å’Œä¸º1
                total = sum(obj['weight'] for obj in self.objectives.values())
                for obj in self.objectives:
                    self.objectives[obj]['weight'] /= total
                
                if adjustment != 0:
                    print(f"åŠ¨æ€æƒé‡è°ƒæ•´: {worst_obj}æƒé‡â†‘ {adjustment:.3f}, {best_obj}æƒé‡â†“ {adjustment:.3f}")
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = "å¤šç›®æ ‡ä¼˜åŒ–æ€§èƒ½æŠ¥å‘Š:\n"
        report += "=" * 50 + "\n"
        
        for obj_name, obj_info in self.objectives.items():
            history = self.metrics_history[obj_name]
            if history:
                avg = np.mean(history[-20:]) if len(history) >= 20 else np.mean(history)
                report += f"{obj_name}(æƒé‡:{obj_info['weight']:.2f}): å¹³å‡å¾—åˆ†={avg:.3f}\n"
                report += f"  æè¿°: {obj_info['description']}\n"
        
        return report


# æ¨¡ä»¿å­¦ä¹ ç®¡ç†å™¨
class ImitationLearningManager:
    def __init__(self, expert_data_path=None):
        self.expert_data_path = expert_data_path
        self.expert_data = []
        self.is_pretrained = False
        
    def load_expert_data(self, path):
        """åŠ è½½ä¸“å®¶ç¤ºèŒƒæ•°æ®"""
        try:
            if os.path.exists(path):
                with open(path, 'rb') as f:
                    self.expert_data = pickle.load(f)
                print(f"å·²åŠ è½½ {len(self.expert_data)} æ¡ä¸“å®¶ç¤ºèŒƒæ•°æ®")
                return True
            else:
                print(f"ä¸“å®¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False
        except Exception as e:
            print(f"åŠ è½½ä¸“å®¶æ•°æ®å¤±è´¥: {e}")
            return False
    
    def collect_expert_demonstration(self, env, num_episodes=10):
        """æ”¶é›†ä¸“å®¶ç¤ºèŒƒæ•°æ®ï¼ˆå¯ä»¥æ‰‹åŠ¨æ§åˆ¶æˆ–ä½¿ç”¨è§„åˆ™æ§åˆ¶å™¨ï¼‰"""
        print(f"å¼€å§‹æ”¶é›†ä¸“å®¶ç¤ºèŒƒæ•°æ® ({num_episodes}ä¸ªepisodes)...")
        
        demonstrations = []
        
        for episode in range(num_episodes):
            print(f"æ”¶é›†ä¸“å®¶ç¤ºèŒƒ Episode {episode + 1}/{num_episodes}")
            
            state = env.reset(episode)
            done = False
            episode_data = []
            
            step_count = 0
            max_steps = 60 * 60  # æœ€å¤§60ç§’
            
            while not done and step_count < max_steps:
                # è¿™é‡Œå¯ä»¥ä½¿ç”¨è§„åˆ™æ§åˆ¶å™¨æˆ–æ‰‹åŠ¨æ§åˆ¶
                # ç¤ºä¾‹ï¼šç®€å•çš„è§„åˆ™æ§åˆ¶å™¨
                action = self._rule_based_controller(env)
                
                new_state, reward, done, _ = env.step(action)
                
                # ä¿å­˜ç¤ºèŒƒæ•°æ®
                episode_data.append({
                    'state': state.copy(),
                    'action': action,
                    'reward': reward,
                    'next_state': new_state.copy(),
                    'done': done
                })
                
                state = new_state
                step_count += 1
            
            demonstrations.extend(episode_data)
            env.cleanup_actors()
        
        # ä¿å­˜ä¸“å®¶æ•°æ®
        self.expert_data = demonstrations
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = f"expert_data_{timestamp}.pkl"
        
        with open(save_path, 'wb') as f:
            pickle.dump(demonstrations, f)
        
        print(f"ä¸“å®¶ç¤ºèŒƒæ•°æ®å·²ä¿å­˜åˆ°: {save_path}, å…± {len(demonstrations)} æ¡è®°å½•")
        return True
    
    def _rule_based_controller(self, env):
        """åŸºäºè§„åˆ™çš„æ§åˆ¶å™¨ï¼ˆä½œä¸ºä¸“å®¶ç¤ºèŒƒï¼‰"""
        # è·å–è½¦è¾†çŠ¶æ€
        vehicle_location = env.vehicle.get_location()
        velocity = env.vehicle.get_velocity()
        speed_kmh = 3.6 * math.sqrt(velocity.x**2 + velocity.y**2)
        
        # ç®€å•è§„åˆ™ï¼šä¿æŒé€Ÿåº¦åœ¨20-35 km/hï¼Œé¿å…éšœç¢ç‰©
        if speed_kmh < 20:
            return 2  # åŠ é€Ÿ
        elif speed_kmh > 35:
            return 0  # å‡é€Ÿ
        else:
            # æ£€æŸ¥å‰æ–¹éšœç¢ç‰©
            has_obstacle_ahead = self._check_obstacle_ahead(env)
            if has_obstacle_ahead:
                return 0  # å‡é€Ÿ
            else:
                return 1  # ä¿æŒ
        
        return 1  # é»˜è®¤ä¿æŒ
    
    def _check_obstacle_ahead(self, env):
        """æ£€æŸ¥å‰æ–¹æ˜¯å¦æœ‰éšœç¢ç‰©ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„éšœç¢ç‰©æ£€æµ‹é€»è¾‘
        # æš‚æ—¶è¿”å›False
        return False
    
    def pretrain_with_behavioral_cloning(self, model, epochs=20):
        """ä½¿ç”¨è¡Œä¸ºå…‹éš†è¿›è¡Œé¢„è®­ç»ƒ"""
        if not self.expert_data:
            print("æ²¡æœ‰ä¸“å®¶æ•°æ®å¯ç”¨ï¼Œè·³è¿‡é¢„è®­ç»ƒ")
            return model
        
        print(f"å¼€å§‹è¡Œä¸ºå…‹éš†é¢„è®­ç»ƒ ({epochs}ä¸ªepochs)...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        states = []
        actions = []
        
        for demo in self.expert_data:
            states.append(demo['state'])
            actions.append(demo['action'])
        
        # å°†çŠ¶æ€å½’ä¸€åŒ–
        states = np.array(states) / 255.0
        
        # å°†åŠ¨ä½œè½¬æ¢ä¸ºone-hotç¼–ç 
        actions_onehot = tf.keras.utils.to_categorical(actions, num_classes=5)
        
        # å¤‡ä»½åŸå§‹ç¼–è¯‘è®¾ç½®
        original_loss = model.loss
        original_optimizer = model.optimizer
        original_metrics = model.metrics_names
        
        # é‡æ–°ç¼–è¯‘æ¨¡å‹ç”¨äºåˆ†ç±»ä»»åŠ¡
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # è®­ç»ƒæ¨¡å‹æ¨¡ä»¿ä¸“å®¶è¡Œä¸º
        history = model.fit(
            states, actions_onehot,
            batch_size=32,
            epochs=epochs,
            validation_split=0.2,
            verbose=1
        )
        
        print(f"é¢„è®­ç»ƒå®Œæˆ - æœ€ç»ˆå‡†ç¡®ç‡: {history.history['accuracy'][-1]:.3f}")
        
        # æ¢å¤åŸå§‹ç¼–è¯‘è®¾ç½®
        model.compile(
            optimizer=original_optimizer,
            loss=original_loss,
            metrics=original_metrics
        )
        
        self.is_pretrained = True
        return model
    
    def train_with_dagger(self, model, env, iterations=5, episodes_per_iter=5):
        """ä½¿ç”¨DAggerç®—æ³•è¿›è¡Œè®­ç»ƒ"""
        print(f"å¼€å§‹DAggerè®­ç»ƒ ({iterations}æ¬¡è¿­ä»£ï¼Œæ¯æ¬¡{episodes_per_iter}ä¸ªepisodes)...")
        
        aggregated_data = self.expert_data.copy()
        
        for iteration in range(iterations):
            print(f"\nDAgger è¿­ä»£ {iteration + 1}/{iterations}")
            
            # ä½¿ç”¨å½“å‰ç­–ç•¥æ”¶é›†æ•°æ®
            new_demos = []
            
            for episode in range(episodes_per_iter):
                print(f"  æ”¶é›†æ•°æ® Episode {episode + 1}/{episodes_per_iter}")
                
                state = env.reset(episode)
                done = False
                step_count = 0
                max_steps = 60 * 60
                
                while not done and step_count < max_steps:
                    # ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                    qs = model.predict(np.array(state).reshape(-1, *state.shape) / 255, verbose=0)[0]
                    action = np.argmax(qs)
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    new_state, reward, done, _ = env.step(action)
                    
                    # ä¸“å®¶çº æ­£ï¼ˆè¿™é‡Œå¯ä»¥æ·»åŠ ä¸“å®¶çº æ­£é€»è¾‘ï¼‰
                    # å¦‚æœç­–ç•¥åŠ¨ä½œä¸ä¸“å®¶å»ºè®®ä¸åŒï¼Œä½¿ç”¨ä¸“å®¶åŠ¨ä½œ
                    expert_action = self._rule_based_controller(env)
                    
                    # ä¿å­˜æ•°æ®ï¼ˆä½¿ç”¨ä¸“å®¶çº æ­£åçš„åŠ¨ä½œï¼‰
                    new_demos.append({
                        'state': state.copy(),
                        'action': expert_action,  # ä½¿ç”¨ä¸“å®¶åŠ¨ä½œ
                        'reward': reward,
                        'next_state': new_state.copy(),
                        'done': done
                    })
                    
                    state = new_state
                    step_count += 1
                
                env.cleanup_actors()
            
            # åˆå¹¶æ•°æ®
            aggregated_data.extend(new_demos)
            
            # åœ¨åˆå¹¶æ•°æ®ä¸Šé‡æ–°è®­ç»ƒ
            states = [d['state'] for d in aggregated_data]
            actions = [d['action'] for d in aggregated_data]
            
            states = np.array(states) / 255.0
            actions_onehot = tf.keras.utils.to_categorical(actions, num_classes=5)
            
            # å¤‡ä»½åŸå§‹ç¼–è¯‘è®¾ç½®
            original_loss = model.loss
            original_optimizer = model.optimizer
            original_metrics = model.metrics_names
            
            # é‡æ–°ç¼–è¯‘ç”¨äºåˆ†ç±»
            model.compile(
                optimizer=Adam(learning_rate=0.0001),
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            
            # è®­ç»ƒæ¨¡å‹
            history = model.fit(
                states, actions_onehot,
                batch_size=32,
                epochs=10,
                validation_split=0.1,
                verbose=0
            )
            
            # æ¢å¤åŸå§‹ç¼–è¯‘è®¾ç½®
            model.compile(
                optimizer=original_optimizer,
                loss=original_loss,
                metrics=original_metrics
            )
            
            print(f"  è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {history.history['accuracy'][-1]:.3f}")
        
        print("DAggerè®­ç»ƒå®Œæˆ!")
        return model


# ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒº
class PrioritizedReplayBuffer:
    def __init__(self, max_size=10000, alpha=0.6, beta_start=0.4, beta_frames=100000):
        self.max_size = max_size
        self.alpha = alpha  # ä¼˜å…ˆçº§ç¨‹åº¦ (0 = å‡åŒ€é‡‡æ ·, 1 = å®Œå…¨ä¼˜å…ˆçº§)
        self.beta_start = beta_start  # é‡è¦æ€§é‡‡æ ·æƒé‡èµ·å§‹å€¼
        self.beta_frames = beta_frames  # betaçº¿æ€§å¢é•¿çš„å¸§æ•°
        self.frame = 1
        
        # ä½¿ç”¨å¾ªç¯ç¼“å†²åŒº
        self.buffer = deque(maxlen=max_size)
        self.priorities = deque(maxlen=max_size)
        
    def __len__(self):
        return len(self.buffer)
    
    def beta(self):
        """çº¿æ€§é€’å¢çš„betaå€¼ï¼Œç”¨äºé‡è¦æ€§é‡‡æ ·æƒé‡"""
        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)
    
    def add(self, experience, error=None):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        if error is None:
            priority = max(self.priorities) if self.priorities else 1.0
        else:
            priority = (abs(error) + 1e-5) ** self.alpha
            
        self.buffer.append(experience)
        self.priorities.append(priority)
        
    def sample(self, batch_size):
        """ä»ç¼“å†²åŒºä¸­é‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        if len(self.buffer) == 0:
            return [], [], []
            
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        priorities = np.array(self.priorities, dtype=np.float32)
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), p=probs, replace=False)
        
        # è·å–æ ·æœ¬
        samples = [self.buffer[i] for i in indices]
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta())
        weights /= weights.max()  # å½’ä¸€åŒ–
        
        # æ›´æ–°å¸§è®¡æ•°å™¨
        self.frame += 1
        
        return indices, samples, weights
    
    def update_priorities(self, indices, errors):
        """æ›´æ–°é‡‡æ ·ç»éªŒçš„ä¼˜å…ˆçº§"""
        for idx, error in zip(indices, errors):
            if 0 <= idx < len(self.priorities):
                self.priorities[idx] = (abs(error) + 1e-5) ** self.alpha