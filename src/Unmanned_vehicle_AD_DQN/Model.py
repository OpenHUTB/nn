# Model.py
import glob
import os
import sys
import random
import time
import numpy as np
import cv2
import math
import matplotlib.pyplot as plt
from collections import deque
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Concatenate, Conv2D, AveragePooling2D, Activation, \
    Flatten, Dropout, BatchNormalization, MaxPooling2D, Multiply, Add, Lambda, Subtract
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.callbacks import TensorBoard
import tensorflow as tf
import tensorflow.keras.backend as backend
from threading import Thread
from Environment import *
from Hyperparameters import *
import pickle
import json
from datetime import datetime


# è‡ªå®šä¹‰TensorBoardç±»
class ModifiedTensorBoard(TensorBoard):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._log_write_dir = self.log_dir
        self.step = 1
        self.writer = tf.summary.create_file_writer(self.log_dir)

    def set_model(self, model):
        self.model = model
        self._train_dir = os.path.join(self._log_write_dir, 'train')
        self._train_step = self.model._train_counter
        self._val_dir = os.path.join(self._log_write_dir, 'validation')
        self._val_step = self.model._test_counter
        self._should_write_train_graph = False

    def on_epoch_end(self, epoch, logs=None):
        self.update_stats(**logs)

    def on_batch_end(self, batch, logs=None):
        pass

    def on_train_end(self, logs=None):
        pass

    def update_stats(self, **stats):
        with self.writer.as_default():
            for key, value in stats.items():
                tf.summary.scalar(key, value, step=self.step)
                self.writer.flush()


# ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒº
class PrioritizedReplayBuffer:
    def __init__(self, max_size=REPLAY_MEMORY_SIZE, alpha=0.6, beta_start=0.4, beta_frames=100000):
        self.max_size = max_size
        self.alpha = alpha  # ä¼˜å…ˆçº§ç¨‹åº¦ (0 = å‡åŒ€é‡‡æ ·, 1 = å®Œå…¨ä¼˜å…ˆçº§)
        self.beta_start = beta_start  # é‡è¦æ€§é‡‡æ ·æƒé‡èµ·å§‹å€¼
        self.beta_frames = beta_frames  # betaçº¿æ€§å¢é•¿çš„å¸§æ•°
        self.frame = 1
        
        # ä½¿ç”¨å¾ªç¯ç¼“å†²åŒº
        self.buffer = deque(maxlen=max_size)
        self.priorities = deque(maxlen=max_size)
        
    def __len__(self):
        return len(self.buffer)
    
    def beta(self):
        """çº¿æ€§é€’å¢çš„betaå€¼ï¼Œç”¨äºé‡è¦æ€§é‡‡æ ·æƒé‡"""
        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)
    
    def add(self, experience, error=None):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        if error is None:
            priority = max(self.priorities) if self.priorities else 1.0
        else:
            priority = (abs(error) + 1e-5) ** self.alpha
            
        self.buffer.append(experience)
        self.priorities.append(priority)
        
    def sample(self, batch_size):
        """ä»ç¼“å†²åŒºä¸­é‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        if len(self.buffer) == 0:
            return [], [], [], []
            
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        priorities = np.array(self.priorities, dtype=np.float32)
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        # è·å–æ ·æœ¬
        samples = [self.buffer[i] for i in indices]
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta())
        weights /= weights.max()  # å½’ä¸€åŒ–
        
        # æ›´æ–°å¸§è®¡æ•°å™¨
        self.frame += 1
        
        return indices, samples, weights
    
    def update_priorities(self, indices, errors):
        """æ›´æ–°é‡‡æ ·ç»éªŒçš„ä¼˜å…ˆçº§"""
        for idx, error in zip(indices, errors):
            if 0 <= idx < len(self.priorities):
                self.priorities[idx] = (abs(error) + 1e-5) ** self.alpha


# è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨
class CurriculumManager:
    def __init__(self, env):
        self.env = env
        self.current_stage = 0
        self.stage_thresholds = [0.3, 0.5, 0.7, 0.85]  # æˆåŠŸç‡é˜ˆå€¼
        self.stage_configs = [
            # é˜¶æ®µ0: å…¥é—¨
            {
                'pedestrian_cross': 4,      # åå­—è·¯å£è¡Œäººæ•°é‡
                'pedestrian_normal': 2,     # æ™®é€šè·¯æ®µè¡Œäººæ•°é‡
                'pedestrian_speed_min': 0.5,  # è¡Œäººæœ€ä½é€Ÿåº¦
                'pedestrian_speed_max': 1.0,  # è¡Œäººæœ€é«˜é€Ÿåº¦
                'max_episode_steps': 1200,   # æœ€å¤§æ­¥æ•° (20ç§’ * 60FPS)
                'success_threshold': 0.3     # è¿›å…¥ä¸‹ä¸€é˜¶æ®µæˆåŠŸç‡
            },
            # é˜¶æ®µ1: åˆçº§
            {
                'pedestrian_cross': 6,
                'pedestrian_normal': 3,
                'pedestrian_speed_min': 0.7,
                'pedestrian_speed_max': 1.3,
                'max_episode_steps': 1800,   # 30ç§’
                'success_threshold': 0.5
            },
            # é˜¶æ®µ2: ä¸­çº§
            {
                'pedestrian_cross': 8,
                'pedestrian_normal': 4,
                'pedestrian_speed_min': 0.8,
                'pedestrian_speed_max': 1.5,
                'max_episode_steps': 2400,   # 40ç§’
                'success_threshold': 0.7
            },
            # é˜¶æ®µ3: é«˜çº§ (æ­£å¸¸éš¾åº¦)
            {
                'pedestrian_cross': 10,
                'pedestrian_normal': 5,
                'pedestrian_speed_min': 1.0,
                'pedestrian_speed_max': 2.0,
                'max_episode_steps': 3600,   # 60ç§’
                'success_threshold': 0.85
            },
            # é˜¶æ®µ4: ä¸“å®¶ (æŒ‘æˆ˜)
            {
                'pedestrian_cross': 12,
                'pedestrian_normal': 6,
                'pedestrian_speed_min': 1.2,
                'pedestrian_speed_max': 2.5,
                'max_episode_steps': 3600,
                'success_threshold': 0.9
            }
        ]
        
        # è®­ç»ƒå†å²
        self.success_history = deque(maxlen=20)  # è®°å½•æœ€è¿‘20è½®çš„æˆåŠŸæƒ…å†µ
        self.reward_history = deque(maxlen=50)   # è®°å½•æœ€è¿‘50è½®çš„å¥–åŠ±
        
    def update_stage(self, success, reward):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µ"""
        # è®°å½•å†å²
        self.success_history.append(1 if success else 0)
        self.reward_history.append(reward)
        
        # è®¡ç®—æœ€è¿‘æˆåŠŸç‡
        if len(self.success_history) >= 10:
            success_rate = sum(self.success_history) / len(self.success_history)
            avg_reward = np.mean(self.reward_history) if self.reward_history else 0
            
            print(f"è¯¾ç¨‹å­¦ä¹  - å½“å‰é˜¶æ®µ: {self.current_stage}, æˆåŠŸç‡: {success_rate:.2f}, å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            if self.current_stage < len(self.stage_configs) - 1:
                next_stage_threshold = self.stage_configs[self.current_stage]['success_threshold']
                if success_rate >= next_stage_threshold and avg_reward > 5:
                    self.current_stage += 1
                    print(f"ğŸ‰ è¯¾ç¨‹å­¦ä¹ : è¿›é˜¶åˆ°é˜¶æ®µ {self.current_stage}!")
                    return True
                    
            # å¦‚æœè¡¨ç°å¤ªå·®ï¼Œé€€å›ä¸Šä¸€é˜¶æ®µ
            if self.current_stage > 0 and success_rate < 0.2:
                self.current_stage -= 1
                print(f"âš ï¸ è¯¾ç¨‹å­¦ä¹ : é€€å›é˜¶æ®µ {self.current_stage}")
                return True
        
        return False
    
    def get_current_config(self):
        """è·å–å½“å‰é˜¶æ®µçš„é…ç½®"""
        return self.stage_configs[min(self.current_stage, len(self.stage_configs) - 1)]
    
    def apply_to_environment(self):
        """å°†å½“å‰é˜¶æ®µé…ç½®åº”ç”¨åˆ°ç¯å¢ƒ"""
        config = self.get_current_config()
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¿®æ”¹Environment.pyä¸­çš„è¡Œäººç”Ÿæˆé€»è¾‘æ¥æ”¯æŒè¿™äº›å‚æ•°
        # æš‚æ—¶è¿”å›é…ç½®ï¼Œç”±å¤–éƒ¨è°ƒç”¨è€…å¤„ç†
        return config


# å¤šç›®æ ‡ä¼˜åŒ–å™¨
class MultiObjectiveOptimizer:
    def __init__(self):
        # å®šä¹‰ä¼˜åŒ–ç›®æ ‡åŠå…¶æƒé‡ï¼ˆå¯åŠ¨æ€è°ƒæ•´ï¼‰
        self.objectives = {
            'safety': {
                'weight': 0.4,
                'description': 'å®‰å…¨é¿éšœå’Œé¿å…ç¢°æ’',
                'metrics': ['collision_avoidance', 'pedestrian_distance']
            },
            'efficiency': {
                'weight': 0.25,
                'description': 'å¿«é€Ÿåˆ°è¾¾ç›®çš„åœ°',
                'metrics': ['progress_speed', 'total_time']
            },
            'comfort': {
                'weight': 0.2,
                'description': 'å¹³ç¨³é©¾é©¶ä½“éªŒ',
                'metrics': ['smoothness', 'steering_changes']
            },
            'rule_following': {
                'weight': 0.15,
                'description': 'éµå®ˆäº¤é€šè§„åˆ™',
                'metrics': ['lane_keeping', 'speed_limit']
            }
        }
        
        # æŒ‡æ ‡è·Ÿè¸ª
        self.metrics_history = {
            'safety': [],
            'efficiency': [],
            'comfort': [],
            'rule_following': []
        }
        
    def compute_composite_reward(self, metrics):
        """è®¡ç®—ç»¼åˆå¥–åŠ±å€¼"""
        composite = 0
        
        for obj_name, obj_info in self.objectives.items():
            if obj_name in metrics:
                # å½’ä¸€åŒ–å¤„ç†æ¯ä¸ªç›®æ ‡çš„è´¡çŒ®
                normalized_value = self._normalize_metric(metrics[obj_name], obj_name)
                composite += normalized_value * obj_info['weight']
                
                # è®°å½•æŒ‡æ ‡å†å²
                self.metrics_history[obj_name].append(normalized_value)
        
        # ç‰¹æ®Šæƒ©ç½šé¡¹
        if metrics.get('collision', False):
            composite -= 10
        if metrics.get('off_road', False):
            composite -= 5
        if metrics.get('dangerous_action', False):
            composite -= 3
            
        return composite
    
    def _normalize_metric(self, value, metric_name):
        """å½’ä¸€åŒ–æŒ‡æ ‡å€¼åˆ°[0, 1]èŒƒå›´"""
        # ä¸åŒæŒ‡æ ‡çš„å½’ä¸€åŒ–æ–¹å¼ä¸åŒ
        normalization_rules = {
            'safety': lambda x: min(max(x / 10, 0), 1),  # å‡è®¾å®‰å…¨åˆ†æ»¡åˆ†10
            'efficiency': lambda x: min(max(x / 100, 0), 1),  # æ•ˆç‡åˆ†æ»¡åˆ†100
            'comfort': lambda x: min(max((x + 5) / 10, 0), 1),  # èˆ’é€‚åº¦[-5, 5] -> [0, 1]
            'rule_following': lambda x: min(max(x, 0), 1)  # è§„åˆ™éµå¾ªåº¦[0, 1]
        }
        
        if metric_name in normalization_rules:
            return normalization_rules[metric_name](value)
        return min(max(value, 0), 1)  # é»˜è®¤æˆªæ–­åˆ°[0, 1]
    
    def adjust_weights(self, performance_feedback):
        """æ ¹æ®æ€§èƒ½åé¦ˆåŠ¨æ€è°ƒæ•´æƒé‡"""
        # å¦‚æœæŸä¸ªç›®æ ‡è¡¨ç°æŒç»­è¾ƒå·®ï¼Œå¢åŠ å…¶æƒé‡
        recent_performance = {}
        for obj in self.objectives:
            if len(self.metrics_history[obj]) >= 10:
                recent_avg = np.mean(self.metrics_history[obj][-10:])
                recent_performance[obj] = recent_avg
        
        if recent_performance:
            # æ‰¾åˆ°è¡¨ç°æœ€å·®çš„ç›®æ ‡
            worst_obj = min(recent_performance, key=recent_performance.get)
            best_obj = max(recent_performance, key=recent_performance.get)
            
            # å¦‚æœæœ€å·®ç›®æ ‡è¡¨ç°ä½äºé˜ˆå€¼ï¼Œå¢åŠ å…¶æƒé‡
            if recent_performance[worst_obj] < 0.3:
                adjustment = 0.05
                self.objectives[worst_obj]['weight'] += adjustment
                self.objectives[best_obj]['weight'] -= adjustment
                
                # ç¡®ä¿æƒé‡æ€»å’Œä¸º1
                total = sum(obj['weight'] for obj in self.objectives.values())
                for obj in self.objectives:
                    self.objectives[obj]['weight'] /= total
                
                print(f"åŠ¨æ€æƒé‡è°ƒæ•´: {worst_obj}æƒé‡â†‘ {adjustment:.3f}, {best_obj}æƒé‡â†“ {adjustment:.3f}")
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = "å¤šç›®æ ‡ä¼˜åŒ–æ€§èƒ½æŠ¥å‘Š:\n"
        report += "=" * 50 + "\n"
        
        for obj_name, obj_info in self.objectives.items():
            history = self.metrics_history[obj_name]
            if history:
                avg = np.mean(history[-20:]) if len(history) >= 20 else np.mean(history)
                report += f"{obj_name}(æƒé‡:{obj_info['weight']:.2f}): å¹³å‡å¾—åˆ†={avg:.3f}\n"
                report += f"  æè¿°: {obj_info['description']}\n"
        
        return report


# æ¨¡ä»¿å­¦ä¹ ç®¡ç†å™¨
class ImitationLearningManager:
    def __init__(self, expert_data_path=None):
        self.expert_data_path = expert_data_path
        self.expert_data = []
        self.is_pretrained = False
        
    def load_expert_data(self, path):
        """åŠ è½½ä¸“å®¶ç¤ºèŒƒæ•°æ®"""
        try:
            if os.path.exists(path):
                with open(path, 'rb') as f:
                    self.expert_data = pickle.load(f)
                print(f"å·²åŠ è½½ {len(self.expert_data)} æ¡ä¸“å®¶ç¤ºèŒƒæ•°æ®")
                return True
            else:
                print(f"ä¸“å®¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False
        except Exception as e:
            print(f"åŠ è½½ä¸“å®¶æ•°æ®å¤±è´¥: {e}")
            return False
    
    def collect_expert_demonstration(self, env, num_episodes=10):
        """æ”¶é›†ä¸“å®¶ç¤ºèŒƒæ•°æ®ï¼ˆå¯ä»¥æ‰‹åŠ¨æ§åˆ¶æˆ–ä½¿ç”¨è§„åˆ™æ§åˆ¶å™¨ï¼‰"""
        print(f"å¼€å§‹æ”¶é›†ä¸“å®¶ç¤ºèŒƒæ•°æ® ({num_episodes}ä¸ªepisodes)...")
        
        demonstrations = []
        
        for episode in range(num_episodes):
            print(f"æ”¶é›†ä¸“å®¶ç¤ºèŒƒ Episode {episode + 1}/{num_episodes}")
            
            state = env.reset(episode)
            done = False
            episode_data = []
            
            while not done:
                # è¿™é‡Œå¯ä»¥ä½¿ç”¨è§„åˆ™æ§åˆ¶å™¨æˆ–æ‰‹åŠ¨æ§åˆ¶
                # ç¤ºä¾‹ï¼šç®€å•çš„è§„åˆ™æ§åˆ¶å™¨
                action = self._rule_based_controller(env)
                
                new_state, reward, done, _ = env.step(action)
                
                # ä¿å­˜ç¤ºèŒƒæ•°æ®
                episode_data.append({
                    'state': state.copy(),
                    'action': action,
                    'reward': reward,
                    'next_state': new_state.copy(),
                    'done': done
                })
                
                state = new_state
            
            demonstrations.extend(episode_data)
            env.cleanup_actors()
        
        # ä¿å­˜ä¸“å®¶æ•°æ®
        self.expert_data = demonstrations
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_path = f"expert_data_{timestamp}.pkl"
        
        with open(save_path, 'wb') as f:
            pickle.dump(demonstrations, f)
        
        print(f"ä¸“å®¶ç¤ºèŒƒæ•°æ®å·²ä¿å­˜åˆ°: {save_path}, å…± {len(demonstrations)} æ¡è®°å½•")
        return True
    
    def _rule_based_controller(self, env):
        """åŸºäºè§„åˆ™çš„æ§åˆ¶å™¨ï¼ˆä½œä¸ºä¸“å®¶ç¤ºèŒƒï¼‰"""
        # è·å–è½¦è¾†çŠ¶æ€
        vehicle_location = env.vehicle.get_location()
        velocity = env.vehicle.get_velocity()
        speed_kmh = 3.6 * math.sqrt(velocity.x**2 + velocity.y**2)
        
        # ç®€å•è§„åˆ™ï¼šä¿æŒé€Ÿåº¦åœ¨20-40 km/hï¼Œé¿å…éšœç¢ç‰©
        if speed_kmh < 20:
            return 2  # åŠ é€Ÿ
        elif speed_kmh > 40:
            return 0  # å‡é€Ÿ
        else:
            # æ£€æŸ¥å‰æ–¹éšœç¢ç‰©
            has_obstacle_ahead = self._check_obstacle_ahead(env)
            if has_obstacle_ahead:
                return 0  # å‡é€Ÿ
            else:
                return 1  # ä¿æŒ
        
        return 1  # é»˜è®¤ä¿æŒ
    
    def _check_obstacle_ahead(self, env):
        """æ£€æŸ¥å‰æ–¹æ˜¯å¦æœ‰éšœç¢ç‰©ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„éšœç¢ç‰©æ£€æµ‹é€»è¾‘
        # æš‚æ—¶è¿”å›False
        return False
    
    def pretrain_with_behavioral_cloning(self, model, epochs=20):
        """ä½¿ç”¨è¡Œä¸ºå…‹éš†è¿›è¡Œé¢„è®­ç»ƒ"""
        if not self.expert_data:
            print("æ²¡æœ‰ä¸“å®¶æ•°æ®å¯ç”¨ï¼Œè·³è¿‡é¢„è®­ç»ƒ")
            return model
        
        print(f"å¼€å§‹è¡Œä¸ºå…‹éš†é¢„è®­ç»ƒ ({epochs}ä¸ªepochs)...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        states = []
        actions = []
        
        for demo in self.expert_data:
            states.append(demo['state'])
            actions.append(demo['action'])
        
        # å°†çŠ¶æ€å½’ä¸€åŒ–
        states = np.array(states) / 255.0
        
        # å°†åŠ¨ä½œè½¬æ¢ä¸ºone-hotç¼–ç 
        actions_onehot = tf.keras.utils.to_categorical(actions, num_classes=5)
        
        # å¤‡ä»½åŸå§‹ç¼–è¯‘è®¾ç½®
        original_loss = model.loss
        original_optimizer = model.optimizer
        original_metrics = model.metrics_names
        
        # é‡æ–°ç¼–è¯‘æ¨¡å‹ç”¨äºåˆ†ç±»ä»»åŠ¡
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # è®­ç»ƒæ¨¡å‹æ¨¡ä»¿ä¸“å®¶è¡Œä¸º
        history = model.fit(
            states, actions_onehot,
            batch_size=32,
            epochs=epochs,
            validation_split=0.2,
            verbose=1
        )
        
        print(f"é¢„è®­ç»ƒå®Œæˆ - æœ€ç»ˆå‡†ç¡®ç‡: {history.history['accuracy'][-1]:.3f}")
        
        # æ¢å¤åŸå§‹ç¼–è¯‘è®¾ç½®
        model.compile(
            optimizer=original_optimizer,
            loss=original_loss,
            metrics=original_metrics
        )
        
        self.is_pretrained = True
        return model
    
    def train_with_dagger(self, model, env, iterations=5, episodes_per_iter=5):
        """ä½¿ç”¨DAggerç®—æ³•è¿›è¡Œè®­ç»ƒ"""
        print(f"å¼€å§‹DAggerè®­ç»ƒ ({iterations}æ¬¡è¿­ä»£ï¼Œæ¯æ¬¡{episodes_per_iter}ä¸ªepisodes)...")
        
        aggregated_data = self.expert_data.copy()
        
        for iteration in range(iterations):
            print(f"\nDAgger è¿­ä»£ {iteration + 1}/{iterations}")
            
            # ä½¿ç”¨å½“å‰ç­–ç•¥æ”¶é›†æ•°æ®
            new_demos = []
            
            for episode in range(episodes_per_iter):
                print(f"  æ”¶é›†æ•°æ® Episode {episode + 1}/{episodes_per_iter}")
                
                state = env.reset(episode)
                done = False
                
                while not done:
                    # ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                    qs = model.predict(np.array(state).reshape(-1, *state.shape) / 255)[0]
                    action = np.argmax(qs)
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    new_state, reward, done, _ = env.step(action)
                    
                    # ä¸“å®¶çº æ­£ï¼ˆè¿™é‡Œå¯ä»¥æ·»åŠ ä¸“å®¶çº æ­£é€»è¾‘ï¼‰
                    # å¦‚æœç­–ç•¥åŠ¨ä½œä¸ä¸“å®¶å»ºè®®ä¸åŒï¼Œä½¿ç”¨ä¸“å®¶åŠ¨ä½œ
                    expert_action = self._rule_based_controller(env)
                    
                    # ä¿å­˜æ•°æ®ï¼ˆä½¿ç”¨ä¸“å®¶çº æ­£åçš„åŠ¨ä½œï¼‰
                    new_demos.append({
                        'state': state.copy(),
                        'action': expert_action,  # ä½¿ç”¨ä¸“å®¶åŠ¨ä½œ
                        'reward': reward,
                        'next_state': new_state.copy(),
                        'done': done
                    })
                    
                    state = new_state
                
                env.cleanup_actors()
            
            # åˆå¹¶æ•°æ®
            aggregated_data.extend(new_demos)
            
            # åœ¨åˆå¹¶æ•°æ®ä¸Šé‡æ–°è®­ç»ƒ
            states = [d['state'] for d in aggregated_data]
            actions = [d['action'] for d in aggregated_data]
            
            states = np.array(states) / 255.0
            actions_onehot = tf.keras.utils.to_categorical(actions, num_classes=5)
            
            # è®­ç»ƒæ¨¡å‹
            history = model.fit(
                states, actions_onehot,
                batch_size=32,
                epochs=10,
                validation_split=0.1,
                verbose=0
            )
            
            print(f"  è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {history.history['accuracy'][-1]:.3f}")
        
        print("DAggerè®­ç»ƒå®Œæˆ!")
        return model


# DQNæ™ºèƒ½ä½“ç±» - å‡çº§ç‰ˆï¼ˆæ•´åˆè®­ç»ƒç­–ç•¥ï¼‰
class DQNAgent:
    def __init__(self, use_dueling=True, use_per=True, use_curriculum=True, use_multi_objective=True):
        # åˆ›å»ºä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.use_dueling = use_dueling
        self.use_per = use_per
        self.use_curriculum = use_curriculum
        self.use_multi_objective = use_multi_objective
        
        if use_dueling:
            self.model = self.create_dueling_model()
            self.target_model = self.create_dueling_model()
        else:
            self.model = self.create_model()
            self.target_model = self.create_model()
            
        self.target_model.set_weights(self.model.get_weights())

        # ç»éªŒå›æ”¾ç¼“å†²åŒº - ä½¿ç”¨PERæˆ–æ ‡å‡†ç¼“å†²åŒº
        if use_per:
            self.replay_buffer = PrioritizedReplayBuffer(max_size=REPLAY_MEMORY_SIZE)
        else:
            self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)

        # è‡ªå®šä¹‰TensorBoard
        self.tensorboard = ModifiedTensorBoard(log_dir=f"logs/{MODEL_NAME}-{int(time.time())}")
        self.target_update_counter = 0  # ç›®æ ‡ç½‘ç»œæ›´æ–°è®¡æ•°å™¨

        # è®­ç»ƒæ§åˆ¶æ ‡å¿—
        self.terminate = False
        self.last_logged_episode = 0
        self.training_initialized = False
        
        # è®­ç»ƒç­–ç•¥ç»„ä»¶
        self.curriculum_manager = None
        self.multi_objective_optimizer = None
        self.imitation_manager = None
        
    def setup_training_strategies(self, env=None):
        """è®¾ç½®è®­ç»ƒç­–ç•¥ç»„ä»¶"""
        if self.use_curriculum and env:
            self.curriculum_manager = CurriculumManager(env)
            print("è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨å·²å¯ç”¨")
        
        if self.use_multi_objective:
            self.multi_objective_optimizer = MultiObjectiveOptimizer()
            print("å¤šç›®æ ‡ä¼˜åŒ–å™¨å·²å¯ç”¨")
        
        # æ¨¡ä»¿å­¦ä¹ ç®¡ç†å™¨ï¼ˆéœ€è¦æ—¶æ‰‹åŠ¨å¯ç”¨ï¼‰
        self.imitation_manager = ImitationLearningManager()

    def create_model(self):
        """åˆ›å»ºæ ‡å‡†æ·±åº¦Qç½‘ç»œæ¨¡å‹"""
        # ä½¿ç”¨å‡½æ•°å¼API
        inputs = Input(shape=(IM_HEIGHT, IM_WIDTH, 3))
        
        # ç¬¬ä¸€å·ç§¯å—
        x = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(inputs)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # ç¬¬äºŒå·ç§¯å—
        x = Conv2D(64, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # ç¬¬ä¸‰å·ç§¯å—
        x = Conv2D(128, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶
        attention = Conv2D(1, (1, 1), padding='same', activation='sigmoid')(x)
        x = Multiply()([x, attention])
        
        # å±•å¹³å±‚
        x = Flatten()(x)
        
        # å…¨è¿æ¥å±‚
        x = Dense(512, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.2)(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.1)(x)
        
        # è¾“å‡ºå±‚ - 5ä¸ªåŠ¨ä½œ
        outputs = Dense(5, activation='linear')(x)
        
        # åˆ›å»ºæ¨¡å‹
        model = Model(inputs=inputs, outputs=outputs)
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(loss="huber", optimizer=Adam(learning_rate=LEARNING_RATE), metrics=["mae"])
        return model
    
    def create_dueling_model(self):
        """åˆ›å»ºDueling DQNæ¨¡å‹æ¶æ„"""
        inputs = Input(shape=(IM_HEIGHT, IM_WIDTH, 3))
        
        # å…±äº«çš„ç‰¹å¾æå–å±‚
        # ç¬¬ä¸€å·ç§¯å—
        x = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(inputs)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # ç¬¬äºŒå·ç§¯å—
        x = Conv2D(64, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # ç¬¬ä¸‰å·ç§¯å—
        x = Conv2D(128, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶
        attention = Conv2D(1, (1, 1), padding='same', activation='sigmoid')(x)
        x = Multiply()([x, attention])
        
        # å±•å¹³å±‚
        x = Flatten()(x)
        
        # å…±äº«çš„å…¨è¿æ¥å±‚
        shared = Dense(512, activation='relu')(x)
        shared = Dropout(0.3)(shared)
        shared = Dense(256, activation='relu')(shared)
        
        # ä»·å€¼æµ (V(s))
        value_stream = Dense(128, activation='relu')(shared)
        value_stream = Dropout(0.2)(value_stream)
        value = Dense(1, activation='linear', name='value')(value_stream)
        
        # ä¼˜åŠ¿æµ (A(s,a))
        advantage_stream = Dense(128, activation='relu')(shared)
        advantage_stream = Dropout(0.2)(advantage_stream)
        advantage = Dense(5, activation='linear', name='advantage')(advantage_stream)
        
        # åˆå¹¶: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
        mean_advantage = Lambda(lambda a: tf.reduce_mean(a, axis=1, keepdims=True))(advantage)
        advantage_centered = Subtract()([advantage, mean_advantage])
        q_values = Add()([value, advantage_centered])
        
        # åˆ›å»ºæ¨¡å‹
        model = Model(inputs=inputs, outputs=q_values)
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(loss="huber", optimizer=Adam(learning_rate=LEARNING_RATE), metrics=["mae"])
        
        return model

    def update_replay_memory(self, transition):
        """æ›´æ–°ç»éªŒå›æ”¾ç¼“å†²åŒº"""
        # transition = (å½“å‰çŠ¶æ€, åŠ¨ä½œ, å¥–åŠ±, æ–°çŠ¶æ€, å®Œæˆæ ‡å¿—)
        if self.use_per:
            # PER: åˆå§‹æ·»åŠ æ—¶ä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§
            self.replay_buffer.add(transition, error=1.0)  # åˆå§‹è¯¯å·®è®¾ä¸º1.0
        else:
            self.replay_memory.append(transition)

    def minibatch_chooser(self):
        """æ”¹è¿›çš„ç»éªŒé‡‡æ ·ç­–ç•¥"""
        if self.use_per:
            # PERé‡‡æ ·
            if len(self.replay_buffer) < MIN_REPLAY_MEMORY_SIZE:
                return [], [], [], []
                
            indices, samples, weights = self.replay_buffer.sample(MINIBATCH_SIZE)
            return indices, samples, weights
        else:
            # æ ‡å‡†é‡‡æ ·
            if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:
                return random.sample(self.replay_memory, min(len(self.replay_memory), MINIBATCH_SIZE))
                
            # åˆ†ç±»ç»éªŒæ ·æœ¬
            positive_samples = []    # é«˜å¥–åŠ±ç»éªŒ
            negative_samples = []    # è´Ÿå¥–åŠ±/ç¢°æ’ç»éªŒ
            neutral_samples = []     # ä¸­æ€§å¥–åŠ±ç»éªŒ
            
            for sample in self.replay_memory:
                _, _, reward, _, done = sample
                
                if done and reward < -5:  # ç¢°æ’æˆ–ä¸¥é‡é”™è¯¯
                    negative_samples.append(sample)
                elif reward > 1:  # ç§¯æç»éªŒ
                    positive_samples.append(sample)
                else:  # ä¸­æ€§ç»éªŒ
                    neutral_samples.append(sample)
            
            # å¹³è¡¡é‡‡æ ·
            batch = []
            
            # é‡‡æ ·è´Ÿç»éªŒ (20%)
            num_negative = min(len(negative_samples), MINIBATCH_SIZE // 5)
            batch.extend(random.sample(negative_samples, num_negative))
            
            # é‡‡æ ·æ­£ç»éªŒ (30%)
            num_positive = min(len(positive_samples), MINIBATCH_SIZE // 3)
            batch.extend(random.sample(positive_samples, num_positive))
            
            # ç”¨ä¸­æ€§ç»éªŒè¡¥å…¨æ‰¹æ¬¡
            remaining = MINIBATCH_SIZE - len(batch)
            if remaining > 0:
                batch.extend(random.sample(neutral_samples, min(remaining, len(neutral_samples))))
            
            # å¦‚æœè¿˜ä¸å¤Ÿï¼Œä»æ•´ä¸ªè®°å¿†åº“éšæœºé‡‡æ ·
            if len(batch) < MINIBATCH_SIZE:
                additional = MINIBATCH_SIZE - len(batch)
                batch.extend(random.sample(self.replay_memory, additional))
                
            random.shuffle(batch)  # æ‰“ä¹±æ‰¹æ¬¡
            return batch

    def train(self):
        """è®­ç»ƒDQNç½‘ç»œ"""
        if self.use_per:
            if len(self.replay_buffer) < MIN_REPLAY_MEMORY_SIZE:
                return
                
            # PER: é‡‡æ ·å¹¶è·å–æƒé‡
            indices, minibatch, weights = self.replay_buffer.sample(MINIBATCH_SIZE)
            if len(minibatch) == 0:
                return
        else:
            if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:
                return
                
            # æ ‡å‡†é‡‡æ ·
            minibatch = self.minibatch_chooser()
            weights = np.ones(len(minibatch))  # æ ‡å‡†è®­ç»ƒæƒé‡ä¸º1

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        current_states = np.array([transition[0] for transition in minibatch]) / 255
        current_qs_list = self.model.predict(current_states, batch_size=PREDICTION_BATCH_SIZE)

        new_current_states = np.array([transition[3] for transition in minibatch]) / 255
        future_qs_list = self.target_model.predict(new_current_states, batch_size=PREDICTION_BATCH_SIZE)

        x = []  # è¾“å…¥çŠ¶æ€
        y = []  # ç›®æ ‡Qå€¼
        errors = []  # TDè¯¯å·®ï¼ˆç”¨äºPERï¼‰

        # è®¡ç®—ç›®æ ‡Qå€¼
        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):
            if not done:
                # ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹è®¡ç®—ç›®æ ‡Qå€¼
                max_future_q = np.max(future_qs_list[index])
                new_q = reward + DISCOUNT * max_future_q
            else:
                new_q = reward  # ç»ˆæ­¢çŠ¶æ€

            current_qs = current_qs_list[index].copy()
            old_q = current_qs[action]  # ç”¨äºè®¡ç®—TDè¯¯å·®
            current_qs[action] = new_q  # æ›´æ–°å¯¹åº”åŠ¨ä½œçš„Qå€¼
            
            # è®¡ç®—TDè¯¯å·®
            td_error = abs(new_q - old_q)
            errors.append(td_error)

            x.append(current_state)
            y.append(current_qs)

        # PER: æ›´æ–°ä¼˜å…ˆçº§
        if self.use_per and len(errors) > 0:
            self.replay_buffer.update_priorities(indices, errors)

        # è®°å½•æ—¥å¿—åˆ¤æ–­
        log_this_step = False
        if self.tensorboard.step > self.last_logged_episode:
            log_this_step = True
            self.last_logged_episode = self.tensorboard.step

        # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦æ ·æœ¬æƒé‡ï¼‰
        self.model.fit(np.array(x) / 255, np.array(y), 
                      batch_size=TRAINING_BATCH_SIZE, 
                      sample_weight=weights if self.use_per else None,
                      verbose=0, shuffle=False,
                      callbacks=[self.tensorboard] if log_this_step else None)

        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if log_this_step:
            self.target_update_counter += 1

        if self.target_update_counter > UPDATE_TARGET_EVERY:
            print("ç›®æ ‡ç½‘ç»œå·²æ›´æ–°")
            self.target_model.set_weights(self.model.get_weights())
            self.target_update_counter = 0

    def train_in_loop(self):
        """åœ¨å•ç‹¬çº¿ç¨‹ä¸­æŒç»­è®­ç»ƒ"""
        # é¢„çƒ­è®­ç»ƒ
        x = np.random.uniform(size=(1, IM_HEIGHT, IM_WIDTH, 3)).astype(np.float32)
        y = np.random.uniform(size=(1, 5)).astype(np.float32)  # æ”¹ä¸º5ä¸ªè¾“å‡º

        self.model.fit(x, y, verbose=False, batch_size=1)
        self.training_initialized = True

        # æŒç»­è®­ç»ƒå¾ªç¯
        while True:
            if self.terminate:
                return
            self.train()
            time.sleep(0.01)  # æ§åˆ¶è®­ç»ƒé¢‘ç‡

    def get_qs(self, state):
        """è·å–çŠ¶æ€çš„Qå€¼"""
        return self.model.predict(np.array(state).reshape(-1, *state.shape) / 255)[0]