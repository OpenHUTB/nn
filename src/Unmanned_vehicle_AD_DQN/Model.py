# Model.py
import glob
import os
import sys
import random
import time
import numpy as np
import cv2
import math
import matplotlib.pyplot as plt
from collections import deque
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Concatenate, Conv2D, AveragePooling2D, Activation, \
    Flatten, Dropout, BatchNormalization, MaxPooling2D, Multiply, Add, Lambda, Subtract, UpSampling2D, Conv2DTranspose, \
    Reshape, Layer, LayerNormalization
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
import tensorflow as tf
import tensorflow.keras.backend as backend
from threading import Thread, Lock
from Environment import *
from Hyperparameters import *
import pickle
import json
from datetime import datetime
from scipy import ndimage


# å¢å¼ºçš„TensorBoardç±»
class EnhancedTensorBoard(TensorBoard):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._log_write_dir = self.log_dir
        self.step = 1
        self.writer = tf.summary.create_file_writer(self.log_dir)
        self.lock = Lock()

    def set_model(self, model):
        self.model = model
        self._train_dir = os.path.join(self._log_write_dir, 'train')
        self._train_step = self.model._train_counter
        self._val_dir = os.path.join(self._log_write_dir, 'validation')
        self._val_step = self.model._test_counter
        self._should_write_train_graph = False

    def on_epoch_end(self, epoch, logs=None):
        self.update_stats(**logs)

    def on_batch_end(self, batch, logs=None):
        pass

    def on_train_end(self, logs=None):
        pass

    def update_stats(self, **stats):
        with self.lock:
            with self.writer.as_default():
                for key, value in stats.items():
                    tf.summary.scalar(key, value, step=self.step)
                self.writer.flush()


# å¢å¼ºçš„ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒº
class EnhancedPrioritizedReplayBuffer:
    def __init__(self, max_size=REPLAY_MEMORY_SIZE, alpha=0.6, beta_start=0.4, beta_frames=100000):
        self.max_size = max_size
        self.alpha = alpha
        self.beta_start = beta_start
        self.beta_frames = beta_frames
        self.frame = 1
        
        # ä½¿ç”¨åˆ†æ®µå­˜å‚¨ä»¥æé«˜æ•ˆç‡
        self.buffer = deque(maxlen=max_size)
        self.priorities = deque(maxlen=max_size)
        self.obstacle_experiences = []  # å­˜å‚¨é¿éšœç›¸å…³ç»éªŒ
        self.success_experiences = []   # å­˜å‚¨æˆåŠŸç»éªŒ
        
    def __len__(self):
        return len(self.buffer)
    
    def beta(self):
        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)
    
    def add(self, experience, error=None, is_obstacle=False, is_success=False):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        if error is None:
            if self.priorities:
                priority = max(self.priorities) * 0.8
            else:
                priority = 1.0
        else:
            priority = (abs(error) + 1e-5) ** self.alpha
            
        self.buffer.append(experience)
        self.priorities.append(priority)
        
        # åˆ†ç±»å­˜å‚¨ç‰¹æ®Šç»éªŒ
        if is_obstacle:
            self.obstacle_experiences.append((experience, error))
            if len(self.obstacle_experiences) > self.max_size // 10:
                self.obstacle_experiences.pop(0)
                
        if is_success:
            self.success_experiences.append((experience, error))
            if len(self.success_experiences) > self.max_size // 10:
                self.success_experiences.pop(0)
                
        self.frame += 1
        
    def sample(self, batch_size, obstacle_ratio=0.3, success_ratio=0.2):
        """æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿åŒ…å«é¿éšœå’ŒæˆåŠŸç»éªŒ"""
        if len(self.buffer) == 0:
            return [], [], [], []
            
        # åŸºç¡€é‡‡æ ·ï¼ˆæ¥è‡ªæ™®é€šç¼“å†²åŒºï¼‰
        base_size = int(batch_size * (1 - obstacle_ratio - success_ratio))
        probs = None
        if base_size > 0:
            priorities = np.array(self.priorities, dtype=np.float32)
            if len(priorities) != len(self.buffer):
                # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ– priorities
                self.priorities = [1.0] * len(self.buffer)
                priorities = np.array(self.priorities, dtype=np.float32)
            probs = priorities ** self.alpha
            probs /= probs.sum()
            base_indices = np.random.choice(len(self.buffer), base_size, p=probs)
        else:
            base_indices = []
            
        # é¿éšœç»éªŒé‡‡æ ·
        obstacle_size = int(batch_size * obstacle_ratio)
        obstacle_indices = []
        if self.obstacle_experiences and obstacle_size > 0:
            obstacle_size = min(obstacle_size, len(self.obstacle_experiences))
            obstacle_samples = random.sample(self.obstacle_experiences, obstacle_size)
            # æ‰¾åˆ°è¿™äº›æ ·æœ¬åœ¨ç¼“å†²åŒºä¸­çš„ç´¢å¼•ï¼ˆè¿‘ä¼¼ï¼‰
            for exp, _ in obstacle_samples:
                try:
                    idx = list(self.buffer).index(exp)
                    obstacle_indices.append(idx)
                except:
                    pass
                    
        # æˆåŠŸç»éªŒé‡‡æ ·
        success_size = int(batch_size * success_ratio)
        success_indices = []
        if self.success_experiences and success_size > 0:
            success_size = min(success_size, len(self.success_experiences))
            success_samples = random.sample(self.success_experiences, success_size)
            for exp, _ in success_samples:
                try:
                    idx = list(self.buffer).index(exp)
                    success_indices.append(idx)
                except:
                    pass
        
        # åˆå¹¶æ‰€æœ‰ç´¢å¼•
        all_indices = list(base_indices) + obstacle_indices + success_indices
        
        # å¦‚æœæ•°é‡ä¸å¤Ÿï¼Œç”¨åŸºç¡€é‡‡æ ·è¡¥è¶³
        if len(all_indices) < batch_size:
            additional = batch_size - len(all_indices)
            if probs is not None:
                additional_indices = np.random.choice(len(self.buffer), additional, p=probs)
            else:
                additional_indices = np.random.choice(len(self.buffer), additional)
            all_indices.extend(additional_indices)
            
        # è·å–æ ·æœ¬
        samples = [self.buffer[i] for i in all_indices]
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        total = len(self.buffer)
        priorities_array = np.array(self.priorities, dtype=np.float32)
        if len(priorities_array) != total:
            # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ–
            self.priorities = [1.0] * total
            priorities_array = np.array(self.priorities, dtype=np.float32)
        
        weights = (total * (priorities_array[all_indices] ** self.alpha) / (priorities_array ** self.alpha).sum()) ** (-self.beta())
        
        # å½’ä¸€åŒ–æƒé‡
        max_weight = weights.max()
        if max_weight > 0:
            weights /= max_weight
        
        return all_indices, samples, weights
    
    def update_priorities(self, indices, errors):
        """æ›´æ–°é‡‡æ ·ç»éªŒçš„ä¼˜å…ˆçº§"""
        for idx, error in zip(indices, errors):
            if 0 <= idx < len(self.priorities):
                # å¯¹äºé¿éšœç›¸å…³çš„ç»éªŒï¼Œç»™äºˆæ›´é«˜çš„ä¼˜å…ˆçº§æƒé‡
                is_obstacle_exp = any(idx == list(self.buffer).index(exp[0]) for exp in self.obstacle_experiences if exp[0] in self.buffer)
                if is_obstacle_exp:
                    error *= 1.5  # æé«˜é¿éšœç»éªŒçš„ä¼˜å…ˆçº§
                self.priorities[idx] = (abs(error) + 1e-5) ** self.alpha


# éšœç¢ç‰©æ³¨æ„åŠ›æ¨¡å—
class ObstacleAttentionLayer(Layer):
    def __init__(self, filters, kernel_size=3, **kwargs):
        super(ObstacleAttentionLayer, self).__init__(**kwargs)
        self.filters = filters
        self.kernel_size = kernel_size
        
    def build(self, input_shape):
        # ç©ºé—´æ³¨æ„åŠ›
        self.spatial_conv = Conv2D(self.filters, (self.kernel_size, self.kernel_size), 
                                   padding='same', activation='relu')
        self.spatial_attention = Conv2D(1, (1, 1), padding='same', activation='sigmoid')
        
        # é€šé“æ³¨æ„åŠ›
        self.channel_gap = GlobalAveragePooling2D()
        self.channel_fc1 = Dense(self.filters // 8, activation='relu')
        self.channel_fc2 = Dense(self.filters, activation='sigmoid')
        
        super(ObstacleAttentionLayer, self).build(input_shape)
        
    def call(self, inputs):
        # ç©ºé—´æ³¨æ„åŠ›
        spatial_features = self.spatial_conv(inputs)
        spatial_attention = self.spatial_attention(spatial_features)
        
        # é€šé“æ³¨æ„åŠ›
        channel_weights = self.channel_gap(inputs)
        channel_weights = self.channel_fc1(channel_weights)
        channel_weights = self.channel_fc2(channel_weights)
        channel_weights = Reshape((1, 1, self.filters))(channel_weights)
        
        # åˆå¹¶æ³¨æ„åŠ›
        attended = Multiply()([inputs, spatial_attention])
        attended = Multiply()([attended, channel_weights])
        
        # æ®‹å·®è¿æ¥
        output = Add()([inputs, attended])
        return output


# è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨ - å¢å¼ºç‰ˆ
class EnhancedCurriculumManager:
    def __init__(self, env):
        self.env = env
        self.current_stage = 0
        self.stage_progress = 0.0  # é˜¶æ®µå†…è¿›åº¦ï¼ˆ0-1ï¼‰
        
        # å¢å¼ºçš„é˜¶æ®µé…ç½®ï¼Œæ›´ä¸“æ³¨äºé¿éšœè®­ç»ƒ
        self.stage_configs = [
            # é˜¶æ®µ0: åŸºç¡€é¿éšœè®­ç»ƒ
            {
                'name': 'åŸºç¡€é¿éšœ',
                'pedestrian_cross': 2,
                'pedestrian_normal': 1,
                'pedestrian_speed_min': 0.3,
                'pedestrian_speed_max': 0.8,
                'max_episode_steps': 900,
                'success_threshold': 0.4,
                'obstacle_focus': 0.8,  # é¿éšœè®­ç»ƒæƒé‡
                'speed_limit': 25
            },
            # é˜¶æ®µ1: ç®€å•åœºæ™¯
            {
                'name': 'ç®€å•åœºæ™¯',
                'pedestrian_cross': 4,
                'pedestrian_normal': 2,
                'pedestrian_speed_min': 0.5,
                'pedestrian_speed_max': 1.2,
                'max_episode_steps': 1200,
                'success_threshold': 0.5,
                'obstacle_focus': 0.7,
                'speed_limit': 30
            },
            # é˜¶æ®µ2: ä¸­ç­‰éš¾åº¦
            {
                'name': 'ä¸­ç­‰éš¾åº¦',
                'pedestrian_cross': 6,
                'pedestrian_normal': 3,
                'pedestrian_speed_min': 0.7,
                'pedestrian_speed_max': 1.5,
                'max_episode_steps': 1800,
                'success_threshold': 0.6,
                'obstacle_focus': 0.6,
                'speed_limit': 35
            },
            # é˜¶æ®µ3: å¤æ‚åœºæ™¯
            {
                'name': 'å¤æ‚åœºæ™¯',
                'pedestrian_cross': 8,
                'pedestrian_normal': 4,
                'pedestrian_speed_min': 0.8,
                'pedestrian_speed_max': 1.8,
                'max_episode_steps': 2400,
                'success_threshold': 0.7,
                'obstacle_focus': 0.5,
                'speed_limit': 40
            },
            # é˜¶æ®µ4: æŒ‘æˆ˜æ¨¡å¼
            {
                'name': 'æŒ‘æˆ˜æ¨¡å¼',
                'pedestrian_cross': 10,
                'pedestrian_normal': 5,
                'pedestrian_speed_min': 1.0,
                'pedestrian_speed_max': 2.2,
                'max_episode_steps': 3000,
                'success_threshold': 0.75,
                'obstacle_focus': 0.4,
                'speed_limit': 45
            },
            # é˜¶æ®µ5: ä¸“å®¶æ¨¡å¼
            {
                'name': 'ä¸“å®¶æ¨¡å¼',
                'pedestrian_cross': 12,
                'pedestrian_normal': 6,
                'pedestrian_speed_min': 1.2,
                'pedestrian_speed_max': 2.5,
                'max_episode_steps': 3600,
                'success_threshold': 0.8,
                'obstacle_focus': 0.3,
                'speed_limit': 50
            }
        ]
        
        # è®­ç»ƒå†å²
        self.success_history = deque(maxlen=20)
        self.reward_history = deque(maxlen=50)
        self.obstacle_avoidance_history = deque(maxlen=30)  # é¿éšœæˆåŠŸç‡å†å²
        
    def update_stage(self, success, reward, obstacle_avoidance_score=0):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µ"""
        # è®°å½•å†å²
        self.success_history.append(1 if success else 0)
        self.reward_history.append(reward)
        if obstacle_avoidance_score > 0:
            self.obstacle_avoidance_history.append(obstacle_avoidance_score)
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if len(self.success_history) >= 10:
            success_rate = sum(self.success_history) / len(self.success_history)
            avg_reward = np.mean(self.reward_history) if self.reward_history else 0
            obstacle_rate = np.mean(self.obstacle_avoidance_history) if self.obstacle_avoidance_history else 0
            
            # åŠ¨æ€æ›´æ–°é˜¶æ®µè¿›åº¦
            current_config = self.get_current_config()
            target_threshold = current_config['success_threshold']
            
            # è®¡ç®—é˜¶æ®µå†…è¿›åº¦
            if target_threshold > 0:
                self.stage_progress = min(1.0, success_rate / target_threshold)
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            if self.current_stage < len(self.stage_configs) - 1:
                next_stage = self.current_stage + 1
                next_threshold = self.stage_configs[next_stage]['success_threshold']
                
                # è¿›é˜¶æ¡ä»¶ï¼šè¾¾åˆ°æˆåŠŸç‡é˜ˆå€¼ä¸”æœ‰ä¸€å®šçš„é¿éšœè¡¨ç°
                if (success_rate >= current_config['success_threshold'] and 
                    avg_reward > 3 and 
                    obstacle_rate > 0.6):
                    
                    self.current_stage = next_stage
                    self.stage_progress = 0.0
                    print(f"ğŸ‰ è¯¾ç¨‹å­¦ä¹ : è¿›é˜¶åˆ°é˜¶æ®µ {self.current_stage} - {self.stage_configs[self.current_stage]['name']}!")
                    return True, 'advance'
                    
            # å¦‚æœè¡¨ç°æŒç»­ä¸ä½³ï¼Œé€€å›ä¸Šä¸€é˜¶æ®µ
            if (self.current_stage > 0 and 
                success_rate < 0.2 and 
                len(self.success_history) >= 15):
                
                self.current_stage -= 1
                self.stage_progress = 0.5  # é€€å›åç»™äºˆä¸­ç­‰è¿›åº¦
                print(f"âš ï¸ è¯¾ç¨‹å­¦ä¹ : é€€å›é˜¶æ®µ {self.current_stage} - {self.stage_configs[self.current_stage]['name']}")
                return True, 'regress'
        
        return False, 'maintain'
    
    def get_current_config(self):
        """è·å–å½“å‰é˜¶æ®µçš„é…ç½®"""
        return self.stage_configs[min(self.current_stage, len(self.stage_configs) - 1)]
    
    def get_stage_info(self):
        """è·å–é˜¶æ®µä¿¡æ¯"""
        config = self.get_current_config()
        return {
            'stage': self.current_stage,
            'name': config['name'],
            'progress': self.stage_progress,
            'pedestrian_total': config['pedestrian_cross'] + config['pedestrian_normal'],
            'difficulty': self.current_stage / len(self.stage_configs)
        }


# å¤šç›®æ ‡ä¼˜åŒ–å™¨ - å¢å¼ºç‰ˆï¼Œä¸“æ³¨äºé¿éšœ
class EnhancedMultiObjectiveOptimizer:
    def __init__(self):
        # åŠ¨æ€æƒé‡è°ƒæ•´ï¼ŒåˆæœŸæ›´æ³¨é‡å®‰å…¨
        self.objectives = {
            'safety': {
                'weight': 0.50,  # æé«˜å®‰å…¨æƒé‡
                'description': 'å®‰å…¨é¿éšœå’Œé¿å…ç¢°æ’',
                'target_value': 0.9,
                'current_performance': 0.0,
                'improvement_rate': 0.0
            },
            'efficiency': {
                'weight': 0.20,
                'description': 'å¿«é€Ÿåˆ°è¾¾ç›®çš„åœ°',
                'target_value': 0.7,
                'current_performance': 0.0,
                'improvement_rate': 0.0
            },
            'comfort': {
                'weight': 0.15,
                'description': 'å¹³ç¨³é©¾é©¶ä½“éªŒ',
                'target_value': 0.6,
                'current_performance': 0.0,
                'improvement_rate': 0.0
            },
            'obstacle_avoidance': {  # æ–°å¢ï¼šä¸“é—¨é’ˆå¯¹éšœç¢ç‰©é¿è®©
                'weight': 0.15,
                'description': 'æœ‰æ•ˆé¿è®©è¡Œäººå’Œå»ºç­‘ç‰©',
                'target_value': 0.8,
                'current_performance': 0.0,
                'improvement_rate': 0.0
            }
        }
        
        # æŒ‡æ ‡è·Ÿè¸ª
        self.metrics_history = {
            'safety': deque(maxlen=100),
            'efficiency': deque(maxlen=100),
            'comfort': deque(maxlen=100),
            'obstacle_avoidance': deque(maxlen=100)
        }
        
        # é¿éšœç‰¹åˆ«å¥–åŠ±å‚æ•°
        self.obstacle_avoidance_bonus = {
            'near_miss': 2.0,  # æˆåŠŸé¿è®©å¥–åŠ±
            'safe_distance': 1.0,  # ä¿æŒå®‰å…¨è·ç¦»å¥–åŠ±
            'collision_penalty': -10.0,  # ç¢°æ’æƒ©ç½š
            'danger_zone_penalty': -3.0  # è¿›å…¥å±é™©åŒºåŸŸæƒ©ç½š
        }
        
    def compute_composite_reward(self, metrics, obstacle_info=None):
        """è®¡ç®—ç»¼åˆå¥–åŠ±å€¼ï¼Œç‰¹åˆ«å…³æ³¨é¿éšœ"""
        composite = 0
        
        # åŸºç¡€ç›®æ ‡è®¡ç®—
        for obj_name, obj_info in self.objectives.items():
            if obj_name in metrics:
                normalized_value = self._normalize_metric(metrics[obj_name], obj_name)
                composite += normalized_value * obj_info['weight']
                
                # æ›´æ–°æ€§èƒ½è®°å½•
                self.metrics_history[obj_name].append(normalized_value)
                self.objectives[obj_name]['current_performance'] = np.mean(
                    self.metrics_history[obj_name]) if self.metrics_history[obj_name] else 0
        
        # ç‰¹åˆ«é¿éšœå¥–åŠ±
        if obstacle_info:
            composite += self._compute_obstacle_avoidance_reward(obstacle_info)
        
        # ç‰¹æ®Šæƒ©ç½šé¡¹ï¼ˆå¢å¼ºï¼‰
        if metrics.get('collision', False):
            composite += self.obstacle_avoidance_bonus['collision_penalty']
        if metrics.get('off_road', False):
            composite -= 5
        if metrics.get('dangerous_action', False):
            composite -= 4
        if metrics.get('near_miss', False):  # æˆåŠŸé¿è®©å¥–åŠ±
            composite += self.obstacle_avoidance_bonus['near_miss']
            
        return composite
    
    def _compute_obstacle_avoidance_reward(self, obstacle_info):
        """è®¡ç®—é¿éšœç‰¹åˆ«å¥–åŠ±"""
        reward = 0
        
        # æ ¹æ®éšœç¢ç‰©è·ç¦»ç»™äºˆå¥–åŠ±
        min_distance = obstacle_info.get('min_distance', float('inf'))
        if min_distance < 100:  # åªè€ƒè™‘100ç±³å†…çš„éšœç¢ç‰©
            if min_distance > 15:  # éå¸¸å®‰å…¨
                reward += self.obstacle_avoidance_bonus['safe_distance'] * 0.5
            elif min_distance > 10:  # å®‰å…¨
                reward += self.obstacle_avoidance_bonus['safe_distance'] * 0.3
            elif min_distance > 5:  # è­¦å‘Šè·ç¦»
                reward -= self.obstacle_avoidance_bonus['danger_zone_penalty'] * 0.5
            else:  # å±é™©è·ç¦»
                reward -= self.obstacle_avoidance_bonus['danger_zone_penalty']
        
        # æˆåŠŸé¿è®©å¥–åŠ±
        if obstacle_info.get('avoidance_success', False):
            reward += self.obstacle_avoidance_bonus['near_miss']
        
        return reward
    
    def _normalize_metric(self, value, metric_name):
        """å½’ä¸€åŒ–æŒ‡æ ‡å€¼"""
        normalization_rules = {
            'safety': lambda x: min(max(x / 10, 0), 1),
            'efficiency': lambda x: min(max(x / 100, 0), 1),
            'comfort': lambda x: min(max((x + 5) / 10, 0), 1),
            'obstacle_avoidance': lambda x: min(max(x, 0), 1)
        }
        
        if metric_name in normalization_rules:
            return normalization_rules[metric_name](value)
        return min(max(value, 0), 1)
    
    def adjust_weights(self, performance_feedback, training_stage=0):
        """åŠ¨æ€è°ƒæ•´æƒé‡ï¼Œè€ƒè™‘è®­ç»ƒé˜¶æ®µ"""
        # é¦–å…ˆæ›´æ–°æŒ‡æ ‡å†å²
        if performance_feedback:
            for obj_name, value in performance_feedback.items():
                if obj_name in self.metrics_history:
                    self.metrics_history[obj_name].append(value)
        
        # æ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´åŸºç¡€æƒé‡
        if training_stage < 2:  # åˆæœŸé˜¶æ®µæ›´æ³¨é‡å®‰å…¨
            self.objectives['safety']['weight'] = 0.6
            self.objectives['efficiency']['weight'] = 0.15
            self.objectives['obstacle_avoidance']['weight'] = 0.15
            self.objectives['comfort']['weight'] = 0.10
        elif training_stage < 4:  # ä¸­æœŸå¹³è¡¡
            self.objectives['safety']['weight'] = 0.5
            self.objectives['efficiency']['weight'] = 0.2
            self.objectives['obstacle_avoidance']['weight'] = 0.15
            self.objectives['comfort']['weight'] = 0.15
        else:  # åæœŸæ³¨é‡æ•ˆç‡
            self.objectives['safety']['weight'] = 0.4
            self.objectives['efficiency']['weight'] = 0.25
            self.objectives['obstacle_avoidance']['weight'] = 0.2
            self.objectives['comfort']['weight'] = 0.15
        
        # åŸºäºæ€§èƒ½åé¦ˆå¾®è°ƒ
        recent_performance = {}
        for obj in self.objectives:
            if len(self.metrics_history[obj]) >= 10:
                recent_avg = np.mean(list(self.metrics_history[obj])[-10:])
                recent_performance[obj] = recent_avg
        
        if recent_performance:
            # å¦‚æœæŸä¸ªç›®æ ‡è¡¨ç°æŒç»­ä½äºé˜ˆå€¼ï¼Œå¢åŠ å…¶æƒé‡
            for obj_name, obj_info in self.objectives.items():
                if obj_name in recent_performance:
                    performance = recent_performance[obj_name]
                    target = obj_info['target_value']
                    
                    if performance < target * 0.7:  # è¡¨ç°ä¸¥é‡ä¸è¶³
                        adjustment = 0.03
                        obj_info['weight'] += adjustment
                        # ä»è¡¨ç°æœ€å¥½çš„ç›®æ ‡ä¸­æ‰£é™¤
                        best_obj = max(recent_performance, key=recent_performance.get)
                        if best_obj != obj_name:
                            self.objectives[best_obj]['weight'] -= adjustment
            
            # ç¡®ä¿æƒé‡æ€»å’Œä¸º1
            total = sum(obj['weight'] for obj in self.objectives.values())
            for obj in self.objectives:
                self.objectives[obj]['weight'] /= total
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = "å¤šç›®æ ‡ä¼˜åŒ–æ€§èƒ½æŠ¥å‘Š (å¢å¼ºé¿éšœç‰ˆ):\n"
        report += "=" * 60 + "\n"
        
        for obj_name, obj_info in self.objectives.items():
            history = self.metrics_history[obj_name]
            if history:
                history_list = list(history)
                avg = np.mean(history_list[-20:]) if len(history_list) >= 20 else np.mean(history_list)
                trend = "â†‘" if len(history_list) >= 2 and history_list[-1] > history_list[-2] else "â†“"
                report += f"{obj_name:20s} æƒé‡:{obj_info['weight']:.2f} å¾—åˆ†:{avg:.3f}{trend}\n"
                report += f"  ç›®æ ‡å€¼:{obj_info['target_value']:.2f} - {obj_info['description']}\n"
        
        # è®¡ç®—æ€»ä½“é¿éšœæˆåŠŸç‡
        if self.metrics_history['obstacle_avoidance']:
            avoidance_rate = np.mean(self.metrics_history['obstacle_avoidance'])
            report += f"\næ€»ä½“é¿éšœæˆåŠŸç‡: {avoidance_rate:.2%}\n"
        
        return report


# æ¨¡ä»¿å­¦ä¹ ç®¡ç†å™¨ - å¢å¼ºç‰ˆ
class EnhancedImitationLearningManager:
    def __init__(self, expert_data_path=None):
        self.expert_data_path = expert_data_path
        self.expert_data = []
        self.is_pretrained = False
        self.avoidance_demos = []  # ä¸“é—¨å­˜å‚¨é¿éšœæ¼”ç¤º
        
    def load_expert_data(self, path):
        """åŠ è½½ä¸“å®¶ç¤ºèŒƒæ•°æ®"""
        try:
            if os.path.exists(path):
                with open(path, 'rb') as f:
                    data = pickle.load(f)
                    if isinstance(data, dict) and 'demonstrations' in data:
                        self.expert_data = data['demonstrations']
                        if 'avoidance_demos' in data:
                            self.avoidance_demos = data['avoidance_demos']
                    else:
                        self.expert_data = data
                print(f"å·²åŠ è½½ {len(self.expert_data)} æ¡ä¸“å®¶ç¤ºèŒƒæ•°æ®")
                if self.avoidance_demos:
                    print(f"å·²åŠ è½½ {len(self.avoidance_demos)} æ¡é¿éšœä¸“ç”¨æ¼”ç¤º")
                return True
            else:
                print(f"ä¸“å®¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False
        except Exception as e:
            print(f"åŠ è½½ä¸“å®¶æ•°æ®å¤±è´¥: {e}")
            return False
    
    def collect_expert_demonstration(self, env, num_episodes=3, focus_avoidance=True):
        """æ”¶é›†ä¸“å®¶ç¤ºèŒƒæ•°æ®ï¼Œç‰¹åˆ«å…³æ³¨é¿éšœåœºæ™¯"""
        print(f"å¼€å§‹æ”¶é›†ä¸“å®¶ç¤ºèŒƒæ•°æ® ({num_episodes}ä¸ªepisodes)...")
        
        demonstrations = []
        avoidance_demos = []
        
        for episode in range(num_episodes):
            print(f"æ”¶é›†ä¸“å®¶ç¤ºèŒƒ Episode {episode + 1}/{num_episodes}")
            
            # è°ƒæ•´ç¯å¢ƒéš¾åº¦ï¼Œä¸“æ³¨äºé¿éšœ
            if focus_avoidance:
                config = {'pedestrian_cross': 8, 'pedestrian_normal': 4}
                env.spawn_pedestrians_with_config(config)
            
            state = env.reset(episode)
            done = False
            episode_data = []
            obstacle_encountered = False
            
            while not done:
                # ä½¿ç”¨å¢å¼ºçš„è§„åˆ™æ§åˆ¶å™¨ï¼Œç‰¹åˆ«æ³¨é‡é¿éšœ
                action = self._enhanced_rule_based_controller(env)
                
                new_state, reward, done, _ = env.step(action)
                
                # æ£€æŸ¥æ˜¯å¦é‡åˆ°éšœç¢ç‰©
                min_distance = getattr(env, 'last_ped_distance', float('inf'))
                if min_distance < 8.0:
                    obstacle_encountered = True
                
                # åªä¿å­˜é¿éšœç›¸å…³çš„ç¤ºèŒƒæ•°æ®
                if min_distance < 8.0:  # åªåœ¨æœ‰éšœç¢ç‰©æ—¶ä¿å­˜æ•°æ®
                    demo_entry = {
                        'state': state.copy(),
                        'action': action,
                        'reward': reward,
                        'next_state': new_state.copy(),
                        'done': done,
                        'obstacle_nearby': min_distance < 8.0,
                        'obstacle_distance': min_distance
                    }
                    
                    episode_data.append(demo_entry)
                    avoidance_demos.append(demo_entry)
                
                state = new_state
            
            # å¦‚æœæ•´ä¸ªepisodeä¸­æœ‰é¿éšœåœºæ™¯ï¼Œä¿å­˜ä¸ºé¿éšœæ¼”ç¤º
            if obstacle_encountered:
                avoidance_demos.extend(episode_data)
            
            demonstrations.extend(episode_data)
            env.cleanup_actors()
        
        # ä¿å­˜ä¸“å®¶æ•°æ®
        self.expert_data = avoidance_demos  # åªä¿å­˜é¿éšœæ¼”ç¤º
        self.avoidance_demos = avoidance_demos
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_data = {
            'demonstrations': avoidance_demos,  # åªä¿å­˜é¿éšœæ¼”ç¤º
            'avoidance_demos': avoidance_demos,
            'collection_date': timestamp,
            'num_episodes': num_episodes,
            'focus_avoidance': focus_avoidance
        }
        
        save_path = f"expert_data_enhanced_{timestamp}.pkl"
        
        with open(save_path, 'wb') as f:
            pickle.dump(save_data, f)
        
        print(f"ä¸“å®¶ç¤ºèŒƒæ•°æ®å·²ä¿å­˜åˆ°: {save_path}")
        print(f"å…± {len(demonstrations)} æ¡è®°å½•ï¼Œå…¶ä¸­ {len(avoidance_demos)} æ¡é¿éšœä¸“ç”¨æ¼”ç¤º")
        return True
    
    def _enhanced_rule_based_controller(self, env):
        """å¢å¼ºçš„åŸºäºè§„åˆ™çš„æ§åˆ¶å™¨ï¼Œç‰¹åˆ«æ³¨é‡é¿éšœ"""
        # è·å–è½¦è¾†çŠ¶æ€
        vehicle_location = env.vehicle.get_location()
        velocity = env.vehicle.get_velocity()
        speed_kmh = 3.6 * math.sqrt(velocity.x**2 + velocity.y**2)
        
        # æ£€æŸ¥å‰æ–¹éšœç¢ç‰©
        obstacle_info = self._check_obstacles_ahead(env)
        has_obstacle = obstacle_info['has_obstacle']
        obstacle_distance = obstacle_info['distance']
        obstacle_direction = obstacle_info['direction']  # 'left', 'right', 'center'
        
        # é¿éšœä¼˜å…ˆçº§æœ€é«˜
        if has_obstacle:
            if obstacle_distance < 5.0:  # ç´§æ€¥é¿è®©
                if obstacle_direction == 'left':
                    return 4  # å³è½¬é¿è®©
                elif obstacle_direction == 'right':
                    return 3  # å·¦è½¬é¿è®©
                else:  # æ­£å‰æ–¹
                    return 0  # ç´§æ€¥åˆ¶åŠ¨
            elif obstacle_distance < 10.0:  # é¢„è­¦è·ç¦»
                if speed_kmh > 20:
                    return 0  # å‡é€Ÿ
                elif obstacle_direction == 'center':
                    # è½»å¾®è½¬å‘é¿è®©
                    return 3 if random.random() > 0.5 else 4
                else:
                    return 1  # ä¿æŒè­¦æƒ•
        
        # é€Ÿåº¦æ§åˆ¶
        if speed_kmh < 15:
            return 2  # åŠ é€Ÿ
        elif speed_kmh > 35:
            return 0  # å‡é€Ÿ
        else:
            return 1  # ä¿æŒ
    
    def _check_obstacles_ahead(self, env):
        """æ£€æŸ¥å‰æ–¹éšœç¢ç‰©ï¼Œè¿”å›è¯¦ç»†ä¿¡æ¯"""
        vehicle_location = env.vehicle.get_location()
        
        has_obstacle = False
        min_distance = float('inf')
        obstacle_direction = 'center'
        
        # æ£€æŸ¥è¡Œäºº
        for walker in env.walker_list:
            if not walker.is_alive:
                continue
                
            ped_location = walker.get_location()
            dx = ped_location.x - vehicle_location.x
            dy = ped_location.y - vehicle_location.y
            
            # åªè€ƒè™‘å‰æ–¹çš„è¡Œäººï¼ˆè½¦è¾†æœå‘ä¸º0åº¦ï¼‰
            if dx > 0 and abs(dy) < 20:  # å‰æ–¹20ç±³å†…
                distance = math.sqrt(dx**2 + dy**2)
                if distance < min_distance:
                    min_distance = distance
                    has_obstacle = True
                    
                    # åˆ¤æ–­éšœç¢ç‰©æ–¹å‘
                    if dy > 2:  # åœ¨è½¦è¾†å³ä¾§
                        obstacle_direction = 'right'
                    elif dy < -2:  # åœ¨è½¦è¾†å·¦ä¾§
                        obstacle_direction = 'left'
                    else:
                        obstacle_direction = 'center'
        
        return {
            'has_obstacle': has_obstacle,
            'distance': min_distance,
            'direction': obstacle_direction
        }
    
    def pretrain_with_behavioral_cloning(self, model, epochs=20, focus_avoidance=True):
        """ä½¿ç”¨è¡Œä¸ºå…‹éš†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯é€‰ä¸“æ³¨é¿éšœ"""
        if not self.expert_data:
            print("æ²¡æœ‰ä¸“å®¶æ•°æ®å¯ç”¨ï¼Œè·³è¿‡é¢„è®­ç»ƒ")
            return model
        
        print(f"å¼€å§‹è¡Œä¸ºå…‹éš†é¢„è®­ç»ƒ ({epochs}ä¸ªepochs)...")
        
        # é€‰æ‹©è®­ç»ƒæ•°æ®
        if focus_avoidance and self.avoidance_demos:
            print(f"ä½¿ç”¨ {len(self.avoidance_demos)} æ¡é¿éšœä¸“ç”¨æ¼”ç¤ºè¿›è¡Œè®­ç»ƒ")
            training_data = self.avoidance_demos
        else:
            print(f"ä½¿ç”¨ {len(self.expert_data)} æ¡å¸¸è§„æ¼”ç¤ºè¿›è¡Œè®­ç»ƒ")
            training_data = self.expert_data
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        states = []
        actions = []
        
        for demo in training_data:
            states.append(demo['state'])
            actions.append(demo['action'])
        
        # æ•°æ®å¢å¼ºï¼šå¯¹å›¾åƒè¿›è¡Œè½»å¾®å˜æ¢ä»¥å¢å¼ºæ³›åŒ–
        augmented_states = []
        augmented_actions = []
        
        for state, action in zip(states, actions):
            # åŸå§‹æ•°æ®
            augmented_states.append(state)
            augmented_actions.append(action)
            
            # æ·»åŠ è½»å¾®äº®åº¦å˜åŒ–
            if random.random() > 0.7:
                bright_state = np.clip(state * random.uniform(0.8, 1.2), 0, 255).astype(np.uint8)
                augmented_states.append(bright_state)
                augmented_actions.append(action)
            
            # æ·»åŠ è½»å¾®å¯¹æ¯”åº¦å˜åŒ–
            if random.random() > 0.7:
                contrast = random.uniform(0.8, 1.2)
                mean = np.mean(state)
                contrast_state = np.clip((state - mean) * contrast + mean, 0, 255).astype(np.uint8)
                augmented_states.append(contrast_state)
                augmented_actions.append(action)
        
        # å°†çŠ¶æ€å½’ä¸€åŒ–
        states_array = np.array(augmented_states) / 255.0
        
        # å°†åŠ¨ä½œè½¬æ¢ä¸ºone-hotç¼–ç 
        actions_onehot = tf.keras.utils.to_categorical(augmented_actions, num_classes=5)
        
        # å¤‡ä»½åŸå§‹ç¼–è¯‘è®¾ç½®
        original_loss = model.loss
        original_optimizer = model.optimizer
        original_metrics = model.metrics_names
        
        # é‡æ–°ç¼–è¯‘æ¨¡å‹ç”¨äºåˆ†ç±»ä»»åŠ¡
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # æ·»åŠ å›è°ƒå‡½æ•°
        callbacks = [
            EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)
        ]
        
        # è®­ç»ƒæ¨¡å‹æ¨¡ä»¿ä¸“å®¶è¡Œä¸º
        history = model.fit(
            states_array, actions_onehot,
            batch_size=16,
            epochs=epochs,
            validation_split=0.2,
            verbose=1,
            callbacks=callbacks
        )
        
        print(f"é¢„è®­ç»ƒå®Œæˆ - æœ€ç»ˆå‡†ç¡®ç‡: {history.history['accuracy'][-1]:.3f}")
        
        # æ¢å¤åŸå§‹ç¼–è¯‘è®¾ç½®
        model.compile(
            optimizer=original_optimizer,
            loss=original_loss,
            metrics=original_metrics
        )
        
        self.is_pretrained = True
        return model


# DQNæ™ºèƒ½ä½“ç±» - å®Œå…¨é‡å†™ï¼Œä¸“æ³¨äºé¿éšœ
class EnhancedDQNAgent:
    def __init__(self, use_dueling=True, use_per=True, use_curriculum=True, 
                 use_multi_objective=True, use_attention=True, use_enhanced_model=True):
        
        # é…ç½®å‚æ•°
        self.use_dueling = use_dueling
        self.use_per = use_per
        self.use_curriculum = use_curriculum
        self.use_multi_objective = use_multi_objective
        self.use_attention = use_attention
        self.use_enhanced_model = use_enhanced_model
        
        # åˆ›å»ºä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        if use_enhanced_model:
            self.model = self.create_enhanced_model()
            self.target_model = self.create_enhanced_model()
        elif use_attention:
            self.model = self.create_attention_model()
            self.target_model = self.create_attention_model()
        elif use_dueling:
            self.model = self.create_dueling_model()
            self.target_model = self.create_dueling_model()
        else:
            self.model = self.create_basic_model()
            self.target_model = self.create_basic_model()
            
        self.target_model.set_weights(self.model.get_weights())

        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        if use_per:
            self.replay_buffer = EnhancedPrioritizedReplayBuffer(max_size=REPLAY_MEMORY_SIZE)
        else:
            self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)

        # è‡ªå®šä¹‰TensorBoard
        self.tensorboard = EnhancedTensorBoard(
            log_dir=f"logs/{MODEL_NAME}-enhanced-{int(time.time())}",
            histogram_freq=0,
            write_graph=True,
            write_images=False
        )
        
        self.target_update_counter = 0
        self.train_step_counter = 0

        # è®­ç»ƒæ§åˆ¶æ ‡å¿—
        self.terminate = False
        self.last_logged_episode = 0
        self.training_initialized = False
        self.training_paused = False
        
        # è®­ç»ƒç­–ç•¥ç»„ä»¶
        self.curriculum_manager = None
        self.multi_objective_optimizer = None
        self.imitation_manager = None
        
        # é¿éšœæ€§èƒ½è·Ÿè¸ª
        self.obstacle_avoidance_stats = {
            'success_count': 0,
            'total_encounters': 0,
            'near_misses': 0,
            'collisions': 0
        }
        
        # é”ç”¨äºçº¿ç¨‹å®‰å…¨
        self.training_lock = Lock()
        
    def setup_training_strategies(self, env=None):
        """è®¾ç½®è®­ç»ƒç­–ç•¥ç»„ä»¶"""
        if self.use_curriculum and env:
            self.curriculum_manager = EnhancedCurriculumManager(env)
            print("å¢å¼ºç‰ˆè¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨å·²å¯ç”¨")
        
        if self.use_multi_objective:
            self.multi_objective_optimizer = EnhancedMultiObjectiveOptimizer()
            print("å¢å¼ºç‰ˆå¤šç›®æ ‡ä¼˜åŒ–å™¨å·²å¯ç”¨")
        
        # æ¨¡ä»¿å­¦ä¹ ç®¡ç†å™¨
        self.imitation_manager = EnhancedImitationLearningManager()
        print("å¢å¼ºç‰ˆæ¨¡ä»¿å­¦ä¹ ç®¡ç†å™¨å·²å¯ç”¨")
        
    def create_basic_model(self):
        """åˆ›å»ºåŸºç¡€æ·±åº¦Qç½‘ç»œæ¨¡å‹"""
        inputs = Input(shape=(IM_HEIGHT, IM_WIDTH, 3))
        
        # ç‰¹å¾æå–å±‚
        x = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(inputs)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        x = Conv2D(64, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        x = Conv2D(128, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # å±•å¹³å±‚
        x = Flatten()(x)
        
        # å…¨è¿æ¥å±‚
        x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)
        x = Dropout(0.3)(x)
        x = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)
        x = Dropout(0.2)(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.1)(x)
        
        # è¾“å‡ºå±‚
        outputs = Dense(5, activation='linear')(x)
        
        # åˆ›å»ºæ¨¡å‹
        model = Model(inputs=inputs, outputs=outputs)
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            loss="huber", 
            optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0),
            metrics=["mae"]
        )
        return model
    
    def create_dueling_model(self):
        """åˆ›å»ºDueling DQNæ¨¡å‹"""
        inputs = Input(shape=(IM_HEIGHT, IM_WIDTH, 3))
        
        # å…±äº«çš„ç‰¹å¾æå–å±‚
        x = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(inputs)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        x = Conv2D(64, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        x = Conv2D(128, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # å±•å¹³å±‚
        x = Flatten()(x)
        
        # å…±äº«çš„å…¨è¿æ¥å±‚
        shared = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)
        shared = Dropout(0.3)(shared)
        shared = Dense(256, activation='relu')(shared)
        
        # ä»·å€¼æµ
        value_stream = Dense(128, activation='relu')(shared)
        value_stream = Dropout(0.2)(value_stream)
        value = Dense(1, activation='linear', name='value')(value_stream)
        
        # ä¼˜åŠ¿æµ
        advantage_stream = Dense(128, activation='relu')(shared)
        advantage_stream = Dropout(0.2)(advantage_stream)
        advantage = Dense(5, activation='linear', name='advantage')(advantage_stream)
        
        # åˆå¹¶: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
        mean_advantage = Lambda(lambda a: tf.reduce_mean(a, axis=1, keepdims=True))(advantage)
        advantage_centered = Subtract()([advantage, mean_advantage])
        q_values = Add()([value, advantage_centered])
        
        # åˆ›å»ºæ¨¡å‹
        model = Model(inputs=inputs, outputs=q_values)
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            loss="huber",
            optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0),
            metrics=["mae"]
        )
        
        return model
    
    def create_attention_model(self):
        """åˆ›å»ºå¸¦æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹"""
        inputs = Input(shape=(IM_HEIGHT, IM_WIDTH, 3))
        
        # ç¬¬ä¸€å·ç§¯å—
        x = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(inputs)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # ç¬¬äºŒå·ç§¯å— + æ³¨æ„åŠ›
        x = Conv2D(64, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = ObstacleAttentionLayer(64)(x)  # æ·»åŠ æ³¨æ„åŠ›å±‚
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # ç¬¬ä¸‰å·ç§¯å— + æ³¨æ„åŠ›
        x = Conv2D(128, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = ObstacleAttentionLayer(128)(x)  # ç¬¬äºŒå±‚æ³¨æ„åŠ›
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # å±•å¹³å±‚
        x = Flatten()(x)
        
        # å…¨è¿æ¥å±‚
        x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)
        x = Dropout(0.3)(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.2)(x)
        
        # Duelingæ¶æ„
        # ä»·å€¼æµ
        value_stream = Dense(128, activation='relu')(x)
        value_stream = Dropout(0.2)(value_stream)
        value = Dense(1, activation='linear', name='value')(value_stream)
        
        # ä¼˜åŠ¿æµ
        advantage_stream = Dense(128, activation='relu')(x)
        advantage_stream = Dropout(0.2)(advantage_stream)
        advantage = Dense(5, activation='linear', name='advantage')(advantage_stream)
        
        # åˆå¹¶
        mean_advantage = Lambda(lambda a: tf.reduce_mean(a, axis=1, keepdims=True))(advantage)
        advantage_centered = Subtract()([advantage, mean_advantage])
        q_values = Add()([value, advantage_centered])
        
        # åˆ›å»ºæ¨¡å‹
        model = Model(inputs=inputs, outputs=q_values)
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            loss="huber",
            optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0),
            metrics=["mae"]
        )
        
        return model
    
    def create_enhanced_model(self):
        """åˆ›å»ºå¢å¼ºç‰ˆæ¨¡å‹ï¼Œä¸“é—¨ç”¨äºé¿éšœ"""
        inputs = Input(shape=(IM_HEIGHT, IM_WIDTH, 3))
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        # åˆ†æ”¯1: å¤§æ„Ÿå—é‡ï¼Œæ£€æµ‹è¿œå¤„éšœç¢ç‰©
        branch1 = Conv2D(32, (7, 7), strides=(2, 2), padding='same')(inputs)
        branch1 = Activation('relu')(branch1)
        branch1 = BatchNormalization()(branch1)
        branch1 = MaxPooling2D(pool_size=(2, 2))(branch1)
        
        # åˆ†æ”¯2: ä¸­ç­‰æ„Ÿå—é‡
        branch2 = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(inputs)
        branch2 = Activation('relu')(branch2)
        branch2 = BatchNormalization()(branch2)
        branch2 = MaxPooling2D(pool_size=(2, 2))(branch2)
        
        # åˆ†æ”¯3: å°æ„Ÿå—é‡ï¼Œæ£€æµ‹è¿‘å¤„ç»†èŠ‚
        branch3 = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)
        branch3 = Activation('relu')(branch3)
        branch3 = BatchNormalization()(branch3)
        branch3 = MaxPooling2D(pool_size=(2, 2))(branch3)
        
        # åˆå¹¶åˆ†æ”¯
        merged = Concatenate()([branch1, branch2, branch3])
        
        # æ·±åº¦ç‰¹å¾æå–
        x = Conv2D(128, (3, 3), padding='same')(merged)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = ObstacleAttentionLayer(128)(x)  # æ³¨æ„åŠ›æœºåˆ¶
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        x = Conv2D(256, (3, 3), padding='same')(x)
        x = Activation('relu')(x)
        x = BatchNormalization()(x)
        x = ObstacleAttentionLayer(256)(x)  # ç¬¬äºŒå±‚æ³¨æ„åŠ›
        x = MaxPooling2D(pool_size=(2, 2))(x)
        
        # å±•å¹³å±‚
        x = Flatten()(x)
        
        # å¯†é›†è¿æ¥å±‚
        x = Dense(1024, activation='relu', kernel_regularizer=l2(1e-4))(x)
        x = Dropout(0.4)(x)
        x = Dense(512, activation='relu', kernel_regularizer=l2(1e-4))(x)
        x = Dropout(0.3)(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.2)(x)
        
        # åŒæµè¾“å‡ºï¼ˆDueling DQNï¼‰
        # ä»·å€¼æµ
        value_stream = Dense(128, activation='relu')(x)
        value_stream = Dropout(0.2)(value_stream)
        value = Dense(1, activation='linear', name='value')(value_stream)
        
        # ä¼˜åŠ¿æµï¼ˆç‰¹åˆ«å…³æ³¨é¿éšœåŠ¨ä½œï¼‰
        advantage_stream = Dense(128, activation='relu')(x)
        advantage_stream = Dropout(0.2)(advantage_stream)
        advantage = Dense(5, activation='linear', name='advantage')(advantage_stream)
        
        # åˆå¹¶: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
        mean_advantage = Lambda(lambda a: tf.reduce_mean(a, axis=1, keepdims=True))(advantage)
        advantage_centered = Subtract()([advantage, mean_advantage])
        q_values = Add()([value, advantage_centered])
        
        # åˆ›å»ºæ¨¡å‹
        model = Model(inputs=inputs, outputs=q_values)
        
        # ç¼–è¯‘æ¨¡å‹
        optimizer = Adam(
            learning_rate=LEARNING_RATE,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7,
            clipnorm=1.0
        )
        
        model.compile(
            loss="huber",
            optimizer=optimizer,
            metrics=["mae"]
        )
        
        print("å¢å¼ºç‰ˆæ¨¡å‹åˆ›å»ºå®Œæˆï¼ˆä¸“é—¨ç”¨äºé¿éšœï¼‰")
        return model
    
    def update_replay_memory(self, transition, is_obstacle=False, is_success=False):
        """æ›´æ–°ç»éªŒå›æ”¾ç¼“å†²åŒº"""
        if self.use_per:
            self.replay_buffer.add(transition, error=1.0, 
                                  is_obstacle=is_obstacle, 
                                  is_success=is_success)
        else:
            self.replay_memory.append(transition)
            
            # å¦‚æœæ˜¯é‡è¦ç»éªŒï¼Œé¢å¤–å­˜å‚¨
            if is_obstacle or is_success:
                self.replay_memory.append(transition)  # é‡è¦ç»éªŒé‡å¤å­˜å‚¨
    
    def train(self):
        """è®­ç»ƒDQNç½‘ç»œ"""
        with self.training_lock:
            if self.training_paused:
                return
                
            if self.use_per:
                if len(self.replay_buffer) < MIN_REPLAY_MEMORY_SIZE:
                    return
                    
                # PERé‡‡æ ·ï¼Œç‰¹åˆ«å…³æ³¨é¿éšœç»éªŒ
                indices, minibatch, weights = self.replay_buffer.sample(
                    MINIBATCH_SIZE, 
                    obstacle_ratio=0.3,  # 30%é¿éšœç»éªŒ
                    success_ratio=0.2     # 20%æˆåŠŸç»éªŒ
                )
                if len(minibatch) == 0:
                    return
            else:
                if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:
                    return
                    
                # æ ‡å‡†é‡‡æ ·
                minibatch = random.sample(self.replay_memory, 
                                         min(MINIBATCH_SIZE, len(self.replay_memory)))
                weights = np.ones(len(minibatch))

            # å‡†å¤‡è®­ç»ƒæ•°æ®
            current_states = np.array([transition[0] for transition in minibatch]) / 255
            current_qs_list = self.model.predict(current_states, 
                                                batch_size=PREDICTION_BATCH_SIZE,
                                                verbose=0)

            new_current_states = np.array([transition[3] for transition in minibatch]) / 255
            future_qs_list = self.target_model.predict(new_current_states, 
                                                      batch_size=PREDICTION_BATCH_SIZE,
                                                      verbose=0)

            x = []  # è¾“å…¥çŠ¶æ€
            y = []  # ç›®æ ‡Qå€¼
            errors = []  # TDè¯¯å·®

            # è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆDouble DQNé£æ ¼ï¼‰
            for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):
                if not done:
                    # Double DQN: ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°
                    next_qs = self.model.predict(np.array([new_state]) / 255, verbose=0)[0]
                    best_action = np.argmax(next_qs)
                    max_future_q = future_qs_list[index][best_action]
                    new_q = reward + DISCOUNT * max_future_q
                else:
                    new_q = reward

                current_qs = current_qs_list[index].copy()
                old_q = current_qs[action]
                current_qs[action] = new_q
                
                # è®¡ç®—TDè¯¯å·®
                td_error = abs(new_q - old_q)
                errors.append(td_error)

                x.append(current_state)
                y.append(current_qs)

            # PER: æ›´æ–°ä¼˜å…ˆçº§
            if self.use_per and len(errors) > 0:
                self.replay_buffer.update_priorities(indices, errors)

            # è®°å½•æ—¥å¿—
            log_this_step = False
            if self.tensorboard.step > self.last_logged_episode:
                log_this_step = True
                self.last_logged_episode = self.tensorboard.step

            # è®­ç»ƒæ¨¡å‹
            self.model.fit(
                np.array(x) / 255, 
                np.array(y),
                batch_size=TRAINING_BATCH_SIZE,
                sample_weight=weights if self.use_per else None,
                verbose=0, 
                shuffle=False,
                callbacks=[self.tensorboard] if log_this_step else None
            )

            self.train_step_counter += 1

            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            if log_this_step:
                self.target_update_counter += 1

            if self.target_update_counter > UPDATE_TARGET_EVERY:
                print("ç›®æ ‡ç½‘ç»œå·²æ›´æ–°")
                self.target_model.set_weights(self.model.get_weights())
                self.target_update_counter = 0
                
    def train_in_loop(self):
        """åœ¨å•ç‹¬çº¿ç¨‹ä¸­æŒç»­è®­ç»ƒ"""
        # é¢„çƒ­è®­ç»ƒ
        x = np.random.uniform(size=(1, IM_HEIGHT, IM_WIDTH, 3)).astype(np.float32)
        y = np.random.uniform(size=(1, 5)).astype(np.float32)

        self.model.fit(x, y, verbose=False, batch_size=1)
        self.training_initialized = True

        print("è®­ç»ƒçº¿ç¨‹å·²å¯åŠ¨")
        
        # æŒç»­è®­ç»ƒå¾ªç¯
        while True:
            if self.terminate:
                print("è®­ç»ƒçº¿ç¨‹ç»ˆæ­¢")
                return
                
            try:
                self.train()
                time.sleep(0.005)  # æ›´é«˜çš„è®­ç»ƒé¢‘ç‡
            except Exception as e:
                print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                time.sleep(0.1)

    def get_qs(self, state):
        """è·å–çŠ¶æ€çš„Qå€¼"""
        return self.model.predict(np.array(state).reshape(-1, *state.shape) / 255, 
                                verbose=0)[0]
    
    def update_obstacle_stats(self, success, is_collision=False, is_near_miss=False):
        """æ›´æ–°é¿éšœç»Ÿè®¡"""
        self.obstacle_avoidance_stats['total_encounters'] += 1
        if success:
            self.obstacle_avoidance_stats['success_count'] += 1
        if is_collision:
            self.obstacle_avoidance_stats['collisions'] += 1
        if is_near_miss:
            self.obstacle_avoidance_stats['near_misses'] += 1
    
    def get_obstacle_avoidance_rate(self):
        """è·å–é¿éšœæˆåŠŸç‡"""
        total = self.obstacle_avoidance_stats['total_encounters']
        if total == 0:
            return 0.0
        return self.obstacle_avoidance_stats['success_count'] / total
    
    def pause_training(self):
        """æš‚åœè®­ç»ƒ"""
        self.training_paused = True
        
    def resume_training(self):
        """æ¢å¤è®­ç»ƒ"""
        self.training_paused = False
    
    def save_model(self, path, include_stats=True):
        """ä¿å­˜æ¨¡å‹"""
        try:
            # å°è¯•ä½¿ç”¨Kerasçš„æ ‡å‡†ä¿å­˜æ–¹æ³•
            self.model.save(path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
        except Exception as e:
            print(f"æ ‡å‡†ä¿å­˜å¤±è´¥ï¼Œå°è¯•æ›¿ä»£æ–¹æ³•: {e}")
            try:
                # å°è¯•ä¿å­˜æƒé‡
                weights_path = path.replace('.model', '_weights.h5')
                self.model.save_weights(weights_path)
                print(f"æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: {weights_path}")

                # ä¿å­˜æ¨¡å‹æ¶æ„
                config_path = path.replace('.model', '_config.json')
                with open(config_path, 'w') as f:
                    f.write(self.model.to_json())
                print(f"æ¨¡å‹æ¶æ„å·²ä¿å­˜åˆ°: {config_path}")
            except Exception as e2:
                print(f"æƒé‡ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")
                print("è·³è¿‡æ¨¡å‹ä¿å­˜ï¼Œç»§ç»­è®­ç»ƒ...")
        
        if include_stats:
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            stats = {
                'obstacle_avoidance_stats': self.obstacle_avoidance_stats,
                'train_step_counter': self.train_step_counter,
                'model_config': {
                    'use_dueling': self.use_dueling,
                    'use_per': self.use_per,
                    'use_attention': self.use_attention,
                    'use_enhanced_model': self.use_enhanced_model
                }
            }
            
            stats_path = path.replace('.model', '_stats.pkl')
            with open(stats_path, 'wb') as f:
                pickle.dump(stats, f)
            print(f"è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_path}")
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        try:
            # å®šä¹‰è‡ªå®šä¹‰å±‚
            custom_objects = {
                'ObstacleAttentionLayer': ObstacleAttentionLayer,
                'Add': Add,
                'Subtract': Subtract,
                'Lambda': Lambda,
                'Multiply': Multiply
            }
            
            self.model = tf.keras.models.load_model(path, custom_objects=custom_objects)
            self.target_model.set_weights(self.model.get_weights())
            print(f"æ¨¡å‹å·²ä» {path} åŠ è½½")
            return True
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False