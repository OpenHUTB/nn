# main.py
import glob
import os
import sys
import random
import time
import numpy as np
import cv2
import math
import matplotlib.pyplot as plt
from collections import deque
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import tensorflow.keras.backend as backend
from threading import Thread

from tqdm import tqdm
import pickle

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from Environment import CarEnv
from Model import DQNAgent
from TrainingStrategies import CurriculumManager, MultiObjectiveOptimizer, ImitationLearningManager
import Hyperparameters

# ä»Hyperparameterså¯¼å…¥æ‰€æœ‰å‚æ•°
from Hyperparameters import *

def ensure_models_directory():
    """ç¡®ä¿modelsç›®å½•å­˜åœ¨"""
    if not os.path.exists('models'):
        os.makedirs('models')
        print("âœ… å·²åˆ›å»º models ç›®å½•")
    return 'models'

def save_model_with_retry(model, filepath, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æ¨¡å‹ä¿å­˜"""
    for attempt in range(max_retries):
        try:
            model.save(filepath)
            print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {os.path.basename(filepath)}")
            return True
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
            time.sleep(1)
    
    print(f"âŒ æ— æ³•ä¿å­˜æ¨¡å‹: {filepath}")
    return False

def create_dummy_state(env):
    """åˆ›å»ºè™šæ‹ŸçŠ¶æ€ç”¨äºæµ‹è¯•"""
    return {
        'image': np.ones((env.im_height, env.im_width, 3)),
        'location': np.array([-81.0, -195.0]),  # 2ç»´
        'speed': np.array([0.0]),
        'heading': np.array([0.0]),
        'last_action': np.array([1])
    }

def extended_reward_calculation(env, action, reward, done, step_info):
    """æ‰©å±•çš„å¥–åŠ±è®¡ç®—å‡½æ•°"""
    # è·å–è½¦è¾†çŠ¶æ€
    vehicle_location = env.vehicle.get_location()
    velocity = env.vehicle.get_velocity()
    speed_kmh = 3.6 * math.sqrt(velocity.x**2 + velocity.y**2)
    
    # è®¡ç®—å¤šç›®æ ‡æŒ‡æ ‡
    metrics = {}
    
    # 1. ååº”æ—¶é—´æŒ‡æ ‡
    reaction_time = 0
    if hasattr(env, 'obstacle_detected_time') and env.obstacle_detected_time is not None:
        if hasattr(env, 'reaction_start_time') and env.reaction_start_time is not None:
            reaction_time = time.time() - env.reaction_start_time
    
    metrics['reaction_time'] = reaction_time
    
    # 2. ä¸»åŠ¨é¿éšœæŒ‡æ ‡
    proactive_action = False
    if hasattr(env, 'suggested_action') and env.suggested_action is not None:
        if action == env.suggested_action:
            proactive_action = True
    
    metrics['proactive_action'] = proactive_action
    
    # 3. å®‰å…¨æ€§æŒ‡æ ‡
    min_ped_distance = getattr(env, 'last_ped_distance', float('inf'))
    safety_score = 0
    if min_ped_distance < 100:
        if min_ped_distance > 12:
            safety_score = 10
        elif min_ped_distance > 8:
            safety_score = 7
        elif min_ped_distance > 5:
            safety_score = 3
        elif min_ped_distance > 3:
            safety_score = 1
        else:
            safety_score = 0
    
    metrics['safety'] = safety_score
    
    # 4. é™æ€éšœç¢ç‰©æŒ‡æ ‡
    if hasattr(env, 'check_static_obstacles'):
        static_distance, _ = env.check_static_obstacles(vehicle_location)
        metrics['static_distance'] = static_distance
        
        if static_distance == 0:
            metrics['static_collision'] = True
        else:
            metrics['static_collision'] = False
    
    # 5. é“è·¯è¾¹ç•ŒæŒ‡æ ‡
    if hasattr(env, 'check_road_boundary'):
        boundary_distance, out_of_boundary = env.check_road_boundary(vehicle_location)
        metrics['off_road'] = out_of_boundary
    
    # 6. æ•ˆç‡æŒ‡æ ‡
    progress = (vehicle_location.x + 81) / 236.0
    efficiency_score = progress * 100
    metrics['efficiency'] = efficiency_score
    
    # 7. èˆ’é€‚åº¦æŒ‡æ ‡
    comfort_score = 5
    
    if hasattr(env, 'last_action') and env.last_action in [3, 4]:
        if getattr(env, 'same_steer_counter', 0) > 2:
            comfort_score = 2
        elif getattr(env, 'same_steer_counter', 0) > 1:
            comfort_score = 3
        else:
            comfort_score = 4
    else:
        comfort_score = 5
    
    metrics['comfort'] = comfort_score
    
    # 8. è§„åˆ™éµå¾ªæŒ‡æ ‡
    rule_score = 0.3
    
    if 20 <= speed_kmh <= 35:
        rule_score = 1.0
    elif 15 <= speed_kmh < 20 or 35 < speed_kmh <= 40:
        rule_score = 0.7
    
    metrics['rule_following'] = rule_score
    
    # 9. ç¢°æ’æ£€æµ‹
    metrics['collision'] = len(getattr(env, 'collision_history', [])) > 0
    
    # 10. å±é™©åŠ¨ä½œæ£€æµ‹
    if speed_kmh > 40 and action in [3, 4]:
        metrics['dangerous_action'] = True
    else:
        metrics['dangerous_action'] = False
    
    return metrics

if __name__ == '__main__':
    FPS = 60
    ep_rewards = [-200]

    print("è‡ªåŠ¨é©¾é©¶æ¨¡å‹è®­ç»ƒå¼€å§‹...")
    print("=" * 60)
    
    # ç¡®ä¿modelsç›®å½•å­˜åœ¨
    models_dir = ensure_models_directory()
    
    # GPUå†…å­˜é…ç½®
    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)
    tf.compat.v1.keras.backend.set_session(
        tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)))

    # åˆ›å»ºæ™ºèƒ½ä½“å’Œç¯å¢ƒ
    print("åˆ›å»ºæ™ºèƒ½ä½“å’Œç¯å¢ƒ...")
    agent = DQNAgent(
        use_dueling=True, 
        use_per=True,
        use_curriculum=True,
        use_multi_objective=True
    )
    
    env = CarEnv()
    
    # è®¾ç½®è®­ç»ƒç­–ç•¥
    agent.setup_training_strategies(env)

    # é¢„çƒ­æ¨¡å‹
    print("é¢„çƒ­æ¨¡å‹...")
    dummy_state = create_dummy_state(env)
    
    try:
        qs = agent.get_qs(dummy_state)
        print(f"âœ… æ¨¡å‹é¢„çƒ­æˆåŠŸï¼ŒQå€¼å½¢çŠ¶: {qs.shape}")
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
        # å°è¯•ç›´æ¥è°ƒç”¨predictè¿›è¡Œé¢„çƒ­
        dummy_image = np.ones((1, env.im_height, env.im_width, 3)) / 255
        dummy_vector = np.zeros((1, 10))
        try:
            qs = agent.model.predict([dummy_image, dummy_vector], verbose=0)
            print(f"âœ… ä½¿ç”¨ç›´æ¥é¢„æµ‹æ–¹æ³•é¢„çƒ­æˆåŠŸï¼ŒQå€¼å½¢çŠ¶: {qs.shape}")
        except Exception as e2:
            print(f"âŒ ç›´æ¥é¢„æµ‹ä¹Ÿå¤±è´¥: {e2}")
            print("æ£€æŸ¥æ¨¡å‹è¾“å…¥ç»´åº¦...")
            print(f"æ¨¡å‹è¾“å…¥: {agent.model.input}")
            print(f"æ¨¡å‹è¾“å‡º: {agent.model.output}")
            sys.exit(1)

    # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
    print("å¯åŠ¨è®­ç»ƒçº¿ç¨‹...")
    trainer_thread = Thread(target=agent.train_in_loop, daemon=True)
    trainer_thread.start()
    
    # ç­‰å¾…è®­ç»ƒåˆå§‹åŒ–å®Œæˆ
    print("ç­‰å¾…è®­ç»ƒåˆå§‹åŒ–å®Œæˆ...")
    start_time = time.time()
    while not agent.training_initialized:
        time.sleep(0.01)
        if time.time() - start_time > 30:
            print("âš ï¸ è®­ç»ƒåˆå§‹åŒ–è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ...")
            break
    
    print("âœ… è®­ç»ƒåˆå§‹åŒ–å®Œæˆ")

    # è®­ç»ƒç»Ÿè®¡å˜é‡
    best_score = -float('inf')
    success_count = 0
    scores = []
    avg_scores = []
    
    # å…¶ä»–ç»Ÿè®¡å˜é‡
    per_stats = {'buffer_size': []}
    multi_obj_stats = {
        'reaction_time': [], 'safety': [], 'efficiency': [], 
        'comfort': [], 'static_avoidance': []
    }
    curriculum_stages = []
    reaction_time_stats = []
    static_collision_stats = []

    # è¿­ä»£è®­ç»ƒè½®æ¬¡
    print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {EPISODES} è½®...")
    print("=" * 60)
    
    for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):
        env.collision_hist = []
        agent.tensorboard.step = episode

        # åº”ç”¨è¯¾ç¨‹å­¦ä¹ é…ç½®
        if agent.curriculum_manager:
            config = agent.curriculum_manager.get_current_config()
            if episode % 50 == 0:
                print(f"è¯¾ç¨‹å­¦ä¹  - é˜¶æ®µ {agent.curriculum_manager.current_stage}({config['difficulty_name']})")
            curriculum_stages.append(agent.curriculum_manager.current_stage)
        
        # é‡ç½®æ¯è½®ç»Ÿè®¡
        score = 0
        step = 1
        episode_metrics = {
            'reaction_time': [], 'safety': [], 'efficiency': [], 
            'comfort': [], 'static_avoidance': []
        }

        # é‡ç½®ç¯å¢ƒ
        try:
            current_state = env.reset(episode)
        except Exception as e:
            print(f"âŒ é‡ç½®ç¯å¢ƒå¤±è´¥: {e}")
            continue

        done = False
        episode_start = time.time()
        static_collision_occurred = False

        # æœ€å¤§æ­¥æ•°
        max_steps_per_episode = SECONDS_PER_EPISODE * FPS
        if agent.curriculum_manager:
            config = agent.curriculum_manager.get_current_config()
            max_steps_per_episode = config['max_episode_steps']

        # è¿è¡Œepisode
        while not done and step < max_steps_per_episode:
            # é€‰æ‹©åŠ¨ä½œ
            if np.random.random() > Hyperparameters.EPSILON:
                try:
                    qs = agent.get_qs(current_state)
                    action = np.argmax(qs)
                    
                    # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ¥è¿‘é™æ€éšœç¢ç‰©ï¼Œè°ƒæ•´åŠ¨ä½œ
                    if hasattr(env, 'check_static_obstacles'):
                        vehicle_location = env.vehicle.get_location()
                        static_distance, _ = env.check_static_obstacles(vehicle_location)
                        
                        if static_distance < 5.0:
                            if action in [3, 4] and qs[0] > qs[action] * 0.7:
                                action = 0
                
                except Exception as e:
                    print(f"âš ï¸ è·å–Qå€¼å¤±è´¥: {e}")
                    action = np.random.randint(0, 5)
            else:
                action = np.random.randint(0, 5)
                
                # æ¢ç´¢æ—¶çš„å®‰å…¨æ£€æŸ¥
                if hasattr(env, 'check_static_obstacles'):
                    vehicle_location = env.vehicle.get_location()
                    static_distance, _ = env.check_static_obstacles(vehicle_location)
                    
                    if static_distance < 3.0:
                        safe_actions = [0, 3, 4]
                        action = np.random.choice(safe_actions)
                
                time.sleep(1 / FPS)

            # æ‰§è¡ŒåŠ¨ä½œ
            try:
                new_state, reward, done, _ = env.step(action)
            except Exception as e:
                print(f"âŒ æ‰§è¡ŒåŠ¨ä½œå¤±è´¥: {e}")
                break

            # æ£€æµ‹é™æ€ç¢°æ’
            static_collision = False
            if hasattr(env, 'check_static_obstacles'):
                vehicle_location = env.vehicle.get_location()
                static_distance, _ = env.check_static_obstacles(vehicle_location)
                if static_distance == 0:
                    static_collision = True
                    static_collision_occurred = True

            # è®¡ç®—å¤šç›®æ ‡æŒ‡æ ‡
            if agent.multi_objective_optimizer:
                try:
                    step_info = {'step': step, 'action': action}
                    metrics = extended_reward_calculation(env, action, reward, done, step_info)
                    
                    for key in episode_metrics:
                        if key in metrics:
                            episode_metrics[key].append(metrics[key])
                    
                    composite_reward = agent.multi_objective_optimizer.compute_composite_reward(metrics)
                    reward = composite_reward
                except Exception as e:
                    print(f"âš ï¸ è®¡ç®—å¤šç›®æ ‡å¥–åŠ±å¤±è´¥: {e}")

            score += reward
            
            # æ›´æ–°ç»éªŒå›æ”¾
            try:
                agent.update_replay_memory((current_state, action, reward, new_state, done))
            except Exception as e:
                print(f"âš ï¸ æ›´æ–°ç»éªŒå›æ”¾å¤±è´¥: {e}")

            current_state = new_state
            step += 1

            if done:
                break

        # æ¸…ç†ç¯å¢ƒ
        try:
            env.cleanup_actors()
        except:
            pass

        # è®°å½•ç»Ÿè®¡
        scores.append(score)
        static_collision_stats.append(1 if static_collision_occurred else 0)
        
        # æ›´æ–°æˆåŠŸè®¡æ•°
        if score > 5:
            success_count += 1

        # ä¿å­˜æ¨¡å‹
        if episode % 10 == 0:
            model_path = f'{models_dir}/{MODEL_NAME}_ep{episode}_score{score:.1f}.model'
            save_model_with_retry(agent.model, model_path)
        
        if score > best_score:
            best_score = score
            model_path = f'{models_dir}/{MODEL_NAME}_best_ep{episode}_score{score:.1f}.model'
            save_model_with_retry(agent.model, model_path)
            print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹: Episode {episode}, å¾—åˆ†: {score:.2f}")

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if episode % 10 == 0:
            avg_score = np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores)
            print(f'è½®æ¬¡: {episode:3d}, å¾—åˆ†: {score:6.2f}, æœ€è¿‘10è½®å¹³å‡: {avg_score:6.2f}, æˆåŠŸ: {success_count:3d}')

        # è¡°å‡æ¢ç´¢ç‡
        if Hyperparameters.EPSILON > Hyperparameters.MIN_EPSILON:
            Hyperparameters.EPSILON *= Hyperparameters.EPSILON_DECAY
            Hyperparameters.EPSILON = max(Hyperparameters.MIN_EPSILON, Hyperparameters.EPSILON)

    # ç»“æŸè®­ç»ƒ
    agent.terminate = True
    trainer_thread.join()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = f'{models_dir}/{MODEL_NAME}_final_ep{EPISODES}_avg{np.mean(scores):.1f}.model'
    if save_model_with_retry(agent.model, final_model_path):
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆ!")
    print("="*60)
    print(f"æœ€ç»ˆç»Ÿè®¡:")
    print(f"  æ€»è½®æ¬¡: {EPISODES}")
    print(f"  æœ€ä½³å¾—åˆ†: {max(scores) if scores else 0:.2f}")
    print(f"  å¹³å‡å¾—åˆ†: {np.mean(scores) if scores else 0:.2f}")
    print(f"  æˆåŠŸç‡: {(success_count/EPISODES)*100:.1f}%")
    print(f"  é™æ€ç¢°æ’ç‡: {np.mean(static_collision_stats) if static_collision_stats else 0:.2%}")
    print(f"  æœ€ç»ˆæ¢ç´¢ç‡: {Hyperparameters.EPSILON:.4f}")
    
    # æ˜¾ç¤ºä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
    print(f"\nå·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶:")
    model_files = glob.glob(f'{models_dir}/*.model')
    if model_files:
        for model_file in sorted(model_files, key=os.path.getmtime)[-10:]:
            file_size = os.path.getsize(model_file) / (1024 * 1024)
            print(f"  ğŸ“ {os.path.basename(model_file)} ({file_size:.1f} MB)")
    else:
        print("  âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")