import os
import json
import numpy as np
from collections import defaultdict
import hashlib
import pickle
import time


class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨ - ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""

    # æ·»åŠ ç¼“å­˜æœºåˆ¶
    _cache_dir = ".analysis_cache"
    _cache_enabled = True

    @staticmethod
    def _get_cache_key(data_dir):
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨ç›®å½•ç»“æ„å’Œæ–‡ä»¶ä¿®æ”¹æ—¶é—´ç”Ÿæˆå“ˆå¸Œ
        file_times = []
        for root, dirs, files in os.walk(data_dir):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    file_times.append(str(os.path.getmtime(file_path)))
                except:
                    pass

        content = data_dir + "".join(sorted(file_times))
        return hashlib.md5(content.encode()).hexdigest()

    @staticmethod
    def _load_from_cache(cache_key):
        """ä»ç¼“å­˜åŠ è½½"""
        if not DataAnalyzer._cache_enabled:
            return None

        cache_file = os.path.join(DataAnalyzer._cache_dir, f"{cache_key}.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    cached_time, analysis = pickle.load(f)
                # ç¼“å­˜æœ‰æ•ˆæœŸ1å°æ—¶
                if time.time() - cached_time < 3600:
                    print(
                        f"ä½¿ç”¨ç¼“å­˜åˆ†æç»“æœ (ç¼“å­˜æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(cached_time))})")
                    return analysis
            except:
                pass
        return None

    @staticmethod
    def _save_to_cache(cache_key, analysis):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        if not DataAnalyzer._cache_enabled:
            return

        os.makedirs(DataAnalyzer._cache_dir, exist_ok=True)
        cache_file = os.path.join(DataAnalyzer._cache_dir, f"{cache_key}.pkl")
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump((time.time(), analysis), f)
        except:
            pass

    @staticmethod
    def analyze_dataset(data_dir, force_refresh=False):
        """åˆ†ææ•°æ®é›†å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        print(f"åˆ†ææ•°æ®é›†: {data_dir}")

        # æ£€æŸ¥ç¼“å­˜
        cache_key = DataAnalyzer._get_cache_key(data_dir)
        if not force_refresh:
            cached = DataAnalyzer._load_from_cache(cache_key)
            if cached:
                return cached

        analysis_start = time.time()

        # å¹¶è¡Œæ‰§è¡Œåˆ†æä»»åŠ¡
        analysis = {
            'basic_stats': DataAnalyzer._get_basic_stats(data_dir),
            'file_distribution': DataAnalyzer._analyze_file_distribution(data_dir),
            'object_statistics': DataAnalyzer._analyze_objects(data_dir),
            'temporal_analysis': DataAnalyzer._analyze_temporal(data_dir),
            'cooperative_data': DataAnalyzer._analyze_cooperative_data(data_dir),
            'quality_metrics': DataAnalyzer._calculate_quality_metrics(data_dir),
            'safety_analysis': DataAnalyzer._analyze_safety_data(data_dir)
        }

        # ç”Ÿæˆè¯„åˆ†
        analysis['overall_score'] = DataAnalyzer._calculate_overall_score(analysis)

        # æ·»åŠ åˆ†æå…ƒæ•°æ®
        analysis['metadata'] = {
            'analysis_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'analysis_duration': round(time.time() - analysis_start, 2),
            'cache_key': cache_key
        }

        # ä¿å­˜åˆ†æç»“æœ
        DataAnalyzer._save_analysis_report(data_dir, analysis)

        # ä¿å­˜åˆ°ç¼“å­˜
        DataAnalyzer._save_to_cache(cache_key, analysis)

        # æ‰“å°æ‘˜è¦
        DataAnalyzer._print_analysis_summary(analysis)

        return analysis

    @staticmethod
    def _get_basic_stats(data_dir):
        """è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        stats = {
            'total_size_mb': 0,
            'file_count': 0,
            'directory_count': 0,
            'data_types': defaultdict(int),
            'largest_files': [],
            'oldest_newest_files': {}
        }

        file_sizes = []
        file_times = []

        for root, dirs, files in os.walk(data_dir):
            stats['directory_count'] += len(dirs)
            stats['file_count'] += len(files)

            for file in files:
                file_path = os.path.join(root, file)
                try:
                    # æ–‡ä»¶å¤§å°
                    file_size = os.path.getsize(file_path)
                    stats['total_size_mb'] += file_size

                    # æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    mtime = os.path.getmtime(file_path)
                    file_times.append((file_path, mtime))

                    # è®°å½•å¤§æ–‡ä»¶
                    if file_size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                        file_sizes.append((file_path, file_size))

                    # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
                    ext = os.path.splitext(file)[1].lower()
                    if ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:
                        stats['data_types']['images'] += 1
                    elif ext == '.json':
                        stats['data_types']['json'] += 1
                    elif ext in ['.txt', '.csv', '.log']:
                        stats['data_types']['text'] += 1
                    elif ext in ['.bin', '.pcd']:
                        stats['data_types']['binary'] += 1
                    elif ext in ['.pkl', '.pickle']:
                        stats['data_types']['pickle'] += 1
                    elif ext == '.gz':
                        stats['data_types']['compressed'] += 1
                    else:
                        stats['data_types']['other'] += 1

                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

        # è½¬æ¢ä¸ºMB
        stats['total_size_mb'] = round(stats['total_size_mb'] / (1024 * 1024), 2)

        # æ‰¾å‡ºæœ€å¤§çš„5ä¸ªæ–‡ä»¶
        file_sizes.sort(key=lambda x: x[1], reverse=True)
        stats['largest_files'] = [
            {'path': os.path.relpath(path, data_dir), 'size_mb': round(size / (1024 * 1024), 2)}
            for path, size in file_sizes[:5]
        ]

        # æ‰¾å‡ºæœ€æ—§å’Œæœ€æ–°çš„æ–‡ä»¶
        if file_times:
            file_times.sort(key=lambda x: x[1])
            oldest = file_times[0]
            newest = file_times[-1]
            stats['oldest_newest_files'] = {
                'oldest': {
                    'path': os.path.relpath(oldest[0], data_dir),
                    'time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(oldest[1]))
                },
                'newest': {
                    'path': os.path.relpath(newest[0], data_dir),
                    'time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(newest[1]))
                }
            }

        return stats

    @staticmethod
    def _analyze_file_distribution(data_dir):
        """åˆ†ææ–‡ä»¶åˆ†å¸ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        distribution = {}

        # å¿«é€Ÿæ‰«æç›®å½•ç»“æ„
        if os.path.exists(data_dir):
            # ä½¿ç”¨os.scandiræé«˜æ•ˆç‡
            with os.scandir(data_dir) as entries:
                for entry in entries:
                    if entry.is_dir():
                        dir_name = entry.name
                        if dir_name == "raw":
                            distribution.update(DataAnalyzer._analyze_raw_data(data_dir))
                        elif dir_name == "stitched":
                            stitched_dir = os.path.join(data_dir, "stitched")
                            if os.path.exists(stitched_dir):
                                stitched_images = [f for f in os.listdir(stitched_dir)
                                                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                                distribution['stitched'] = {
                                    'total': len(stitched_images),
                                    'formats': defaultdict(int)
                                }
                                for img in stitched_images:
                                    ext = os.path.splitext(img)[1].lower()
                                    distribution['stitched']['formats'][ext] += 1
                        elif dir_name == "annotations":
                            annotations_dir = os.path.join(data_dir, "annotations")
                            if os.path.exists(annotations_dir):
                                json_files = [f for f in os.listdir(annotations_dir)
                                              if f.lower().endswith('.json')]
                                distribution['annotations'] = len(json_files)
                        elif dir_name == "lidar":
                            lidar_dir = os.path.join(data_dir, "lidar")
                            if os.path.exists(lidar_dir):
                                distribution['lidar'] = DataAnalyzer._analyze_lidar_data(lidar_dir)
                        elif dir_name == "fusion":
                            fusion_dir = os.path.join(data_dir, "fusion")
                            if os.path.exists(fusion_dir):
                                distribution['fusion'] = DataAnalyzer._analyze_fusion_data(fusion_dir)
                        elif dir_name == "safety_reports":
                            safety_dir = os.path.join(data_dir, "safety_reports")
                            if os.path.exists(safety_dir):
                                distribution['safety_reports'] = DataAnalyzer._analyze_safety_reports(safety_dir)

        return distribution

    @staticmethod
    def _analyze_raw_data(data_dir):
        """åˆ†æåŸå§‹æ•°æ®"""
        distribution = {}
        raw_path = os.path.join(data_dir, "raw")

        if not os.path.exists(raw_path):
            return distribution

        # åˆ†æåŸå§‹å›¾åƒ
        for raw_dir in os.listdir(raw_path):
            full_path = os.path.join(raw_path, raw_dir)
            if os.path.isdir(full_path):
                camera_stats = {}
                total_images = 0

                for camera_dir in os.listdir(full_path):
                    camera_path = os.path.join(full_path, camera_dir)
                    if os.path.isdir(camera_path):
                        images = [f for f in os.listdir(camera_path)
                                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                        camera_stats[camera_dir] = len(images)
                        total_images += len(images)

                distribution[f'raw_{raw_dir}'] = {
                    'cameras': camera_stats,
                    'total_images': total_images,
                    'camera_count': len(camera_stats)
                }

        return distribution

    @staticmethod
    def _analyze_lidar_data(lidar_dir):
        """åˆ†æLiDARæ•°æ®"""
        lidar_stats = {
            'bin': 0,
            'npy': 0,
            'json': 0,
            'batch': 0,
            'pcd': 0,
            'total_size_mb': 0
        }

        total_size = 0
        for root, dirs, files in os.walk(lidar_dir):
            for file in files:
                file_path = os.path.join(root, file)
                ext = os.path.splitext(file)[1].lower()

                try:
                    file_size = os.path.getsize(file_path)
                    total_size += file_size

                    if ext == '.bin':
                        lidar_stats['bin'] += 1
                    elif ext == '.npy':
                        lidar_stats['npy'] += 1
                    elif ext == '.json':
                        lidar_stats['json'] += 1
                    elif 'batch' in file:
                        lidar_stats['batch'] += 1
                    elif ext == '.pcd':
                        lidar_stats['pcd'] += 1
                except:
                    pass

        lidar_stats['total_size_mb'] = round(total_size / (1024 * 1024), 2)
        return lidar_stats

    @staticmethod
    def _analyze_fusion_data(fusion_dir):
        """åˆ†æèåˆæ•°æ®"""
        fusion_stats = {
            'sync_files': 0,
            'calibration_files': 0,
            'total_size_mb': 0,
            'formats': defaultdict(int)
        }

        total_size = 0
        for root, dirs, files in os.walk(fusion_dir):
            for file in files:
                file_path = os.path.join(root, file)
                ext = os.path.splitext(file)[1].lower()

                try:
                    file_size = os.path.getsize(file_path)
                    total_size += file_size
                    fusion_stats['formats'][ext] += 1

                    if 'sync' in file:
                        fusion_stats['sync_files'] += 1
                    elif 'calib' in file or 'intrinsic' in file or 'extrinsic' in file:
                        fusion_stats['calibration_files'] += 1
                except:
                    pass

        fusion_stats['total_size_mb'] = round(total_size / (1024 * 1024), 2)
        return fusion_stats

    @staticmethod
    def _analyze_safety_reports(safety_dir):
        """åˆ†æå®‰å…¨æŠ¥å‘Šæ•°æ®"""
        safety_stats = {
            'reports': 0,
            'high_risk': 0,
            'medium_risk': 0,
            'low_risk': 0,
            'total_interactions': 0
        }

        json_files = [f for f in os.listdir(safety_dir) if f.lower().endswith('.json')]
        safety_stats['reports'] = len(json_files)

        if json_files:
            # é‡‡æ ·åˆ†æå‡ ä¸ªæ–‡ä»¶
            sample_files = json_files[:min(5, len(json_files))]
            for json_file in sample_files:
                try:
                    with open(os.path.join(safety_dir, json_file), 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    if 'high_risk_cases' in data:
                        safety_stats['high_risk'] += data['high_risk_cases']
                    if 'medium_risk_cases' in data:
                        safety_stats['medium_risk'] += data['medium_risk_cases']
                    if 'low_risk_cases' in data:
                        safety_stats['low_risk_cases'] += data['low_risk_cases']
                    if 'total_interactions' in data:
                        safety_stats['total_interactions'] += data['total_interactions']
                except:
                    pass

        return safety_stats

    @staticmethod
    def _analyze_objects(data_dir):
        """åˆ†æç‰©ä½“ç»Ÿè®¡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        annotations_dir = os.path.join(data_dir, "annotations")

        if not os.path.exists(annotations_dir):
            return {
                'total_objects': 0,
                'by_class': {},
                'by_frame': {},
                'class_distribution': {},
                'object_density': 0,
                'frames_with_objects': 0
            }

        object_stats = {
            'total_objects': 0,
            'by_class': defaultdict(int),
            'by_frame': defaultdict(int),
            'class_distribution': {},
            'object_density': 0,
            'frames_with_objects': 0,
            'objects_per_frame_stats': {},
            'class_combinations': set()
        }

        json_files = [f for f in os.listdir(annotations_dir)
                      if f.lower().endswith('.json') and f.startswith('frame_')]

        if not json_files:
            return object_stats

        # é‡‡æ ·åˆ†æï¼Œé¿å…å¤„ç†æ‰€æœ‰æ–‡ä»¶
        sample_size = min(50, len(json_files))
        sample_files = random.sample(json_files, sample_size) if len(json_files) > 50 else json_files

        objects_per_frame = []

        for json_file in sample_files:
            try:
                with open(os.path.join(annotations_dir, json_file), 'r', encoding='utf-8') as f:
                    data = json.load(f)

                frame_id = data.get('frame_id', 0)
                objects = data.get('objects', [])

                object_stats['by_frame'][frame_id] = len(objects)
                object_stats['total_objects'] += len(objects)
                objects_per_frame.append(len(objects))

                if objects:
                    object_stats['frames_with_objects'] += 1

                # ç»Ÿè®¡ç±»åˆ«å’Œç»„åˆ
                frame_classes = set()
                for obj in objects:
                    obj_class = obj.get('class', 'unknown')
                    object_stats['by_class'][obj_class] += 1
                    frame_classes.add(obj_class)

                if frame_classes:
                    object_stats['class_combinations'].add(tuple(sorted(frame_classes)))

            except Exception as e:
                print(f"åˆ†ææ ‡æ³¨æ–‡ä»¶ {json_file} å¤±è´¥: {e}")

        # ä¼°ç®—æ€»æ•°
        if sample_files:
            avg_objects_per_file = object_stats['total_objects'] / len(sample_files)
            object_stats['total_objects'] = int(avg_objects_per_file * len(json_files))
            object_stats['frames_with_objects'] = int(
                (object_stats['frames_with_objects'] / len(sample_files)) * len(json_files))

        # è®¡ç®—ç±»åˆ†å¸ƒç™¾åˆ†æ¯”
        if object_stats['total_objects'] > 0:
            total = sum(object_stats['by_class'].values())
            for obj_class, count in object_stats['by_class'].items():
                object_stats['class_distribution'][obj_class] = round(
                    count / total * 100, 2
                )

        # è®¡ç®—ç‰©ä½“å¯†åº¦ç»Ÿè®¡
        if objects_per_frame:
            object_stats['objects_per_frame_stats'] = {
                'min': min(objects_per_frame),
                'max': max(objects_per_frame),
                'mean': round(np.mean(objects_per_frame), 2),
                'median': round(np.median(objects_per_frame), 2),
                'std': round(np.std(objects_per_frame), 2)
            }
            object_stats['object_density'] = round(np.mean(objects_per_frame), 2)

        # è½¬æ¢ç»„åˆä¸ºå¯åºåˆ—åŒ–çš„åˆ—è¡¨
        object_stats['class_combinations'] = [
            list(combo) for combo in object_stats['class_combinations']
        ]

        return object_stats

    @staticmethod
    def _analyze_temporal(data_dir):
        """åˆ†ææ—¶é—´åˆ†å¸ƒï¼ˆå¢å¼ºç‰ˆï¼‰"""
        temporal_stats = {
            'frame_intervals': [],
            'total_duration': 0,
            'frame_rate': 0,
            'temporal_coverage': 0,
            'frame_consistency': 100,
            'time_range': {}
        }

        metadata_file = os.path.join(data_dir, "metadata", "collection_info.json")
        if os.path.exists(metadata_file):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                collection_stats = metadata.get('collection_stats', {})
                temporal_stats['total_duration'] = collection_stats.get('duration_seconds', 0)
                temporal_stats['frame_rate'] = collection_stats.get('frame_rate', 0)

                # è®¡ç®—æ—¶é—´è¦†ç›–
                if 'performance' in metadata:
                    perf = metadata['performance']
                    if 'total_runtime' in perf and temporal_stats['total_duration'] > 0:
                        temporal_stats['temporal_coverage'] = round(
                            min(100, temporal_stats['total_duration'] / perf['total_runtime'] * 100), 1
                        )

                # è·å–æ—¶é—´èŒƒå›´
                if 'collection' in metadata:
                    coll = metadata['collection']
                    temporal_stats['time_range'] = {
                        'start_time': coll.get('start_time', 'unknown'),
                        'end_time': coll.get('end_time', 'unknown'),
                        'duration_hours': round(coll.get('duration', 0) / 3600, 2)
                    }

            except Exception as e:
                print(f"åˆ†æå…ƒæ•°æ®å¤±è´¥: {e}")

        # ä»æ–‡ä»¶æ—¶é—´æ¨æ–­æ—¶é—´èŒƒå›´
        try:
            all_files = []
            for root, dirs, files in os.walk(data_dir):
                for file in files:
                    if file.endswith(('.png', '.jpg', '.jpeg', '.json', '.bin')):
                        file_path = os.path.join(root, file)
                        try:
                            mtime = os.path.getmtime(file_path)
                            all_files.append((file_path, mtime))
                        except:
                            pass

            if all_files:
                all_files.sort(key=lambda x: x[1])
                oldest = all_files[0][1]
                newest = all_files[-1][1]

                if not temporal_stats['time_range']:
                    temporal_stats['time_range'] = {
                        'start_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(oldest)),
                        'end_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(newest)),
                        'duration_hours': round((newest - oldest) / 3600, 2)
                    }
        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶æ—¶é—´å¤±è´¥: {e}")

        return temporal_stats

    @staticmethod
    def _analyze_cooperative_data(data_dir):
        """åˆ†æååŒæ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        coop_dir = os.path.join(data_dir, "cooperative")

        if not os.path.exists(coop_dir):
            return {
                'v2x_messages': 0,
                'shared_perception': 0,
                'vehicles_count': 0,
                'communication_stats': {},
                'cooperation_level': 'none',
                'data_quality': {}
            }

        analysis = {
            'v2x_messages': 0,
            'shared_perception_frames': 0,
            'total_vehicles': 0,
            'ego_vehicles': 0,
            'cooperative_vehicles': 0,
            'v2x_stats': {
                'total_messages': 0,
                'message_types': defaultdict(int),
                'average_message_size': 0,
                'message_frequency': 0
            },
            'shared_objects_count': 0,
            'communication_range': 0,
            'collaborative_detections': 0,
            'cooperation_level': 'low',
            'data_quality': {
                'message_completeness': 0,
                'perception_consistency': 0,
                'temporal_alignment': 0
            }
        }

        # V2Xæ¶ˆæ¯ç»Ÿè®¡
        v2x_dir = os.path.join(coop_dir, "v2x_messages")
        v2x_files = []
        if os.path.exists(v2x_dir):
            v2x_files = [f for f in os.listdir(v2x_dir) if f.lower().endswith('.json')]
            analysis['v2x_messages'] = len(v2x_files)

        # å…±äº«æ„ŸçŸ¥ç»Ÿè®¡
        perception_dir = os.path.join(coop_dir, "shared_perception")
        perception_files = []
        if os.path.exists(perception_dir):
            perception_files = [f for f in os.listdir(perception_dir) if f.lower().endswith('.json')]
            analysis['shared_perception_frames'] = len(perception_files)

        # è¯»å–ååŒæ‘˜è¦
        coop_summary = {}
        summary_file = os.path.join(coop_dir, "cooperative_summary.json")
        if os.path.exists(summary_file):
            try:
                with open(summary_file, 'r', encoding='utf-8') as f:
                    coop_summary = json.load(f)
            except:
                pass

        # åˆ†æV2Xæ¶ˆæ¯å†…å®¹
        if v2x_files:
            total_size = 0
            valid_messages = 0
            sample_size = min(20, len(v2x_files))

            for v2x_file in v2x_files[:sample_size]:
                try:
                    with open(os.path.join(v2x_dir, v2x_file), 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    message = data.get('message', {})
                    message_type = message.get('message_type', 'unknown')
                    analysis['v2x_stats']['message_types'][message_type] += 1

                    file_size = os.path.getsize(os.path.join(v2x_dir, v2x_file))
                    total_size += file_size
                    valid_messages += 1

                    # æ£€æŸ¥æ¶ˆæ¯å®Œæ•´æ€§
                    required_fields = ['sender_id', 'message_type', 'timestamp']
                    completeness = sum(1 for field in required_fields if field in message) / len(required_fields)
                    analysis['data_quality']['message_completeness'] += completeness

                except:
                    pass

            if valid_messages > 0:
                analysis['v2x_stats']['average_message_size'] = round(total_size / valid_messages, 2)
                analysis['data_quality']['message_completeness'] = round(
                    analysis['data_quality']['message_completeness'] / valid_messages * 100, 1
                )

        # åˆ†æå…±äº«æ„ŸçŸ¥æ•°æ®
        if perception_files:
            consistent_frames = 0
            sample_size = min(10, len(perception_files))

            for perception_file in perception_files[:sample_size]:
                try:
                    with open(os.path.join(perception_dir, perception_file), 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
                    if 'shared_objects' in data and 'timestamp' in data and 'frame_id' in data:
                        consistent_frames += 1
                except:
                    pass

            if sample_size > 0:
                analysis['data_quality']['perception_consistency'] = round(
                    consistent_frames / sample_size * 100, 1
                )

        # ä»æ‘˜è¦ä¸­è·å–æ•°æ®
        analysis.update({
            'total_vehicles': coop_summary.get('total_vehicles', 0),
            'ego_vehicles': coop_summary.get('ego_vehicles', 0),
            'cooperative_vehicles': coop_summary.get('cooperative_vehicles', 0),
            'shared_objects_count': coop_summary.get('shared_objects_count', 0),
            'communication_range': coop_summary.get('communication_range', 0),
            'collaborative_detections': coop_summary.get('v2x_stats', {}).get('collaborative_detections', 0)
        })

        # è®¡ç®—åˆä½œæ°´å¹³
        cooperation_score = 0
        if analysis['v2x_messages'] > 50 and analysis['shared_perception_frames'] > 20:
            cooperation_score = 90
            analysis['cooperation_level'] = 'high'
        elif analysis['v2x_messages'] > 10 and analysis['shared_perception_frames'] > 5:
            cooperation_score = 60
            analysis['cooperation_level'] = 'medium'
        elif analysis['v2x_messages'] > 0 or analysis['shared_perception_frames'] > 0:
            cooperation_score = 30
            analysis['cooperation_level'] = 'low'

        analysis['cooperation_score'] = cooperation_score

        return analysis

    @staticmethod
    def _analyze_safety_data(data_dir):
        """åˆ†æå®‰å…¨æ•°æ®"""
        safety_dir = os.path.join(data_dir, "safety_reports")

        if not os.path.exists(safety_dir):
            return {
                'total_reports': 0,
                'risk_levels': {'high': 0, 'medium': 0, 'low': 0},
                'safety_score': 0,
                'pedestrian_interactions': 0,
                'average_distance': 0
            }

        json_files = [f for f in os.listdir(safety_dir) if f.lower().endswith('.json')]

        safety_data = {
            'total_reports': len(json_files),
            'risk_levels': {'high': 0, 'medium': 0, 'low': 0},
            'safety_score': 0,
            'pedestrian_interactions': 0,
            'average_distance': 0,
            'near_misses': 0,
            'safety_warnings': 0
        }

        if json_files:
            distances = []
            for json_file in json_files[:min(10, len(json_files))]:
                try:
                    with open(os.path.join(safety_dir, json_file), 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    if 'high_risk_cases' in data:
                        safety_data['risk_levels']['high'] += data['high_risk_cases']
                    if 'medium_risk_cases' in data:
                        safety_data['risk_levels']['medium'] += data['medium_risk_cases']
                    if 'low_risk_cases' in data:
                        safety_data['risk_levels']['low'] += data['low_risk_cases']
                    if 'total_interactions' in data:
                        safety_data['pedestrian_interactions'] += data['total_interactions']
                    if 'average_distance' in data:
                        distances.append(data['average_distance'])
                    if 'near_misses' in data:
                        safety_data['near_misses'] += data['near_misses']
                    if 'safety_warnings' in data:
                        safety_data['safety_warnings'] += data['safety_warnings']

                except Exception as e:
                    print(f"åˆ†æå®‰å…¨æŠ¥å‘Š {json_file} å¤±è´¥: {e}")

            if distances:
                safety_data['average_distance'] = round(np.mean(distances), 2)

            # è®¡ç®—å®‰å…¨è¯„åˆ†
            total_risks = sum(safety_data['risk_levels'].values())
            if total_risks > 0:
                high_risk_ratio = safety_data['risk_levels']['high'] / total_risks
                safety_data['safety_score'] = max(0, 100 - high_risk_ratio * 100)
            else:
                safety_data['safety_score'] = 100

        return safety_data

    @staticmethod
    def _calculate_quality_metrics(data_dir):
        """è®¡ç®—è´¨é‡æŒ‡æ ‡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        quality_metrics = {
            'completeness_score': 0,
            'consistency_score': 0,
            'diversity_score': 0,
            'cooperative_score': 0,
            'temporal_score': 0,
            'structural_score': 0,
            'safety_score': 0,
            'issues_found': [],
            'recommendations': []
        }

        # 1. æ£€æŸ¥å®Œæ•´æ€§
        required_dirs = [
            "raw/vehicle",
            "raw/infrastructure",
            "stitched",
            "metadata",
            "cooperative"
        ]

        optional_dirs = [
            "lidar",
            "fusion",
            "annotations",
            "calibration",
            "safety_reports"
        ]

        missing_required = []
        missing_optional = []

        for dir_path in required_dirs:
            full_path = os.path.join(data_dir, dir_path)
            if not os.path.exists(full_path):
                missing_required.append(dir_path)

        for dir_path in optional_dirs:
            full_path = os.path.join(data_dir, dir_path)
            if not os.path.exists(full_path):
                missing_optional.append(dir_path)

        if missing_required:
            quality_metrics['issues_found'].append(f"ç¼ºå¤±å¿…è¦ç›®å½•: {missing_required}")
            quality_metrics['completeness_score'] = 100 - (len(missing_required) * 20)
        else:
            quality_metrics['completeness_score'] = 100

        # 2. ç»“æ„è¯„åˆ†
        structure_score = 100
        if missing_optional:
            structure_score -= len(missing_optional) * 5
            quality_metrics['recommendations'].append(f"å»ºè®®æ·»åŠ å¯é€‰ç›®å½•: {missing_optional[:3]}")

        quality_metrics['structural_score'] = max(0, structure_score)

        # 3. æ£€æŸ¥ä¸€è‡´æ€§ï¼ˆå›¾åƒæ•°é‡ï¼‰
        raw_vehicle = os.path.join(data_dir, "raw", "vehicle")
        if os.path.exists(raw_vehicle):
            camera_counts = []
            for camera_dir in os.listdir(raw_vehicle):
                camera_path = os.path.join(raw_vehicle, camera_dir)
                if os.path.isdir(camera_path):
                    images = [f for f in os.listdir(camera_path)
                              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                    camera_counts.append(len(images))

            if camera_counts:
                max_diff = max(camera_counts) - min(camera_counts) if camera_counts else 0
                if max_diff > 10:
                    quality_metrics['issues_found'].append(f"æ‘„åƒå¤´å›¾åƒæ•°é‡ä¸¥é‡ä¸ä¸€è‡´: æœ€å¤§å·®å¼‚{max_diff}å¼ ")
                    quality_metrics['consistency_score'] = 60
                elif max_diff > 5:
                    quality_metrics['issues_found'].append(f"æ‘„åƒå¤´å›¾åƒæ•°é‡ä¸ä¸€è‡´: å·®å¼‚{max_diff}å¼ ")
                    quality_metrics['consistency_score'] = 75
                else:
                    quality_metrics['consistency_score'] = 95
            else:
                quality_metrics['consistency_score'] = 70
                quality_metrics['issues_found'].append("è½¦è¾†æ‘„åƒå¤´æ— å›¾åƒæ•°æ®")
        else:
            quality_metrics['consistency_score'] = 50

        # 4. å¤šæ ·æ€§è¯„åˆ†ï¼ˆåŸºäºç‰©ä½“ç±»åˆ«ï¼‰
        object_stats = DataAnalyzer._analyze_objects(data_dir)
        num_classes = len(object_stats.get('by_class', {}))
        class_distribution = object_stats.get('class_distribution', {})

        if num_classes >= 8:
            quality_metrics['diversity_score'] = 95
        elif num_classes >= 5:
            quality_metrics['diversity_score'] = 80
        elif num_classes >= 3:
            quality_metrics['diversity_score'] = 65
            quality_metrics['issues_found'].append(f"ç‰©ä½“ç±»åˆ«è¾ƒå°‘: {num_classes}ç±»")
        else:
            quality_metrics['diversity_score'] = 40
            quality_metrics['issues_found'].append(f"ç‰©ä½“ç±»åˆ«è¿‡å°‘: {num_classes}ç±»")

        # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒæ˜¯å¦å‡è¡¡
        if class_distribution:
            values = list(class_distribution.values())
            if max(values) > 70:  # æŸä¸ªç±»åˆ«å æ¯”è¶…è¿‡70%
                quality_metrics['diversity_score'] *= 0.8  # é™ä½åˆ†æ•°
                quality_metrics['recommendations'].append("æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒä¸å‡è¡¡ï¼Œå»ºè®®æ”¶é›†æ›´å¤šæ ·åŒ–çš„åœºæ™¯")

        # 5. ååŒè¯„åˆ†
        cooperative_data = DataAnalyzer._analyze_cooperative_data(data_dir)
        quality_metrics['cooperative_score'] = cooperative_data.get('cooperation_score', 0)

        if quality_metrics['cooperative_score'] < 50:
            quality_metrics['issues_found'].append("ååŒæ•°æ®è¾ƒå°‘æˆ–è´¨é‡ä¸é«˜")
            quality_metrics['recommendations'].append("å¢åŠ V2Xæ¶ˆæ¯å’Œå…±äº«æ„ŸçŸ¥æ•°æ®çš„ç”Ÿæˆ")

        # 6. æ—¶é—´è¯„åˆ†
        temporal_data = DataAnalyzer._analyze_temporal(data_dir)
        frame_rate = temporal_data.get('frame_rate', 0)
        duration = temporal_data.get('total_duration', 0)

        if frame_rate >= 5.0:
            quality_metrics['temporal_score'] = 95
        elif frame_rate >= 2.0:
            quality_metrics['temporal_score'] = 80
        elif frame_rate >= 1.0:
            quality_metrics['temporal_score'] = 60
        else:
            quality_metrics['temporal_score'] = 30
            quality_metrics['issues_found'].append(f"å¸§ç‡è¾ƒä½: {frame_rate:.2f} FPS")

        if duration < 30:
            quality_metrics['temporal_score'] *= 0.8  # æ—¶é•¿ä¸è¶³ï¼Œé™ä½åˆ†æ•°
            quality_metrics['recommendations'].append("å»ºè®®å¢åŠ æ•°æ®æ”¶é›†æ—¶é•¿ä»¥è·å¾—æ›´å®Œæ•´çš„æ—¶é—´åºåˆ—")

        # 7. å®‰å…¨è¯„åˆ†
        safety_data = DataAnalyzer._analyze_safety_data(data_dir)
        quality_metrics['safety_score'] = safety_data.get('safety_score', 0)

        if quality_metrics['safety_score'] < 80:
            quality_metrics['issues_found'].append(f"å®‰å…¨è¯„åˆ†è¾ƒä½: {quality_metrics['safety_score']}")
            quality_metrics['recommendations'].append("å»ºè®®å¢åŠ è¡Œäººå®‰å…¨ç›¸å…³çš„åœºæ™¯å’Œæ•°æ®æ”¶é›†")

        # é™åˆ¶åˆ†æ•°åœ¨0-100ä¹‹é—´
        for key in ['completeness_score', 'consistency_score', 'diversity_score',
                    'cooperative_score', 'temporal_score', 'structural_score', 'safety_score']:
            quality_metrics[key] = max(0, min(100, quality_metrics[key]))

        return quality_metrics

    @staticmethod
    def _calculate_overall_score(analysis):
        """è®¡ç®—æ€»ä½“è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        weights = {
            'completeness': 0.15,  # å®Œæ•´æ€§
            'consistency': 0.12,  # ä¸€è‡´æ€§
            'temporal': 0.12,  # æ—¶é—´æ€§
            'structural': 0.08,  # ç»“æ„æ€§
            'diversity': 0.12,  # å¤šæ ·æ€§
            'cooperative': 0.12,  # ååŒæ€§
            'safety': 0.19,  # å®‰å…¨æ€§
            'quality_bonus': 0.10  # è´¨é‡åŠ æˆ
        }

        quality = analysis['quality_metrics']

        # åŸºç¡€åˆ†æ•°
        base_score = (
                quality['completeness_score'] * weights['completeness'] +
                quality['consistency_score'] * weights['consistency'] +
                quality['temporal_score'] * weights['temporal'] +
                quality['structural_score'] * weights['structural'] +
                quality['diversity_score'] * weights['diversity'] +
                quality['cooperative_score'] * weights['cooperative'] +
                quality['safety_score'] * weights['safety']
        )

        # è´¨é‡åŠ æˆï¼ˆåŸºäºé—®é¢˜æ•°é‡ï¼‰
        issues_count = len(quality.get('issues_found', []))
        quality_bonus = max(0, 100 - issues_count * 5) * weights['quality_bonus']

        total_score = base_score + quality_bonus

        # é¢å¤–åŠ æˆï¼ˆå¦‚æœæ•°æ®é›†ç‰¹åˆ«ä¼˜ç§€ï¼‰
        if (quality['completeness_score'] >= 95 and
                quality['consistency_score'] >= 90 and
                quality['diversity_score'] >= 85 and
                quality['safety_score'] >= 90):
            total_score += 5

        return round(min(total_score, 100), 1)

    @staticmethod
    def _save_analysis_report(data_dir, analysis):
        """ä¿å­˜åˆ†ææŠ¥å‘Šï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        metadata_dir = os.path.join(data_dir, "metadata")
        os.makedirs(metadata_dir, exist_ok=True)

        report_file = os.path.join(metadata_dir, "dataset_analysis.json")

        # ä¿å­˜å®Œæ•´æŠ¥å‘Š
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)

        # ä¿å­˜æ‘˜è¦æŠ¥å‘Šï¼ˆè½»é‡ç‰ˆï¼‰
        summary = {
            'overall_score': analysis['overall_score'],
            'quality_metrics': analysis['quality_metrics'],
            'basic_stats': {
                'total_size_mb': analysis['basic_stats']['total_size_mb'],
                'file_count': analysis['basic_stats']['file_count'],
                'directory_count': analysis['basic_stats']['directory_count']
            },
            'object_statistics': {
                'total_objects': analysis['object_statistics']['total_objects'],
                'num_classes': len(analysis['object_statistics']['by_class'])
            },
            'safety_data': analysis.get('safety_analysis', {}),
            'analysis_metadata': analysis.get('metadata', {})
        }

        summary_file = os.path.join(metadata_dir, "dataset_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        print(f"æ•°æ®é›†åˆ†ææŠ¥å‘Šä¿å­˜: {report_file}")
        print(f"æ•°æ®é›†æ‘˜è¦ä¿å­˜: {summary_file}")

    @staticmethod
    def _print_analysis_summary(analysis):
        """æ‰“å°åˆ†ææ‘˜è¦ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\n" + "=" * 70)
        print("æ•°æ®é›†åˆ†ææ‘˜è¦")
        print("=" * 70)

        # åŸºæœ¬ç»Ÿè®¡
        basic = analysis['basic_stats']
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"  æ€»å¤§å°: {basic['total_size_mb']} MB")
        print(f"  æ–‡ä»¶æ•°: {basic['file_count']:,}")
        print(f"  ç›®å½•æ•°: {basic['directory_count']}")
        print(f"  æ•°æ®ç±»å‹åˆ†å¸ƒ:")
        for data_type, count in basic['data_types'].items():
            print(f"    {data_type}: {count:,}")

        if basic['largest_files']:
            print(f"  æœ€å¤§çš„æ–‡ä»¶:")
            for file_info in basic['largest_files'][:3]:
                print(f"    {file_info['path']}: {file_info['size_mb']} MB")

        # æ–‡ä»¶åˆ†å¸ƒ
        distribution = analysis['file_distribution']
        print(f"\nğŸ“ æ–‡ä»¶åˆ†å¸ƒ:")
        for key, value in distribution.items():
            if isinstance(value, dict):
                if 'total' in value:
                    print(f"  {key}: {value['total']:,}")
                    if 'formats' in value:
                        for fmt, count in value['formats'].items():
                            print(f"    {fmt}: {count:,}")
                else:
                    print(f"  {key}:")
                    for subkey, subvalue in value.items():
                        print(f"    {subkey}: {subvalue:,}")
            else:
                print(f"  {key}: {value:,}")

        # ç‰©ä½“ç»Ÿè®¡
        objects = analysis['object_statistics']
        print(f"\nğŸ¯ ç‰©ä½“ç»Ÿè®¡:")
        print(f"  æ€»ç‰©ä½“æ•°: {objects['total_objects']:,}")
        print(f"  æœ‰ç‰©ä½“çš„å¸§æ•°: {objects['frames_with_objects']:,}")
        print(f"  å¹³å‡æ¯å¸§ç‰©ä½“æ•°: {objects['object_density']:.2f}")

        if objects['by_class']:
            print(f"  ç±»åˆ«åˆ†å¸ƒ:")
            for obj_class, count in sorted(objects['by_class'].items(), key=lambda x: x[1], reverse=True)[:5]:
                percentage = objects['class_distribution'].get(obj_class, 0)
                print(f"    {obj_class}: {count:,} ({percentage}%)")

        if objects['objects_per_frame_stats']:
            stats = objects['objects_per_frame_stats']
            print(f"  æ¯å¸§ç‰©ä½“æ•°ç»Ÿè®¡:")
            print(f"    æœ€å°: {stats['min']}, æœ€å¤§: {stats['max']}, å¹³å‡: {stats['mean']}, ä¸­ä½æ•°: {stats['median']}")

        # å®‰å…¨æ•°æ®åˆ†æ
        if 'safety_analysis' in analysis:
            safety = analysis['safety_analysis']
            print(f"\nğŸš¸ å®‰å…¨æ•°æ®åˆ†æ:")
            print(f"  å®‰å…¨è¯„åˆ†: {safety.get('safety_score', 0)}/100")
            print(f"  é£é™©ç­‰çº§åˆ†å¸ƒ:")
            print(f"    é«˜é£é™©: {safety.get('risk_levels', {}).get('high', 0)}")
            print(f"    ä¸­é£é™©: {safety.get('risk_levels', {}).get('medium', 0)}")
            print(f"    ä½é£é™©: {safety.get('risk_levels', {}).get('low', 0)}")
            print(f"  è¡Œäººäº¤äº’æ¬¡æ•°: {safety.get('pedestrian_interactions', 0)}")
            print(f"  å¹³å‡è·ç¦»: {safety.get('average_distance', 0):.2f}ç±³")
            print(f"  è¿‘è·ç¦»äº‹ä»¶: {safety.get('near_misses', 0)}")
            print(f"  å®‰å…¨è­¦å‘Š: {safety.get('safety_warnings', 0)}")

        # ååŒæ•°æ®åˆ†æ
        cooperative = analysis['cooperative_data']
        print(f"\nğŸ¤ ååŒæ•°æ®åˆ†æ:")
        print(f"  åˆä½œæ°´å¹³: {cooperative['cooperation_level'].upper()}")
        print(f"  V2Xæ¶ˆæ¯: {cooperative['v2x_messages']:,}")
        print(f"  å…±äº«æ„ŸçŸ¥å¸§: {cooperative['shared_perception_frames']:,}")
        print(f"  è½¦è¾†æ€»æ•°: {cooperative['total_vehicles']}")
        print(f"    â”œ ä¸»è½¦: {cooperative['ego_vehicles']}")
        print(f"    â”” ååŒè½¦: {cooperative['cooperative_vehicles']}")
        print(f"  å…±äº«å¯¹è±¡æ•°: {cooperative['shared_objects_count']:,}")
        print(f"  åä½œæ£€æµ‹æ•°: {cooperative['collaborative_detections']:,}")

        if cooperative['v2x_stats']['message_types']:
            print(f"  V2Xæ¶ˆæ¯ç±»å‹:")
            for msg_type, count in cooperative['v2x_stats']['message_types'].items():
                print(f"    {msg_type}: {count}")

        # æ—¶é—´åˆ†æ
        temporal = analysis['temporal_analysis']
        print(f"\nâ° æ—¶é—´åˆ†æ:")
        print(f"  æ€»æ—¶é•¿: {temporal['total_duration']:.1f}ç§’ ({temporal['total_duration'] / 60:.1f}åˆ†é’Ÿ)")
        print(f"  å¹³å‡å¸§ç‡: {temporal['frame_rate']:.2f} FPS")
        print(f"  æ—¶é—´è¦†ç›–ç‡: {temporal['temporal_coverage']}%")

        if temporal['time_range']:
            tr = temporal['time_range']
            print(f"  æ—¶é—´èŒƒå›´: {tr.get('start_time', 'N/A')} åˆ° {tr.get('end_time', 'N/A')}")
            if 'duration_hours' in tr:
                print(f"  æŒç»­æ—¶é—´: {tr['duration_hours']:.1f}å°æ—¶")

        # è´¨é‡æŒ‡æ ‡
        quality = analysis['quality_metrics']
        print(f"\nğŸ“ˆ è´¨é‡æŒ‡æ ‡:")
        metrics = [
            ('å®Œæ•´æ€§', quality['completeness_score']),
            ('ä¸€è‡´æ€§', quality['consistency_score']),
            ('ç»“æ„æ€§', quality['structural_score']),
            ('æ—¶é—´æ€§', quality['temporal_score']),
            ('å¤šæ ·æ€§', quality['diversity_score']),
            ('ååŒæ€§', quality['cooperative_score']),
            ('å®‰å…¨æ€§', quality['safety_score'])
        ]

        for name, score in metrics:
            bar = "â–ˆ" * int(score / 5)
            print(f"  {name:8s}: {score:3.0f}/100 {bar}")

        if quality['issues_found']:
            print(f"\nâš ï¸  å‘ç°çš„é—®é¢˜ ({len(quality['issues_found'])}):")
            for i, issue in enumerate(quality['issues_found'][:5], 1):
                print(f"    {i}. {issue}")
            if len(quality['issues_found']) > 5:
                print(f"    ... è¿˜æœ‰ {len(quality['issues_found']) - 5} ä¸ªé—®é¢˜")

        if quality['recommendations']:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(quality['recommendations'][:3], 1):
                print(f"    {i}. {rec}")

        print(f"\nâ­ æ€»ä½“è¯„åˆ†: {analysis['overall_score']}/100")

        overall_score = analysis['overall_score']
        if overall_score >= 90:
            print("ğŸ‰ æ•°æ®é›†è´¨é‡ä¼˜ç§€ - å¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒ")
        elif overall_score >= 80:
            print("ğŸ‘ æ•°æ®é›†è´¨é‡è‰¯å¥½ - å»ºè®®è¿›è¡Œå°‘é‡æ•°æ®å¢å¼º")
        elif overall_score >= 70:
            print("âš ï¸  æ•°æ®é›†è´¨é‡ä¸€èˆ¬ - å»ºè®®è¿›è¡Œæ•°æ®æ¸…æ´—å’Œå¢å¼º")
        elif overall_score >= 60:
            print("ğŸ”§ æ•°æ®é›†è´¨é‡éœ€è¦æ”¹è¿› - å»ºè®®è¡¥å……ç¼ºå¤±æ•°æ®")
        else:
            print("ğŸš¨ æ•°æ®é›†è´¨é‡è¾ƒå·® - éœ€è¦å¤§è§„æ¨¡æ”¹è¿›")

        # åˆ†æå…ƒæ•°æ®
        if 'metadata' in analysis:
            meta = analysis['metadata']
            print(f"\nğŸ“ åˆ†æä¿¡æ¯:")
            print(f"  åˆ†ææ—¶é—´: {meta.get('analysis_time', 'N/A')}")
            print(f"  åˆ†æè€—æ—¶: {meta.get('analysis_duration', 0):.1f}ç§’")

        print("=" * 70)

    @staticmethod
    def generate_comparison_report(data_dirs, output_file=None):
        """ç”Ÿæˆå¤šä¸ªæ•°æ®é›†çš„æ¯”è¾ƒæŠ¥å‘Š"""
        comparisons = {}

        for data_dir in data_dirs:
            if os.path.exists(data_dir):
                analysis = DataAnalyzer.analyze_dataset(data_dir)
                comparisons[os.path.basename(data_dir)] = {
                    'overall_score': analysis['overall_score'],
                    'basic_stats': analysis['basic_stats'],
                    'quality_metrics': analysis['quality_metrics'],
                    'object_statistics': {
                        'total_objects': analysis['object_statistics']['total_objects'],
                        'num_classes': len(analysis['object_statistics']['by_class'])
                    },
                    'safety_analysis': analysis.get('safety_analysis', {})
                }

        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(comparisons, f, indent=2, ensure_ascii=False)
            print(f"æ¯”è¾ƒæŠ¥å‘Šä¿å­˜åˆ°: {output_file}")

        return comparisons