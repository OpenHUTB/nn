import carla
import random
import queue
import numpy as np
import cv2
import cvips_utils as utils

# ================= é…ç½®åŒºåŸŸ =================
IMAGE_W, IMAGE_H = 640, 360 
FOV = 90.0
TARGET_FPS = 30
# ===========================================

def get_w2c_matrix(cam_transform):
    """æž„å»ºç»å¯¹å¯¹é½çš„ W2C çŸ©é˜µ"""
    world_2_cam_ue = np.linalg.inv(utils.get_matrix(cam_transform))
    calibration = np.array([
        [0, 1, 0, 0],
        [0, 0, -1, 0],
        [1, 0, 0, 0],
        [0, 0, 0, 1]
    ])
    return np.dot(calibration, world_2_cam_ue)

def draw_3d_box_generic(img, transform, bb, is_walker, K, w2c):
    """
    é€šç”¨ 3D ç”»æ¡†å‡½æ•°
    transform: ç‰©ä½“çš„ä¸–ç•Œå˜æ¢ (carla.Transform)
    bb: ç‰©ä½“çš„ç¢°æ’žç›’ (carla.BoundingBox)
    is_walker: æ˜¯å¦ä¸ºè¡Œäºº (å†³å®šé¢œè‰²)
    """
    # 1. èŽ·å–ç‰©ä½“çš„ä¸–ç•Œå˜æ¢çŸ©é˜µ
    obj_to_world = utils.get_matrix(transform)
    
    # 2. è®¡ç®— 8 ä¸ªé¡¶ç‚¹ (è€ƒè™‘ä¸­å¿ƒåç§» loc å’Œ èŒƒå›´ ext)
    ext = bb.extent
    loc = bb.location
    corners = np.array([
        [loc.x+ext.x, loc.y+ext.y, loc.z+ext.z, 1], [loc.x+ext.x, loc.y-ext.y, loc.z+ext.z, 1],
        [loc.x+ext.x, loc.y-ext.y, loc.z-ext.z, 1], [loc.x+ext.x, loc.y+ext.y, loc.z-ext.z, 1],
        [loc.x-ext.x, loc.y+ext.y, loc.z+ext.z, 1], [loc.x-ext.x, loc.y-ext.y, loc.z+ext.z, 1],
        [loc.x-ext.x, loc.y-ext.y, loc.z-ext.z, 1], [loc.x-ext.x, loc.y+ext.y, loc.z-ext.z, 1]
    ])
    
    pixels = []
    for corner in corners:
        world_pos = np.dot(obj_to_world, corner)
        p = utils.get_image_point(carla.Location(x=world_pos[0], y=world_pos[1], z=world_pos[2]), K, w2c)
        if p is None: return img
        pixels.append(tuple(p))

    color = (0,0,255) if is_walker else (0,255,0)
    edges = [(0,1),(1,2),(2,3),(3,0),(4,5),(5,6),(6,7),(7,4),(0,4),(1,5),(2,6),(3,7)]
    for s, e in edges:
        cv2.line(img, pixels[s], pixels[e], color, 1)
    return img

def main():
    # 1. çŽ¯å¢ƒåˆå§‹åŒ–
    client = carla.Client('localhost', 2000)
    client.set_timeout(20.0)
    world = client.get_world()
    
    settings = world.get_settings()
    settings.synchronous_mode = True
    settings.fixed_delta_seconds = 1.0/TARGET_FPS
    world.apply_settings(settings)
    
    tm = client.get_trafficmanager(8000)
    tm.set_synchronous_mode(True)

    # 2. å¥å£®ç”Ÿæˆä¸»è½¦
    bp_lib = world.get_blueprint_library()
    spawn_points = world.get_map().get_spawn_points()
    ego_vehicle = None
    print("æ­£åœ¨å¯»æ‰¾ç©ºä½ç”Ÿæˆä¸»è½¦...")
    while ego_vehicle is None:
        spawn_point = random.choice(spawn_points)
        ego_vehicle = world.try_spawn_actor(bp_lib.find('vehicle.tesla.model3'), spawn_point)
    
    ego_vehicle.set_autopilot(True, tm.get_port())
    print(f"ä¸»è½¦å·²å°±ç»ªï¼Œä½ç½®: {spawn_point.location}")

    # 3. é…ç½® 6 æ‘„åƒå¤´
    cam_bp = bp_lib.find('sensor.camera.rgb')
    cam_bp.set_attribute('image_size_x', str(IMAGE_W))
    cam_bp.set_attribute('image_size_y', str(IMAGE_H))
    cam_bp.set_attribute('fov', str(FOV))
    
    mounts = {
        'Front':      carla.Transform(carla.Location(x=1.5, z=2.0), carla.Rotation(yaw=0)),
        'FrontLeft':  carla.Transform(carla.Location(x=1.5, z=2.0), carla.Rotation(yaw=-60)),
        'FrontRight': carla.Transform(carla.Location(x=1.5, z=2.0), carla.Rotation(yaw=60)),
        'Back':       carla.Transform(carla.Location(x=-1.5, z=2.0), carla.Rotation(yaw=180)),
        'BackLeft':   carla.Transform(carla.Location(x=-1.5, z=2.0), carla.Rotation(yaw=-120)),
        'BackRight':  carla.Transform(carla.Location(x=-1.5, z=2.0), carla.Rotation(yaw=120))
    }
    
    cams = {}; queues = {}
    for name, trans in mounts.items():
        c = world.spawn_actor(cam_bp, trans, attach_to=ego_vehicle)
        q = queue.Queue(); c.listen(q.put)
        cams[name] = c; queues[name] = q

    K = utils.build_projection_matrix(IMAGE_W, IMAGE_H, FOV)

    # 4. æ‰«æåœ°å›¾é™æ€è½¦ (ID 10 ä»£è¡¨æ‰€æœ‰è½¦è¾†ç±»åž‹)
    print("æ­£åœ¨æ‰«æåœ°å›¾é™æ€è½¦è¾†...")
    static_vehicles = world.get_environment_objects(10)
    print(f"æ‰«æå®Œæˆï¼šæ‰¾åˆ° {len(static_vehicles)} è¾†é™æ€è½¦")

    # 5. ä¸»å¾ªçŽ¯
    print("\nðŸš€ å…¨åœºæ™¯æ£€æµ‹å¯åŠ¨ï¼æŒ‰ 'q' é€€å‡º")
    try:
        while True:
            current_frame = world.tick()
            snapshot = world.get_snapshot()
            
            # ä¸¥æ ¼å¸§å¯¹é½å–å›¾
            imgs = {}
            for name, q in queues.items():
                while True:
                    data = q.get()
                    if data.frame == current_frame:
                        imgs[name] = data
                        break
                    if data.frame > current_frame: break # é˜²æ­¢æ­»å¾ªçŽ¯
            if len(imgs) < 6: continue

            # èŽ·å–åŠ¨æ€ NPC
            all_actors = list(world.get_actors().filter('vehicle.*')) + \
                         list(world.get_actors().filter('walker.pedestrian.*'))
            
            frame_list = []
            display_order = ['FrontLeft', 'Front', 'FrontRight', 'BackLeft', 'Back', 'BackRight']
            
            for name in display_order:
                raw = imgs[name]
                img = np.reshape(np.frombuffer(raw.raw_data, dtype="uint8"), (IMAGE_H, IMAGE_W, 4))[:,:,:3].copy()
                cam_sn = snapshot.find(cams[name].id)
                
                if cam_sn:
                    w2c = get_w2c_matrix(cam_sn.get_transform())
                    cam_loc = cam_sn.get_transform().location

                    # --- ç»˜åˆ¶åŠ¨æ€ç‰©ä½“ ---
                    for actor in all_actors:
                        if actor.id == ego_vehicle.id: continue
                        actor_sn = snapshot.find(actor.id)
                        if actor_sn:
                            if actor_sn.get_transform().location.distance(cam_loc) < 45:
                                is_walker = 'walker' in actor.type_id
                                img = draw_3d_box_generic(img, actor_sn.get_transform(), actor.bounding_box, is_walker, K, w2c)

                    # --- ç»˜åˆ¶é™æ€ç‰©ä½“ (è·¯è¾¹è½¦) ---
                    for obj in static_vehicles:
                        if obj.transform.location.distance(cam_loc) < 45:
                            img = draw_3d_box_generic(img, obj.transform, obj.bounding_box, False, K, w2c)
                
                cv2.putText(img, name, (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                frame_list.append(img)

            # æ‹¼æŽ¥æ˜¾ç¤º
            combined = np.vstack([np.hstack(frame_list[:3]), np.hstack(frame_list[3:])])
            cv2.imshow("CVIPS Full Scene Detection", combined)
            if cv2.waitKey(1) & 0xFF == ord('q'): break

    finally:
        print("æ­£åœ¨æ¸…ç†çŽ¯å¢ƒ...")
        settings = world.get_settings(); settings.synchronous_mode = False; world.apply_settings(settings)
        ego_vehicle.destroy()
        for c in cams.values(): c.destroy()
        cv2.destroyAllWindows()

if __name__ == '__main__':
    main()